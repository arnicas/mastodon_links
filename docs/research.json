[
  {
    "link": "https://arxiv.org/abs/2305.14334",
    "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence",
    "latest": "2023-05-24T09:58:54+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110423070110153900",
      "content": "<p>Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence</p><p>Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski  Trevor Darrell</p><p>tl;dr: diffusion features are good descriptors for semantic corrs, if aggregated among timesteps.</p><p><a href=\"https://arxiv.org/abs/2305.14334\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14334</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41467-023-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D",
    "title": "Spatially-optimized urban greening for reduction of population exposure to land surface temperature extremes | Nature Communications",
    "latest": "2023-05-24T09:55:34+00:00",
    "last_post": {
      "url": "https://datasci.social/@mszll/110423057006850663",
      "content": "<p>Spatially-optimized urban greening for reduction of population exposure to land surface temperature extremes</p><p><a href=\"https://www.nature.com/articles/s41467-023-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41467-023</span><span class=\"invisible\">-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D</span></a></p>"
    },
    "people": [
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13705",
    "title": "DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models",
    "latest": "2023-05-24T09:52:59+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110423046869119371",
      "content": "<p>DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models</p><p>Lijun Li, Li'an Zhuo, Bang Zhang, Liefeng Bo, Chen Chen</p><p>tl;dr: diffusion models can do mesh reconstruction.<br><a href=\"https://arxiv.org/abs/2305.13705\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13705</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13455v1",
    "title": "https://arxiv.org/abs/2305.13455v1",
    "latest": "2023-05-24T09:37:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422986361169984",
      "content": "<p>\ud83d\udcdd Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents \ud83d\udcda</p><p>\"Presents a general framework for implementing and evaluating games with LLMs, including five concrete games as an exemplary set of settings to probe a range of capabilities (e.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clp-research/clembench\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/clp-research/clembe</span><span class=\"invisible\">nch</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13455v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13455v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13417v1",
    "title": "https://arxiv.org/abs/2305.13417v1",
    "latest": "2023-05-24T09:27:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422947048051474",
      "content": "<p>\ud83d\udcdd Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT \ud83d\udcda</p><p>\"Our visualization tool is created on top of the attention mechanism to visualize the flow of information inside the model by analyzing the tokens projected from the attention heads and memory values.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/shacharKZ/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/shacharKZ/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13417v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13417v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13413v1",
    "title": "Syntactic Knowledge via Graph Attention with BERT in Machine Translation",
    "latest": "2023-05-24T08:57:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422828823261253",
      "content": "<p>\ud83d\udcdd Syntactic Knowledge via Graph Attention with BERT in Machine Translation \ud83d\udcda</p><p>\"GAT and BERT jointly represent syntactic dependency feature as explicit knowledge of the source language to enrich source language representations and guide target language generation, which can improve translation quality across machine translation tasks without sacrificing BLEU scores.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13413v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13413v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1080/03080188.2022.2150807",
    "title": "doi.org/10.1080/03080188.2022....",
    "latest": "2023-05-24T08:42:33+00:00",
    "last_post": {
      "url": "https://mastodon.social/@lakens/110422769862919296",
      "content": "<p>Lovely paper on facts and objectivity by Philippe Stamenkovic <a href=\"https://doi.org/10.1080/03080188.2022.2150807\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1080/03080188.2022.</span><span class=\"invisible\">2150807</span></a>. Extremely accessible introduction to this topic. Some quotes: </p><p>\u201cObjectivity is the source of the authority which science enjoys in society, and a precondition of public trust in science: it is one of the main reasons (alongside truth) why we value science.\u201d</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      },
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13412v1",
    "title": "https://arxiv.org/abs/2305.13412v1",
    "latest": "2023-05-24T08:37:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422750300473291",
      "content": "<p>\ud83d\udcdd Element-Aware Summarization with Large Language Models: Expert-Aligned Evaluation and Chain-of-Thought Method \ud83d\udcda</p><p>\"Proposes a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with human writing mindset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Alsace08/SumCoT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Alsace08/SumCoT</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13412v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13412v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/00031224231168074",
    "title": "doi.org/10.1177/00031224231168...",
    "latest": "2023-05-24T08:22:15+00:00",
    "last_post": {
      "url": "https://fediscience.org/@MarkRubin/110417275690043774",
      "content": "<p>New study finds \u201carticles that develop and present a new method tend to be more disruptive.\u201d <a href=\"https://doi.org/10.1177/00031224231168074\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/00031224231168</span><span class=\"invisible\">074</span></a></p><p>Reminds me of Greenwald (2012): \u201cAn analysis of the recent history of Nobel Prizes in science unexpectedly revealed that these awards were given much more often for creation of methods and for method-based discoveries than for developments of new theory\u201d: <a href=\"https://doi.org/10.1177/1745691611434210\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/17456916114342</span><span class=\"invisible\">10</span></a> </p><p><a href=\"https://fediscience.org/tags/Science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Science</span></a><br><a href=\"https://fediscience.org/tags/Theory\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Theory</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/1745691611434210",
    "title": "doi.org/10.1177/17456916114342...",
    "latest": "2023-05-24T08:22:15+00:00",
    "last_post": {
      "url": "https://fediscience.org/@MarkRubin/110417275690043774",
      "content": "<p>New study finds \u201carticles that develop and present a new method tend to be more disruptive.\u201d <a href=\"https://doi.org/10.1177/00031224231168074\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/00031224231168</span><span class=\"invisible\">074</span></a></p><p>Reminds me of Greenwald (2012): \u201cAn analysis of the recent history of Nobel Prizes in science unexpectedly revealed that these awards were given much more often for creation of methods and for method-based discoveries than for developments of new theory\u201d: <a href=\"https://doi.org/10.1177/1745691611434210\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/17456916114342</span><span class=\"invisible\">10</span></a> </p><p><a href=\"https://fediscience.org/tags/Science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Science</span></a><br><a href=\"https://fediscience.org/tags/Theory\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Theory</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.09676",
    "title": "Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications",
    "latest": "2023-05-24T08:08:02+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mymarkup/110422625514652656",
      "content": "<p>Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications</p><p>\u201dBroadly, our findings suggest that though multidisciplinary venues intend to cater to more general audiences, some fields' writing norms may act as barriers rather than bridges, and thus impede the dispersion of scholarly ideas.\u201d</p><p><a href=\"https://arxiv.org/abs/2212.09676\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2212.09676</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.social/tags/preprintwatch\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>preprintwatch</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13403v1",
    "title": "https://arxiv.org/abs/2305.13403v1",
    "latest": "2023-05-24T08:07:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422632150721880",
      "content": "<p>\ud83d\udcdd GATology for Linguistics: What Syntactic Dependencies It Knows \ud83d\udcda</p><p>\"A graph neural network which is a strategy for modeling and representing explicit syntactic knowledge, and can work with pre-trained models such as BERT in downstream tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/UniversalDependencies/UD_\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/UniversalDependenci</span><span class=\"invisible\">es/UD_</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13403v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13403v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/25152459211007467",
    "title": "doi.org/10.1177/25152459211007...",
    "latest": "2023-05-24T07:56:53+00:00",
    "last_post": {
      "url": "https://ecoevo.social/@sortee/110422354226072945",
      "content": "<p>Evidence from psychology shows that <a href=\"https://ecoevo.social/tags/registeredreports\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>registeredreports</span></a> have a lower positive result rate than the standard literature &amp; likely reduces <a href=\"https://ecoevo.social/tags/publicationbias\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>publicationbias</span></a> &amp; QRPs </p><p>\ud83d\udcf0by <span class=\"h-card\"><a href=\"https://mastodon.online/@annescheel\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>annescheel</span></a></span> Mitchell Schijen &amp; <span class=\"h-card\"><a href=\"https://mastodon.social/@lakens\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lakens</span></a></span>  <a href=\"https://doi.org/10.1177/25152459211007467\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/25152459211007</span><span class=\"invisible\">467</span></a></p><p>Time for eco &amp; evo to support Registered Reports!</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13401v1",
    "title": "https://arxiv.org/abs/2305.13401v1",
    "latest": "2023-05-24T07:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422514226278980",
      "content": "<p>\ud83d\udcdd A Study of Conceptual Language Similarity: Comparison and Evaluation \ud83d\udcda</p><p>\"The conceptual similarity is based on the hypothesis that two languages are similar if they encode the meanings of concepts using similar linguistic structures, such as word order and verbal inflection.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/DianDYu/language\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/DianDYu/language</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13401v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13401v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13386v1",
    "title": "Can LLMs facilitate interpretation of pre-trained language models?",
    "latest": "2023-05-24T07:27:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422474828156047",
      "content": "<p>\ud83d\udcdd Can LLMs Facilitate Interpretation of Pre-Trained Language Models? \ud83d\udcda</p><p>\"A: Proposes using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models (LM).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13386v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13386v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://doi.org/10.18452/25440",
    "title": "http://doi.org/10.18452/25440",
    "latest": "2023-05-24T07:26:43+00:00",
    "last_post": {
      "url": "https://mastodon.social/@hauschke/110422471735229407",
      "content": "<p>Ulrike K\u00fcsters vom IRB stellt das Positionspapier der <span class=\"h-card\"><a href=\"https://openbiblio.social/@dini\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>dini</span></a></span> AG FIS  vor:</p><p>Management von Forschungsinformationen in Hochschulen und Forschungseinrichtungen. Eine Standortbestimmung (2022)</p><p><a href=\"http://doi.org/10.18452/25440\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">doi.org/10.18452/25440</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.social/tags/111bibliocon\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>111bibliocon</span></a> <a href=\"https://mastodon.social/tags/FIS\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>FIS</span></a> <a href=\"https://mastodon.social/tags/Forschungsberichterstattung\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Forschungsberichterstattung</span></a> <a href=\"https://mastodon.social/tags/Forschungsinformationssystem\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Forschungsinformationssystem</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@hauschke",
        "display_name": "hauschke"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280902",
    "title": "The efficacy of interventions in reducing belief in conspiracy theories: A systematic review",
    "latest": "2023-05-24T06:30:50+00:00",
    "last_post": {
      "url": "https://mastodon.social/@matthewfacciani/110395774887850512",
      "content": "<p>Meta-analysis reveals that teaching people how to think critically (i.e. prebunking) combats misinformation more effectively than trying to debunk false claims. <br><a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280902\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0280902</span></a><br><a href=\"https://mastodon.social/tags/misinformation\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>misinformation</span></a> <a href=\"https://mastodon.social/tags/MisinfoResearch\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MisinfoResearch</span></a> <a href=\"https://mastodon.social/tags/prebunking\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>prebunking</span></a> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/sociology\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>sociology</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/psychology\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>psychology</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/communicationscholars\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>communicationscholars</span></a></span></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@dougholton",
        "display_name": "Doug Holton"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.11140.pdf",
    "title": "https://arxiv.org/pdf/2305.11140.pdf",
    "latest": "2023-05-24T06:21:09+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@fbk_mt/110422213868329357",
      "content": "<p>Our pick of the week by Beatrice Savoldi: \"Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model\" Amrhein et al., 2023</p><p><a href=\"https://arxiv.org/pdf/2305.11140.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.11140.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://sigmoid.social/tags/pickoftheweek\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>pickoftheweek</span></a> <a href=\"https://sigmoid.social/tags/gender\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>gender</span></a> <a href=\"https://sigmoid.social/tags/bias\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>bias</span></a> <a href=\"https://sigmoid.social/tags/fair\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>fair</span></a> <a href=\"https://sigmoid.social/tags/fairness\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>fairness</span></a> <a href=\"https://sigmoid.social/tags/debiasing\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>debiasing</span></a> <a href=\"https://sigmoid.social/tags/translation\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>translation</span></a> <a href=\"https://sigmoid.social/tags/MT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MT</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@fbk_mt",
        "display_name": "MT Group at FBK"
      }
    ]
  },
  {
    "link": "https://academic.oup.com/pnasnexus/article/2/3/pgad051/7059318?login=false",
    "title": "Complex systems of secrecy: the offshore networks of oligarchs",
    "latest": "2023-05-24T06:01:16+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mikeolson/110422135730518945",
      "content": "<p>Kleptocracy depends on good infrastructure for money laundering. That infrastructure operates worldwide, but the US and UK provide a great deal of it. The most effective way to sanction criminals, make money laundering much harder and penalize corruption and kleptocracy would be to impose know-your-customer and reporting obligations on lawyers and accountants.</p><p>This post comes to you thanks to this excellent PNAS paper:</p><p><a href=\"https://academic.oup.com/pnasnexus/article/2/3/pgad051/7059318?login=false\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">academic.oup.com/pnasnexus/art</span><span class=\"invisible\">icle/2/3/pgad051/7059318?login=false</span></a></p><p>Came my way via 3QD:</p><p><a href=\"https://3quarksdaily.com/3quarksdaily/2023/05/sean-carrolls-mindscape-podcast-brooke-harrington-on-offshore-wealth-as-a-complex-system.html\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">3quarksdaily.com/3quarksdaily/</span><span class=\"invisible\">2023/05/sean-carrolls-mindscape-podcast-brooke-harrington-on-offshore-wealth-as-a-complex-system.html</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@mikeolson",
        "display_name": "Mike Olson"
      }
    ]
  },
  {
    "link": "https://doi.org/10.36253/jlis.it-498",
    "title": "https://doi.org/10.36253/jlis.it-498",
    "latest": "2023-05-24T05:53:57+00:00",
    "last_post": {
      "url": "https://hcommons.social/@jcpeyssard/110422106908824416",
      "content": "<p>Snijder, Ronald. 2023. \u201cBooks in a bubble.: Assessing the OAPEN Library Collection\u201d. JLIS.It 14 (2):75-92. <a href=\"https://doi.org/10.36253/jlis.it-498\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.36253/jlis.it-498</span><span class=\"invisible\"></span></a>.</p>"
    },
    "people": [
      {
        "url": "https://hcommons.social/@jcpeyssard",
        "display_name": "Jean-Christophe Peyssard"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2210.06927",
    "title": "Prediction can be safely used as a proxy for explanation in causally consistent Bayesian generalized linear models",
    "latest": "2023-05-24T04:20:36+00:00",
    "last_post": {
      "url": "https://fediscience.org/@scholzmx/110417237170725257",
      "content": "<p>Updated preprint is up on arxiv. It is now focused on just the prediction as proxy for explanation question. Finding remains that prediction is a valid and reliable proxy for explanation in causally unbiased GLMs when comparing models with the same linear predictor term.<br><a href=\"https://arxiv.org/abs/2210.06927\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2210.06927</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.08167.pdf",
    "title": "https://arxiv.org/pdf/2304.08167.pdf",
    "latest": "2023-05-24T03:49:39+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618137001202",
      "content": "<p>\"Classification of news spreading barriers\" an approach to barrier classification where the semantics of news articles are inferred through Wikipedia concepts. </p><p>(Sittar. et al, 2023)</p><p><a href=\"https://arxiv.org/pdf/2304.08167.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.08167.pdf</span><span class=\"invisible\"></span></a> <a href=\"https://twitter.com/WikiResearch/status/1660675562862518272\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1660675562862518272</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/pdf/10.1145/3579517",
    "title": "dl.acm.org/doi/pdf/10.1145/357...",
    "latest": "2023-05-24T03:49:39+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618163571101",
      "content": "<p>\"Queer Identities, Normative Databases: Challenges to Capturing Queerness On @Wikidata\", showing inherent and unaddressed frictions when translating queer identities to the confines of a structured database. </p><p>( Weathington and Brubaker 2023)</p><p><a href=\"https://dl.acm.org/doi/pdf/10.1145/3579517\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/357</span><span class=\"invisible\">9517</span></a> <a href=\"https://twitter.com/WikiResearch/status/1661060518772195335\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1661060518772195335</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09497",
    "title": "Curious Rhythms: Temporal Regularities of Wikipedia Consumption",
    "latest": "2023-05-24T03:49:38+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618084830656",
      "content": "<p>\"Curious Rhythms: Temporal Regularities of <span class=\"h-card\"><a href=\"https://mastodon.social/@Wikipedia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Wikipedia</span></a></span> Consumption\" a large-scale analysis of billions of timezone-corrected page requests mined from English Wikipedia's server logs.</p><p>(Piccardi et al, 2023)</p><p><a href=\"https://arxiv.org/abs/2305.09497\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09497</span><span class=\"invisible\"></span></a> <a href=\"https://twitter.com/WikiResearch/status/1658871787692736512\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1658871787692736512</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.05403.pdf",
    "title": "https://arxiv.org/pdf/2305.05403.pdf",
    "latest": "2023-05-24T03:49:37+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618061046604",
      "content": "<p>\"Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey\" including fundamental methodologies and practice-oriented recommendations on how to choose between different approaches for a specific problem</p><p>(Razniewski et al, 2023)</p><p><a href=\"https://arxiv.org/pdf/2305.05403.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.05403.pdf</span><span class=\"invisible\"></span></a><br>@Bosch_AI <a href=\"https://twitter.com/WikiResearch/status/1658472419185786880\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1658472419185786880</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13047",
    "title": "Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media",
    "latest": "2023-05-24T02:18:03+00:00",
    "last_post": {
      "url": "https://mastodon.social/@andreskarjus/110417263335240534",
      "content": "<p>Preprint w Mark Mets <span class=\"h-card\"><a href=\"https://mastodon.social/@IndrekIbrus\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>IndrekIbrus</span></a></span> <span class=\"h-card\"><a href=\"https://mastodon.social/@schichmax\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>schichmax</span></a></span> <br>\"Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media\"<br><a href=\"https://arxiv.org/abs/2305.13047\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13047</span><span class=\"invisible\"></span></a><br>We compare 2 corpora, a mainstream publisher &amp; a rightwing populist online outlet. Rightwingers are ofc more negative &amp;talk more about immigration - but now we can quantify. Compared multiple LLMs &amp;ChatGPT3.5, which gets same F1 as a finetuned RoBERTa. But zero-shot! That's pretty awesome.</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14224",
    "title": "mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations",
    "latest": "2023-05-24T02:06:24+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421212155208290",
      "content": "<p>mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations</p><p>Outperforms mT5 at the same parameter sizes by a large margin on NLU and NLG tasks in 40+ languages.</p><p><a href=\"https://arxiv.org/abs/2305.14224\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14224</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14177",
    "title": "ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry",
    "latest": "2023-05-24T01:58:09+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421179739560551",
      "content": "<p>ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry</p><p>Presents a set of open-source RL environments, ChemGymRL, based on the Open AI Gym template</p><p><a href=\"https://arxiv.org/abs/2305.14177\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14177</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13840",
    "title": "https://arxiv.org/abs/2305.13840",
    "latest": "2023-05-24T01:54:16+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421164483028998",
      "content": "<p>Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models</p><p>proj: <a href=\"https://controlavideo.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">controlavideo.github.io/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.13840\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13840</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14337",
    "title": "Anchor Prediction: Automatic Refinement of Internet Links",
    "latest": "2023-05-24T01:48:13+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421140695324439",
      "content": "<p>Anchor Prediction: Automatic Refinement of Internet Links</p><p>Releases the AuthoAnchors dataset, a collection of 34k naturally-occurring anchored links.</p><p><a href=\"https://arxiv.org/abs/2305.14337\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14337</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14330",
    "title": "https://arxiv.org/abs/2305.14330",
    "latest": "2023-05-24T01:33:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421081480423378",
      "content": "<p>Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation</p><p>repo: <a href=\"https://github.com/KU-CVLAB/DirecT2V\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/KU-CVLAB/DirecT2V</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14330\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14330</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14233",
    "title": "https://arxiv.org/abs/2305.14233",
    "latest": "2023-05-24T01:30:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421069670056432",
      "content": "<p>Enhancing Chat Language Models by Scaling High-quality Instructional Conversations</p><p>- UltraChat contains 1.5M high-quality, diverse multi-turn dialogues<br>- UltraLLaMA outperforms the SotA open-source model, Vicuna </p><p>repo: <a href=\"https://github.com/thunlp/UltraChat\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/thunlp/UltraChat</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14233\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14233</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14325",
    "title": "https://arxiv.org/abs/2305.14325",
    "latest": "2023-05-24T01:12:11+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420998965406370",
      "content": "<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate</p><p>proj: <a href=\"https://composable-models.github.io/llm_debate/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">composable-models.github.io/ll</span><span class=\"invisible\">m_debate/</span></a><br>repo: <a href=\"https://github.com/composable-models/llm_multiagent_debate\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/composable-models/l</span><span class=\"invisible\">lm_multiagent_debate</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14325\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14325</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.00740",
    "title": "Inspecting and Editing Knowledge Representations in Language Models",
    "latest": "2023-05-24T01:06:13+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420975523967772",
      "content": "<p>RT @evanqed@twitter.com</p><p>You\u2019ve heard of model editing, now get ready for \ud83d\udcdd representation editing \ud83d\udcdd</p><p>In our new paper, we find directions in LM rep space that make the LM assert a fact is true.</p><p>Paper: <a href=\"https://arxiv.org/abs/2304.00740\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.00740</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/evandez/REMEDI\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/evandez/REMEDI</span><span class=\"invisible\"></span></a><br>w/ @belindazli@twitter.com @jacobandreas@twitter.com</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/evanqed/status/1661172005083791364\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/evanqed/status/166</span><span class=\"invisible\">1172005083791364</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13534",
    "title": "How Language Model Hallucinations Can Snowball",
    "latest": "2023-05-24T01:01:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420956304706540",
      "content": "<p>How Language Model Hallucinations Can Snowball</p><p><a href=\"https://arxiv.org/abs/2305.13534\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13534</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13735",
    "title": "Aligning Large Language Models through Synthetic Feedback",
    "latest": "2023-05-24T00:59:23+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420948643044011",
      "content": "<p>Aligning Large Language Models through Synthetic Feedback</p><p>Proposes a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs.</p><p><a href=\"https://arxiv.org/abs/2305.13735\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13735</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/1706.03762",
    "title": "https://arxiv.org/abs/1706.03762",
    "latest": "2023-05-23T22:49:13+00:00",
    "last_post": {
      "url": "https://mastodon.gamedev.place/@jay/110420242881451785",
      "content": "<p>GPT - Generatively Pre-trained Transformer <br> <br>\"Let's Build GPT. From Scratch. In Code. Spelled Out\" by Andrej Karpathy<br><a href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">youtube.com/watch?v=kCc8FmEb1n</span><span class=\"invisible\">Y</span></a><br>(Coding starts at 42:13 mark) <br> <br>\"ChatGPT... is trained on a good chunk of the Internet\" (3:26 mark)  \ud83e\udd14 &lt;-- Meet Mr. Copyright \u00a9 <br> <br>Andrej Karpathy<br><a href=\"https://build.microsoft.com/en-US/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8?source=/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">build.microsoft.com/en-US/spea</span><span class=\"invisible\">kers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8?source=/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8</span></a> <br> <br>\"Attention Is All You Need\" Seminal Paper on AI Transformers<br><a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/1706.03762</span><span class=\"invisible\"></span></a> <br> <br><a href=\"https://mastodon.gamedev.place/tags/Microsoft\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Microsoft</span></a> <a href=\"https://mastodon.gamedev.place/tags/MicrosoftBuild\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MicrosoftBuild</span></a> <a href=\"https://mastodon.gamedev.place/tags/Build2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Build2023</span></a> <a href=\"https://mastodon.gamedev.place/tags/ChatGPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ChatGPT</span></a> <a href=\"https://mastodon.gamedev.place/tags/GPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT</span></a> <a href=\"https://mastodon.gamedev.place/tags/OpenAI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAI</span></a> <a href=\"https://mastodon.gamedev.place/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://mastodon.gamedev.place/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <a href=\"https://mastodon.gamedev.place/tags/Copyright\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Copyright</span></a> <a href=\"https://mastodon.gamedev.place/tags/Infringement\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Infringement</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13252",
    "title": "\"According to ...\" Prompting Language Models Improves Quoting from Pre-Training Data",
    "latest": "2023-05-23T22:27:54+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420123288249295",
      "content": "<p>RT @orionweller@twitter.com</p><p>Can we guide LLMs to quote text from their pre-training data using prefixes like \"According To ..\", improving grounding and reducing hallucination? We discovered that LLMs do have this capability and can increase or decrease quoting on request \ud83e\udd2f</p><p>\ud83d\udcdd:<a href=\"https://arxiv.org/abs/2305.13252\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13252</span><span class=\"invisible\"></span></a> 1/5</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/orionweller/status/1660994241773047810\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/orionweller/status</span><span class=\"invisible\">/1660994241773047810</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://osf.io/fawj3",
    "title": "https://osf.io/fawj3",
    "latest": "2023-05-23T20:30:24+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@ElineVG/110419463003101195",
      "content": "<p>I had a wonderful time this morning presenting my poster on an efficient Bayesian observer model predicting attractive and repulsive temporal context effects @VSSMtg <a href=\"https://fosstodon.org/tags/VSS2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>VSS2023</span></a>! If you missed it and are interested, find the poster here: <a href=\"https://osf.io/fawj3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">osf.io/fawj3</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://scicomm.xyz/@sharoz",
        "display_name": "Steve Haroz (@sharoz on \ud83d\udc24)"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/20319525231167983",
    "title": "doi.org/10.1177/20319525231167...",
    "latest": "2023-05-23T20:19:14+00:00",
    "last_post": {
      "url": "https://someone.elses.computer/@mikarv/110258344105065543",
      "content": "<p>In case you\u2019ve been hearing about the draft EU <a href=\"https://someone.elses.computer/tags/PlatformWorkDirective\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PlatformWorkDirective</span></a> and wanted to learn more, our paper on its <a href=\"https://someone.elses.computer/tags/algorithmicManagement\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>algorithmicManagement</span></a> provisions (w Six Silberman &amp; <span class=\"h-card\"><a href=\"https://someone.elses.computer/@RDBinns\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>RDBinns</span></a></span>) now out in the European Labour Law Journal. <br>tl;dr: generally v well drafted, novel aspects worth studying, may create unintended effects, could benefit from some tweaks to avoid tensions and to shore up its complexities in a situation of high power asymmetries. <a href=\"https://doi.org/10.1177/20319525231167983\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/20319525231167</span><span class=\"invisible\">983</span></a> <a href=\"https://someone.elses.computer/tags/gigEconomy\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>gigEconomy</span></a> <a href=\"https://someone.elses.computer/tags/platformWork\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>platformWork</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.scot/@karengregory",
        "display_name": "Karen Gregory"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05322",
    "title": "https://doi.org/10.21105/joss.05322",
    "latest": "2023-05-23T20:08:48+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110419806006542749",
      "content": "<p>Just published in JOSS: 'SarcGraph: A Python package for analyzing the contractile behavior of pluripotent stem cell-derived cardiomyocytes' <a href=\"https://doi.org/10.21105/joss.05322\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05322</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13281",
    "title": "LM vs LM: Detecting Factual Errors via Cross Examination",
    "latest": "2023-05-23T19:28:39+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110419648178699377",
      "content": "<p>RT @johnjnay@twitter.com</p><p>LLM vs LLM: Detecting Errors via Cross Examining Agents</p><p>-An incorrect claim is likely to result in inconsistency w/ other claims<br>-Multi-turn interactions between LLM that generated claim and Examiner LLM</p><p>-Outperforms baselines across factual benchmarks</p><p><a href=\"https://arxiv.org/abs/2305.13281\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13281</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/johnjnay/status/1660830409276440579\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/johnjnay/status/16</span><span class=\"invisible\">60830409276440579</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09781",
    "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification",
    "latest": "2023-05-23T18:55:24+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110387182088605932",
      "content": "<p>SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification</p><p>Significantly reduces the end-to-end latency while provably preserving model quality</p><p><a href=\"https://arxiv.org/abs/2305.09781\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09781</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.sagepub.com/doi/10.1177/15554120221115393",
    "title": "journals.sagepub.com/doi/10.11...",
    "latest": "2023-05-23T18:55:08+00:00",
    "last_post": {
      "url": "https://scholar.social/@electricarchaeo/110419516378183749",
      "content": "<p>hell ya. I always give a little cheer when I see things that we put up on Epoiesen cited in other articles. Today, a reflection on writing history through games cites work that McCall published with us on Epoiesen: <a href=\"https://journals.sagepub.com/doi/10.1177/15554120221115393\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.sagepub.com/doi/10.11</span><span class=\"invisible\">77/15554120221115393</span></a></p>"
    },
    "people": [
      {
        "url": "https://scholar.social/@telliott",
        "display_name": "Tom Elliott"
      },
      {
        "url": "https://scholar.social/@electricarchaeo",
        "display_name": "Shawn Graham"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12182",
    "title": "https://arxiv.org/abs/2305.12182",
    "latest": "2023-05-23T18:42:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419465347473367",
      "content": "<p>Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages</p><p>repo: <a href=\"https://github.com/cisnlp/Glot500\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/cisnlp/Glot500</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.12182\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12182</span><span class=\"invisible\"></span></a><br>HuggingFace: <a href=\"https://huggingface.co/cis-lmu/glot500-base\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">huggingface.co/cis-lmu/glot500</span><span class=\"invisible\">-base</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13016",
    "title": "Iterative Forward Tuning Boosts In-context Learning in Language Models",
    "latest": "2023-05-23T18:41:59+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464644045437",
      "content": "<p>RT @huybery@twitter.com</p><p>In-context learning as the mysterious ability in LMs, and some works points to a link with gradient descent.<br>This inspired us to propose \ud83e\udd14deep-thinking, which boosts ICL by iterative forward tuning.<br>It is possible to tune LMs without backpropagation!</p><p><a href=\"https://arxiv.org/abs/2305.13016\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13016</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/huybery/status/1660835495310499840\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/huybery/status/166</span><span class=\"invisible\">0835495310499840</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13230",
    "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis",
    "latest": "2023-05-23T18:41:56+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464459802469",
      "content": "<p>To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis</p><p>Studies what would happen if we train LLM with repeated data and how we can alleviate the LLM mult-epoch degradation.</p><p><a href=\"https://arxiv.org/abs/2305.13230\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13230</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13144",
    "title": "Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline",
    "latest": "2023-05-23T18:41:53+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464272109741",
      "content": "<p>RT @zhengzangw@twitter.com</p><p>Can we use <a href=\"https://sigmoid.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> to speed up <a href=\"https://sigmoid.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> inference? \ud83d\udd25Excited to share our new work: Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline.\ud83e\uddf54</p><p>ArXiv: <a href=\"https://arxiv.org/abs/2305.13144\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13144</span><span class=\"invisible\"></span></a><br>Blog: <a href=\"https://zhengzangw.github.io/blogs/seqsch/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">zhengzangw.github.io/blogs/seq</span><span class=\"invisible\">sch/</span></a><br>Code: <a href=\"https://github.com/zhengzangw/Sequence-Scheduling\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/zhengzangw/Sequence</span><span class=\"invisible\">-Scheduling</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/zhengzangw/status/1660833269972074496\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/zhengzangw/status/</span><span class=\"invisible\">1660833269972074496</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13245",
    "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    "latest": "2023-05-23T18:41:50+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464061647289",
      "content": "<p>RT @michielsdj@twitter.com</p><p>New paper! Multi-query attention trades quality for speed and requires training a new model. Instead uptrain improved MQ variant from existing multi-head model! </p><p>Work with Joshua Ainslie, James Lee-Thorp, @_theopompus@twitter.com, Federico Lebron, Sumit Sanghai.</p><p><a href=\"https://arxiv.org/abs/2305.13245\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13245</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/michielsdj/status/1660839037903536129\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/michielsdj/status/</span><span class=\"invisible\">1660839037903536129</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12524",
    "title": "TheoremQA: A Theorem-driven Question Answering dataset",
    "latest": "2023-05-23T18:41:49+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463995882500",
      "content": "<p>RT @WenhuChen@twitter.com</p><p>New Arxiv: <a href=\"https://arxiv.org/abs/2305.12524\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12524</span><span class=\"invisible\"></span></a></p><p>GPT-4/PaLM-2 have both shown almost perfect performance on existing grade school math dataset. What about more challenging STEM questions, especially the ones which require specific theorems, like Stoke's theorem, Wiener Process, etc?</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/WenhuChen/status/1660832837715611648\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WenhuChen/status/1</span><span class=\"invisible\">660832837715611648</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13009",
    "title": "https://arxiv.org/abs/2305.13009",
    "latest": "2023-05-23T18:41:41+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463458618269",
      "content": "<p>Textually Pretrained Speech Language Models</p><p>Presents the largest SpeechLM both in terms of number of parameters and training data.</p><p>proj: <a href=\"https://pages.cs.huji.ac.il/adiyoss-lab/twist/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">pages.cs.huji.ac.il/adiyoss-la</span><span class=\"invisible\">b/twist/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.13009\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13009</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13035",
    "title": "Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design",
    "latest": "2023-05-23T18:41:38+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463271311196",
      "content": "<p>Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design</p><p>Their shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute.</p><p><a href=\"https://arxiv.org/abs/2305.13035\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13035</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://doi.org/10.54900/g0qks-tcz98",
    "title": "https://doi.org/10.54900/g0qks-tcz98",
    "latest": "2023-05-23T17:24:14+00:00",
    "last_post": {
      "url": "https://ravenation.club/@mpe/110419158940172271",
      "content": "<p>Attempts at automating journal subject classification by my @CrossrefOrg colleague, Esha Datta<br><a href=\"https://doi.org/10.54900/g0qks-tcz98\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.54900/g0qks-tcz98</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12234",
    "title": "The Damage to Lunar Orbiting Spacecraft Caused by the Ejecta of Lunar Landers",
    "latest": "2023-05-23T17:10:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110419103143950353",
      "content": "<p>The Damage to Lunar Orbiting Spacecraft Caused by the Ejecta of Lunar Landers - <a href=\"https://arxiv.org/abs/2305.12234\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12234</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://bayes.club/@akhilrao",
        "display_name": "edgeworth boxlord"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://doi.org/10.5281/zenodo.5526634",
    "title": "https://doi.org/10.5281/zenodo.5526634",
    "latest": "2023-05-23T17:01:30+00:00",
    "last_post": {
      "url": "https://mastodon.social/@brembs/110418324534008701",
      "content": "<p>Could this be the paradigm shift all of <a href=\"https://mastodon.social/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a> has been waiting for?</p><p>Council of the EU adopts new principles:<br>\"interoperable, not-for-profit infrastructures for publishing based on open source software and open standards\"<br><a href=\"https://data.consilium.europa.eu/doc/document/ST-8827-2023-INIT/en/pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">data.consilium.europa.eu/doc/d</span><span class=\"invisible\">ocument/ST-8827-2023-INIT/en/pdf</span></a></p><p>and now ten major research organizations support the proposal:<br><a href=\"https://www.coalition-s.org/wp-content/uploads/2023/05/JointResponse2CouncilScholCommConclusions.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">coalition-s.org/wp-content/upl</span><span class=\"invisible\">oads/2023/05/JointResponse2CouncilScholCommConclusions.pdf</span></a></p><p>What they propose is nearly identical to our proposal:<br><a href=\"https://doi.org/10.5281/zenodo.5526634\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.5281/zenodo.5526634</span><span class=\"invisible\"></span></a></p><p>Does this now get the ball rolling, or is it just words on paper?</p>"
    },
    "people": [
      {
        "url": "https://ecoevo.social/@cboettig",
        "display_name": "Carl Boettiger"
      },
      {
        "url": "https://ciberlandia.pt/@villares",
        "display_name": "Alexandre B A Villares \ud83d\udc0d \u2614"
      },
      {
        "url": "https://mastodon.social/@edzer",
        "display_name": "Edzer Pebesma"
      },
      {
        "url": "https://mastodon.social/@igor",
        "display_name": "Igor Brigadir"
      },
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://hci.social/@jbigham",
        "display_name": "Jeff Bigham"
      },
      {
        "url": "https://bbq.snoot.com/@ProgGrrl",
        "display_name": "ProgGrrl"
      },
      {
        "url": "https://datasci.social/@yy",
        "display_name": "YY Ahn"
      },
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://econtwitter.net/@ito",
        "display_name": "Tim Gushue"
      },
      {
        "url": "https://sfba.social/@morgandawn",
        "display_name": "morgandawn"
      },
      {
        "url": "https://mastodon.social/@tiffanycli",
        "display_name": "Tiffany Li"
      },
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      },
      {
        "url": "https://mas.to/@enridaga",
        "display_name": "enridaga"
      },
      {
        "url": "https://starbase80.wtf/@5ciFiGirl",
        "display_name": "Sci-Fi Girl"
      },
      {
        "url": "https://fediscience.org/@mario_angst_sci",
        "display_name": "Mario Angst"
      },
      {
        "url": "https://h4.io/@joshisanonymous",
        "display_name": "Joshua McNeill"
      }
    ]
  },
  {
    "link": "https://doi.org/10.54337/jovi.v1i1.7782",
    "title": "doi.org/10.54337/jovi.v1i1.778...",
    "latest": "2023-05-23T16:55:55+00:00",
    "last_post": {
      "url": "https://hci.social/@jovi/110418478552305809",
      "content": "<p>JoVI's first publication is now live! See <a href=\"https://doi.org/10.54337/jovi.v1i1.7782\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.54337/jovi.v1i1.778</span><span class=\"invisible\">2</span></a> for our inaugural editorial &amp; mission statement.</p><p>/cc <span class=\"h-card\"><a href=\"https://hci.social/@khornbaek\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>khornbaek</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@amelia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>amelia</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@arvind\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>arvind</span></a></span> <span class=\"h-card\"><a href=\"https://mastodon.social/@scheidegger\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>scheidegger</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@jschwabish\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>jschwabish</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@elm\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>elm</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@lane\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lane</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@lace\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lace</span></a></span> <span class=\"h-card\"><a href=\"https://hci.social/@floe\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>floe</span></a></span> <span class=\"h-card\"><a href=\"https://fediscience.org/@mjskay\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>mjskay</span></a></span> <span class=\"h-card\"><a href=\"https://mstdn.science/@lonnibesancon\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lonnibesancon</span></a></span></p>"
    },
    "people": [
      {
        "url": "https://vis.social/@elm",
        "display_name": "Niklas Elmqvist"
      },
      {
        "url": "https://vis.social/@dr_tj",
        "display_name": "Dr. T.J. Jankun-Kelly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12212v1",
    "title": "Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT",
    "latest": "2023-05-23T16:30:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110418946391923185",
      "content": "<p>\ud83d\udcdd Prompt ChatGPT in MNER: Improved Multimodal Named Entity Recognition Method Based on Auxiliary Refining Knowledge From ChatGPT \ud83d\udcda</p><p>\"Prompt ChatGPT In MNER (PGIM) utilizes ChatGPT, an implicit knowledge engine, to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12212v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12212v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12074v1",
    "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
    "latest": "2023-05-23T10:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417412880049300",
      "content": "<p>\ud83d\udcdd DisCo: Distilled Student Models Co-Training for Semi-Supervised Text Mining \ud83d\udcda</p><p>\"A novel co-training technique to optimize multiple small student models generated from a large PLM using knowledge distillation under diversified views: model and data views produced by different distillation strategies and input augmentations, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12074v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12074v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12147",
    "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4",
    "latest": "2023-05-23T10:00:05+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110417412101749441",
      "content": "<p>LogiCoT: Logical Chain-of-Thought instruction-tuning data collection with GPT-4. ~ Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang. <a href=\"https://arxiv.org/abs/2305.12147\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12147</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/GPT4\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT4</span></a> <a href=\"https://mathstodon.xyz/tags/Logic\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Logic</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11252v1",
    "title": "Brain-inspired learning in artificial neural networks: a review",
    "latest": "2023-05-23T09:47:20+00:00",
    "last_post": {
      "url": "https://mastodon.social/@achterbrain/110417362350083227",
      "content": "<p>New <a href=\"https://mastodon.social/tags/review\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>review</span></a> led by Samuel Schmidgall:<br>'Brain-inspired learning in artificial neural networks'<br><a href=\"https://arxiv.org/abs/2305.11252v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11252v1</span><span class=\"invisible\"></span></a></p><p>This was a great <a href=\"https://mastodon.social/tags/collaboration\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>collaboration</span></a> supported by OpenBioML's new <a href=\"https://mastodon.social/tags/NeuroAI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuroAI</span></a> initiative! Looking forward to many more projects coming out of this <a href=\"https://mastodon.social/tags/open\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>open</span></a> collaborative <a href=\"https://mastodon.social/tags/research\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>research</span></a> community!</p><p><a href=\"https://mastodon.social/tags/neuroscience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>neuroscience</span></a> <a href=\"https://mastodon.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@achterbrain",
        "display_name": "Jascha Achterberg"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12057v1",
    "title": "https://arxiv.org/abs/2305.12057v1",
    "latest": "2023-05-23T09:40:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417334069138419",
      "content": "<p>\ud83d\udcdd Accurate Knowledge Distillation with N-Best Reranking \ud83d\udcda</p><p>\"Leverages a diverse set of models, including publicly available large pretrained models, to provide more accurate pseudo-labels for training student models via n-best reranking.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/fairseq/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/fa</span><span class=\"invisible\">irseq/</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12057v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12057v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.12073v1.pdf",
    "title": "https://arxiv.org/pdf/2305.12073v1.pdf",
    "latest": "2023-05-23T09:14:35+00:00",
    "last_post": {
      "url": "https://mastodon.social/@tiago_ribeiro/110417142228641639",
      "content": "<p>\"GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance\"</p><p>\"Our findings reinforce the exceptional performance of the GELU activation function, which attains the highest test accuracy and lowest test loss among the activation functions investigated. Other activation functions, such as Hardswish and ReLU6, exhibit commendable performance as well...\"</p><p><a href=\"https://mastodon.social/tags/GELU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GELU</span></a> <a href=\"https://mastodon.social/tags/ReLU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReLU</span></a> <a href=\"https://mastodon.social/tags/HardShrink\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>HardShrink</span></a> <a href=\"https://mastodon.social/tags/leakyReLU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>leakyReLU</span></a> <a href=\"https://mastodon.social/tags/ReLU6\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReLU6</span></a></p><p>\ud83d\udd17<a href=\"https://arxiv.org/pdf/2305.12073v1.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.12073v1.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12029v1",
    "title": "https://arxiv.org/abs/2305.12029v1",
    "latest": "2023-05-23T09:00:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417177104587367",
      "content": "<p>\ud83d\udcdd MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Introduces two models based on BERT and RoBERTa as baselines on this dataset, which can be used as a benchmark for future research to tackle this task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/huashen218/MultiTurnCleanup.git\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/huashen218/MultiTur</span><span class=\"invisible\">nCleanup.git</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12029v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12029v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12027v1",
    "title": "Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings",
    "latest": "2023-05-23T08:30:12+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417059049321121",
      "content": "<p>\ud83d\udcdd Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings \ud83d\udcda\ud83d\udc7e</p><p>\"DUCK infuses prior knowledge on entity types in entity representations by learning to place entities of similar type close to each other in the embedding space, using boxes and hyperspheres.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12027v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12027v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12018v1",
    "title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases",
    "latest": "2023-05-23T08:00:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416940920589699",
      "content": "<p>\ud83d\udcdd BOLT: Fast Energy-Based Controlled Text Generation with Tunable Biases \ud83d\udcda</p><p>\"Proposes BOLT which relies on tunable biases to adjust output logits directly to achieve fast convergence in controlled generation while maintaining the generator's autoregressive nature to assert a strong control on token-wise conditional dependencies and fluent output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12018v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12018v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12001v1",
    "title": "https://arxiv.org/abs/2305.12001v1",
    "latest": "2023-05-23T07:40:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416862157122454",
      "content": "<p>\ud83d\udcdd OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models \ud83d\udcda</p><p>\"Entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/metaseq\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/me</span><span class=\"invisible\">taseq</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12001v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12001v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12196",
    "title": "Experimental results from applying GPT-4 to an unpublished formal language",
    "latest": "2023-05-23T07:27:50+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110416776676268021",
      "content": "<p>Experimental results from applying GPT-4 to an unpublished formal language. ~ Gregor vom Scheidt (@GregorVScheidt). <a href=\"https://arxiv.org/abs/2305.12196\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12196</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/ChatGPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ChatGPT</span></a> <a href=\"https://mathstodon.xyz/tags/GPT4\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT4</span></a> <a href=\"https://mathstodon.xyz/tags/FunctionalProgramming\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>FunctionalProgramming</span></a> <a href=\"https://mathstodon.xyz/tags/Logic\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Logic</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11326",
    "title": "Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data",
    "latest": "2023-05-23T07:27:07+00:00",
    "last_post": {
      "url": "https://fediscience.org/@JordiCabot/110416810949851518",
      "content": "<p>Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data</p><p>&lt;- From a CSV to a fully functional chatbot </p><p>Paper: <a href=\"https://arxiv.org/abs/2305.11326\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11326</span><span class=\"invisible\"></span></a><br>Summary: <a href=\"https://livablesoftware.com/chatbots-open-data-project/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">livablesoftware.com/chatbots-o</span><span class=\"invisible\">pen-data-project/</span></a></p><p><a href=\"https://fediscience.org/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://fediscience.org/tags/csv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>csv</span></a> <a href=\"https://fediscience.org/tags/OpenData\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenData</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@JordiCabot",
        "display_name": "Jordi Cabot"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12000v1",
    "title": "Deep Learning Approaches to Lexical Simplification: A Survey",
    "latest": "2023-05-23T07:10:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416744250292836",
      "content": "<p>\ud83d\udcdd Deep Learning Approaches to Lexical Simplification: A Survey \ud83d\udcda</p><p>\"LS is the lexical component of Text Simplification (TS) with the aim of making texts more accessible to various target populations such as children, people with language and learning disorders, foreigners or people with low literacy.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12000v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12000v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2109.06096",
    "title": "https://arxiv.org/abs/2109.06096",
    "latest": "2023-05-23T06:38:47+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416620928970772",
      "content": "<p>As always a small model behaves just like an undertrained model<br><a href=\"https://twitter.com/LChoshen/status/1506245430912430091\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/LChoshen/status/15</span><span class=\"invisible\">06245430912430091</span></a><br><a href=\"https://arxiv.org/abs/2109.06096\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2109.06096</span><span class=\"invisible\"></span></a><br>So you can also pick an undertrained model, which would be better than a trained model in detecting who generated the text.</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2301.11305",
    "title": "https://arxiv.org/abs/2301.11305",
    "latest": "2023-05-23T06:33:49+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416601360081415",
      "content": "<p>First the problem, given a text you want to know whether a human wrote it. You've been in NLP lately I am sure a teacher, sister, nephew etc. called and told you they suspect someone handed them a GPT text.<br>Problem: how can you tell<br>The approach<br>Randomly replace words<br>Then see how much it changed the sentence probability\\likelihood</p><p>presented by <br><a href=\"https://arxiv.org/abs/2301.11305\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2301.11305</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09859",
    "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
    "latest": "2023-05-23T06:33:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416598590927915",
      "content": "<p>Opposite scaling law: detection of machine-generated text is done better by smaller models</p><p>Everyone (outside <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a>...) is afraid GPT would cheat for them, which pushes for detection methods</p><p><a href=\"https://arxiv.org/abs/2305.09859\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09859</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <a href=\"https://sigmoid.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11993v1",
    "title": "https://arxiv.org/abs/2305.11993v1",
    "latest": "2023-05-23T06:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416586994729304",
      "content": "<p>\ud83d\udcdd Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis \ud83d\udcda</p><p>\"A specialised Flan-T5 language model is trained to take usage examples and a usage cluster as input and output a natural language definition of that usage cluster as its output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ltgoslo/definition_modeling\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/ltgoslo/definition_</span><span class=\"invisible\">modeling</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11993v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11993v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11979v1",
    "title": "https://arxiv.org/abs/2305.11979v1",
    "latest": "2023-05-23T05:30:12+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416351236599275",
      "content": "<p>\ud83d\udcdd A Weak Supervision Approach for Few-Shot Aspect Based Sentiment \ud83d\udcda</p><p>\"Uses weak supervision on unlabeled data using the output of a simple baseline, and then fine-tune a pre-trained sequence-to-sequence model on the generated dataset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/robertvacareanu/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/robertvacareanu/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11979v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11979v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.10559",
    "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
    "latest": "2023-05-23T05:19:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416307592130037",
      "content": "<p>We also have seen claims ICL is quite similar to taking a gradient step<br>arxiv.org/abs/2211.15661<br><a href=\"https://arxiv.org/abs/2212.10559\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2212.10559</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2202.12837",
    "title": "http://arxiv.org/abs/2202.12837",
    "latest": "2023-05-23T05:18:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416304404784912",
      "content": "<p>So we have seen papers showing that models gain a lot from seeing examples (ICL) with random labels<br><a href=\"http://arxiv.org/abs/2202.12837\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2202.12837</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09731",
    "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
    "latest": "2023-05-23T05:17:36+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416301704135043",
      "content": "<p>In-Context-Learning == gradient descent or disregards labels completely?!<br>Why not both?</p><p>Models recognize the task but also learn it<br>&amp; The benefits of actual learning grow with # examples and model size</p><p> <br><a href=\"https://arxiv.org/abs/2305.09731\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09731</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11952v1",
    "title": "Self-QA: Unsupervised Knowledge Guided Language Model Alignment",
    "latest": "2023-05-23T05:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416233204460998",
      "content": "<p>\ud83d\udcdd Self-Qa: Unsupervised Knowledge Guided Language Model Alignment \ud83d\udcda</p><p>\"Self-QA replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge, enabling the model to generate a larger quantity of correct and domain-specific instruction data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11952v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11952v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11938v1",
    "title": "https://arxiv.org/abs/2305.11938v1",
    "latest": "2023-05-23T04:40:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416154477268328",
      "content": "<p>\ud83d\udcdd XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages \ud83d\udcda</p><p>\"XTREME-UP is a benchmark defined by three criteria: its focus on the scarce-data scenario rather than zero-shot (rather than few-shot); its focus on user-centric tasks rather than linguistic probing tasks; and its focus on under-represented languages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/google-research/xtreme-up\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/google-research/xtr</span><span class=\"invisible\">eme-up</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11938v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11938v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.02819",
    "title": "GPT detectors are biased against non-native English writers",
    "latest": "2023-05-23T04:32:53+00:00",
    "last_post": {
      "url": "https://river.group.lt/@rgb/110408082519926313",
      "content": "<p><a href=\"https://arxiv.org/abs/2304.02819\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.02819</span><span class=\"invisible\"></span></a><br>GPT detectors are biased against non-native English writers</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11916v1",
    "title": "F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks",
    "latest": "2023-05-23T04:20:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416075706791889",
      "content": "<p>\ud83d\udcdd F-Pabee: Flexible-Patience-Based Early Exiting for Single-Label and Multi-Label Text Classification Tasks \ud83d\udcda</p><p>\"A Flexible-Patience-Based Early Exiting Method (F-PABEE) has been proposed to alleviate the problems mentioned above for single-label classification (SLC) and multi-label classification (MLC) tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11916v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11916v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11862v1",
    "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models",
    "latest": "2023-05-23T03:42:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415926410383122",
      "content": "<p>\ud83d\udcdd Reducing Sequence Length by Predicting Edit Operations with Large Language Models \ud83d\udcda</p><p>\"A local sequence transduction task is represented as a sequence of edit operations on the input text, which is learned by instruction tuning for large language models (LLMs).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11862v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11862v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11826v1",
    "title": "STOAT: Structured Data to Analytical Text With Controls",
    "latest": "2023-05-23T03:22:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415847795639417",
      "content": "<p>\ud83d\udcdd STOAT: Structured Data to Analytical Text with Controls \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a reasoning category aware table-to-text generation model with vector quantization to infuse the reasoning category in the output descriptions and also to control the reasoning category generation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11826v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11826v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13048",
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "latest": "2023-05-23T02:12:40+00:00",
    "last_post": {
      "url": "https://genomic.social/@PhilippBayer/110415566437178829",
      "content": "<p>'RWKV: Reinventing RNNs for the Transformer Era'<br><a href=\"https://arxiv.org/abs/2305.13048\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13048</span><span class=\"invisible\"></span></a></p><p>'Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters.'</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@davidmortensen",
        "display_name": "David Mortensen"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11626v1",
    "title": "CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search",
    "latest": "2023-05-23T02:12:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415572481058584",
      "content": "<p>\ud83d\udcdd CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search \ud83d\udcda</p><p>\"A combination of GraphCodeBERT pre-training and cross-consistency training of a language model across different programming languages using XCD dataset as supervision signal.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11626v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11626v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11625v1",
    "title": "Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets",
    "latest": "2023-05-23T02:02:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415533142570361",
      "content": "<p>\ud83d\udcdd Searching by Code: A New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets \ud83d\udcda</p><p>\"SnippeR uses a single encoder for both the code snippet and the natural language query, and learns to rank the candidate answer by the cosine similarity of the two.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11625v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11625v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11744v1",
    "title": "Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval",
    "latest": "2023-05-23T01:52:58+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_ir/110415139983596635",
      "content": "<p>\ud83d\udcdd Inference-Time Re-Ranker Relevance Feedback for Neural Information Retrieval \ud83d\udcbf\ud83d\udcda</p><p>\"We update the retriever's query vector using a lightweight inference-time distillation of the re-ranker's prediction for that instance and perform a second retrieval with the updated query vector.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/IR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>IR</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11744v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11744v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://recsys.social/@karlhigley",
        "display_name": "Karl Higley"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11596v1",
    "title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation",
    "latest": "2023-05-23T01:32:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415415119788630",
      "content": "<p>\ud83d\udcdd Mitigating Backdoor Poisoning Attacks Through the Lens of Spurious Correlation \ud83d\udcda</p><p>\"Works by filtering out instances with potentially problematic correlations between features and labels, and using a subset of the rest to retrain the models and remove backdoor behaviours.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CR</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11596v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11596v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11553v1",
    "title": "https://arxiv.org/abs/2305.11553v1",
    "latest": "2023-05-23T01:12:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415336656025295",
      "content": "<p>\ud83d\udcdd Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information \ud83d\udcda</p><p>\"Considers each abstract as a recurrent cycle of sentences and place segmentation boundaries by greedily optimizing the NMI score between premises and conclusions in each cycle of sentences.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mediacloud/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mediacloud/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11553v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11553v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13304",
    "title": "https://arxiv.org/abs/2305.13304",
    "latest": "2023-05-23T01:12:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415336394152761",
      "content": "<p>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text</p><p>Generates a paragraph at each timestep and updates its language-based long-short term memory stored on the hard drive and the prompt, respectively. </p><p>repo: <a href=\"https://github.com/aiwaves-cn/RecurrentGPT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/aiwaves-cn/Recurren</span><span class=\"invisible\">tGPT</span></a> <br>abs: <a href=\"https://arxiv.org/abs/2305.13304\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13304</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13292",
    "title": "VideoLLM: Modeling Video Sequence with Large Language Models",
    "latest": "2023-05-23T01:03:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415301110279075",
      "content": "<p>VideoLLM: Modeling Video Sequence with Large Language Models</p><p>Leverages the sequence reasoning capabilities of pre-trained LLMs from NLP for video sequence understanding.</p><p><a href=\"https://arxiv.org/abs/2305.13292\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13292</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13301",
    "title": "Training Diffusion Models with Reinforcement Learning",
    "latest": "2023-05-23T01:00:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415289384934263",
      "content": "<p>Training Diffusion Models with Reinforcement Learning</p><p>Presents an RL-based framework for training denoising diffusion models to directly optimize a variety of reward functions</p><p><a href=\"https://arxiv.org/abs/2305.13301\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13301</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2303.12712.pdf",
    "title": "https://arxiv.org/pdf/2303.12712.pdf",
    "latest": "2023-05-23T00:42:46+00:00",
    "last_post": {
      "url": "https://techhub.social/@freemanwd/110415146468249466",
      "content": "<p>Microsoft researchers wonder if GPT4 is a form of AGI based on several experiments <br><a href=\"https://arxiv.org/pdf/2303.12712.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2303.12712.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@andrey",
        "display_name": "Andrey Kurenkov"
      },
      {
        "url": "https://fosstodon.org/@amueller",
        "display_name": "Andreas Mueller"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://genart.social/@tca",
        "display_name": "tiago"
      },
      {
        "url": "https://mastodon.social/@bruces",
        "display_name": "Bruce Sterling @bruces"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11449v1",
    "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast",
    "latest": "2023-05-23T00:32:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415179239334587",
      "content": "<p>\ud83d\udcdd Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-Tuning Slow and Fast \ud83d\udcda</p><p>\"Proposes a method named Fine-tuning slow and fast with four training policies to improve the performance gap for multilingual tasks, by reducing forgetting and improving the fine-tuning process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11449v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11449v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11251v1",
    "title": "Computational thematics: Comparing algorithms for clustering the genres of literary fiction",
    "latest": "2023-05-23T00:12:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415100611809059",
      "content": "<p>\ud83d\udcdd Computational Thematics: Comparing Algorithms for Clustering the Genres of Literary Fiction \ud83d\udcda</p><p>\"S are applied to a corpus of books belonging to four pre-tagged genres of fiction (Fantasy, Horror, Science Fiction, and Romance), and are then validated against the \"ground truth\" genre labels.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11251v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11251v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11243v1",
    "title": "Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses",
    "latest": "2023-05-23T00:02:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415061229184234",
      "content": "<p>\ud83d\udcdd Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses \ud83d\udcda\ud83d\udc7e</p><p>\"LaMDA is a large language model (LM), a type of AModels trained to predict the next word given a sequence of words, that was trained on a large number of webpages and books.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11243v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11243v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11231v1",
    "title": "Recent Trends in Unsupervised Summarization",
    "latest": "2023-05-22T23:22:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414903863283183",
      "content": "<p>\ud83d\udcdd Recent Trends in Unsupervised Summarization \ud83d\udcda</p><p>\"Unsupervised approaches learn to summarize from the input data and do not need labeled datasets for training, unlike supervised approaches that require labeled datasets for training the models to summarize the content.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11231v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11231v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2302.12173",
    "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
    "latest": "2023-05-22T23:05:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110414836730125554",
      "content": "<p>Compromising LLM-Integrated Applications with Indirect Prompt Injection - <a href=\"https://arxiv.org/abs/2302.12173\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2302.12173</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@raphaelmilliere",
        "display_name": "Rapha\u00ebl Milli\u00e8re"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206v1",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T23:02:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414825637860258",
      "content": "<p>\ud83d\udcdd LIMA: Less Is More for Alignment \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"LIMA is a 65B parameter language model trained for 150B tokens, using a supervised loss, from 1,000 carefully curated prompts and their responses.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11206v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11186v1",
    "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
    "latest": "2023-05-22T22:52:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414785973568218",
      "content": "<p>\ud83d\udcdd Compress, Then Prompt: Improving Accuracy-Efficiency Trade-Off of LLM Inference with Transferable Prompt \ud83d\udcda\ud83e\udde0</p><p>\"A prompt learning paradigm that cultivates an additive prompt over a compressed LLM to bolster their accuracy is presented and empirically tested in this work, shedding light on new possibilities for enhancing the balance between accuracy and efficiency in LLM inference.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11186v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11186v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11719v1",
    "title": "https://arxiv.org/abs/2305.11719v1",
    "latest": "2023-05-22T21:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414463541864356",
      "content": "<p>\ud83d\udcdd Information Screening Whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling \ud83d\udd2d\ud83d\udcda</p><p>\"Represents the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ChocoWu/MRE-ISE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/ChocoWu/MRE-ISE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11719v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11719v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/74djc/",
    "title": "http://osf.io/74djc/",
    "latest": "2023-05-22T21:04:05+00:00",
    "last_post": {
      "url": "https://botsin.space/@EdArXivBot/110395685386849135",
      "content": "<p>Unlocking Financial Success: Empowering Higher Ed Students and Developing Financial Literacy Interventions at Scale <a href=\"http://osf.io/74djc/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/74djc/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@dougholton",
        "display_name": "Doug Holton"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11845v1",
    "title": "https://arxiv.org/abs/2305.11845v1",
    "latest": "2023-05-22T20:18:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414180509844595",
      "content": "<p>\ud83d\udcdd RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"RxnScribe is an end-to-end model that takes as input a chemical reaction diagram and outputs a structured representation in a machine-readable format.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/thomas0809/RxnScribe\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/thomas0809/RxnScrib</span><span class=\"invisible\">e</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11845v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11845v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11806v1",
    "title": "https://arxiv.org/abs/2305.11806v1",
    "latest": "2023-05-22T19:54:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414086009856181",
      "content": "<p>\ud83d\udcdd The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics \ud83d\udcda</p><p>\"Develops and compare several explanation methods and demonstrate their effectiveness for interpreting state-of-the-art neural metrics based on BERT and ROBERTA fine-tuned on sentence-level human judgments from the WMT Metrics Shared Task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Unbabel/COMET\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Unbabel/COMET</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11806v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11806v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11790v1",
    "title": "Prompting with Pseudo-Code Instructions",
    "latest": "2023-05-22T19:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413944317609304",
      "content": "<p>\ud83d\udcdd Prompting with Pseudo-Code Instructions \ud83d\udcda</p><p>\"A pre-trained language model is fine-tuned using a prompt in the form of pseudocode instructions for a task in addition to the natural language instruction for the task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11790v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11790v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11789v1",
    "title": "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach",
    "latest": "2023-05-22T19:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413897105704111",
      "content": "<p>\ud83d\udcdd Solving NLP Problems Through Human-System Collaboration: A Discussion-Based Approach \ud83d\udcda</p><p>\"Creates a dataset with a human-in-the-loop setup where an annotator can discuss a natural language inference task with a chatbot and receive feedback.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11789v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11789v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41593-023-01333-4",
    "title": "Addressing the ethical and societal challenges posed by genome-wide association studies of behavioral and brain-related traits - Nature Neuroscience",
    "latest": "2023-05-22T18:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413833974408684",
      "content": "<p>Ethical, societal implications of genome-wide behavioral, brain-related traits - <a href=\"https://www.nature.com/articles/s41593-023-01333-4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41593-023</span><span class=\"invisible\">-01333-4</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778v1",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T18:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413802737748919",
      "content": "<p>\ud83d\udcdd Cross-Lingual Supervision Improves Large Language Models Pre-Training \ud83d\udcda\ud83e\udde0</p><p>\"A pre-training objective mixing a self-supervised language modeling and a supervised machine translation objectives, including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11778v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11761v1",
    "title": "ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation",
    "latest": "2023-05-22T18:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413755777159926",
      "content": "<p>\ud83d\udcdd ReSeTOX: Re-Learning Attention Weights for Toxicity Mitigation in Machine Translation \ud83d\udcda</p><p>\"Works by dynamically adjusting the key-value self-attention weights and re-evaluates the beam search hypotheses in case of identified added toxicity during the inference process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11761v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11761v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/u26ze/",
    "title": "http://osf.io/u26ze/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645261557408",
      "content": "<p>Gender and retention patterns among U.S. faculty <a href=\"http://osf.io/u26ze/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u26ze/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/u5kd2/",
    "title": "http://osf.io/u5kd2/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645213145496",
      "content": "<p>Power users: Canadian sex workers' use of technology post COVID <a href=\"http://osf.io/u5kd2/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u5kd2/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/7s9g4/",
    "title": "http://osf.io/7s9g4/",
    "latest": "2023-05-22T18:02:01+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645169835083",
      "content": "<p>Intelligent transport design with a dual focus <a href=\"http://osf.io/7s9g4/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/7s9g4/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/pqzux/",
    "title": "http://osf.io/pqzux/",
    "latest": "2023-05-22T17:56:07+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413621983917418",
      "content": "<p>Lithic raw materials provenance in the Caribbean islands: flint, jasper, obsidian and so on. <a href=\"http://osf.io/pqzux/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/pqzux/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11062",
    "title": "Scaling Transformer to 1M tokens and beyond with RMT",
    "latest": "2023-05-22T17:53:06+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@mikeiavelli/110254216906691830",
      "content": "<p>Recurrent Memory Transformer (RMT) retains information across up to 2 million tokens (!) \u2b50, significantly exceeding the largest input size reported for transformer models (64K tokens) and GPT-4's 32K tokens.</p><p>Paper: <a href=\"https://arxiv.org/abs/2304.11062\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11062</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/booydar/t5-experiments/tree/scaling-report\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/booydar/t5-experime</span><span class=\"invisible\">nts/tree/scaling-report</span></a></p><p><a href=\"https://mathstodon.xyz/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> <a href=\"https://mathstodon.xyz/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <br><a href=\"https://mathstodon.xyz/tags/NeuralNetworks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuralNetworks</span></a> <a href=\"https://mathstodon.xyz/tags/NN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NN</span></a><br><a href=\"https://mathstodon.xyz/tags/DeepLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DeepLearning</span></a><br><a href=\"https://mathstodon.xyz/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mathstodon.xyz/tags/Transformers\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Transformers</span></a> <a href=\"https://mathstodon.xyz/tags/RNN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>RNN</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://mastodon.online/@vladiliescu",
        "display_name": "Vlad Iliescu \ud83d\udc2c"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01702-w",
    "title": "When will global warming actually hit the landmark 1.5 \u00baC limit?",
    "latest": "2023-05-22T17:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413598086407934",
      "content": "<p>When will global warming hit the landmark 1.5 \u00baC limit? - <a href=\"https://www.nature.com/articles/d41586-023-01702-w\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01702-w</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11554",
    "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
    "latest": "2023-05-22T17:23:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110413493153748217",
      "content": "<p>RT @Ber18791531@twitter.com</p><p>\ud83e\udd14ChatGPT plug-in is impressive, but is \"learning tools in the context\" the ultimate solution?<br>Check out our ToolkenGPT (<a href=\"https://arxiv.org/abs/2305.11554\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11554</span><span class=\"invisible\"></span></a>), which can handle massive tools and understand them better with new \"toolken\" embeddings.</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Ber18791531/status/1660520584382672897\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Ber18791531/status</span><span class=\"invisible\">/1660520584382672897</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11595v1",
    "title": "Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate",
    "latest": "2023-05-22T17:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413425257031910",
      "content": "<p>\ud83d\udcdd Diving Into the Inter-Consistency of Large Language Models: An Insightful Analysis Through Debate \ud83d\udcda\ud83d\udc7e</p><p>\"Designs a formal debate framework with three-stage debate including the fair debate, mismatched debate, and roundtable debate stages for inter-consistency learning between LLMs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11595v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11595v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11579v1",
    "title": "https://arxiv.org/abs/2305.11579v1",
    "latest": "2023-05-22T16:42:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413331017830359",
      "content": "<p>\ud83d\udcdd Speech-Text Dialog Pre-Training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment \ud83d\udcda\ud83d\udc7e</p><p>\"First pre-trains on speech-text dialogs via response selection task and temporal position prediction task, and then fine-tunes the pre-trained model on downstream STD tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/AlibabaResearch/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/AlibabaResearch/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11579v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11579v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05335",
    "title": "https://doi.org/10.21105/joss.05335",
    "latest": "2023-05-22T16:30:36+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110413285723110427",
      "content": "<p>Just published in JOSS: 'chombo-discharge: An AMR code for gas discharge simulations in complex geometries' <a href=\"https://doi.org/10.21105/joss.05335\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05335</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.jtrangeo.2023.103590",
    "title": "doi.org/10.1016/j.jtrangeo.202...",
    "latest": "2023-05-22T16:19:41+00:00",
    "last_post": {
      "url": "https://datasci.social/@UrbanDemog/110413146017535927",
      "content": "<p>New paper \"Evaluating the impact of public transport travel time inaccuracy and variability on socio-spatial inequalities in accessibility\", out in the Journal of Transport Geography. A great study led by Kaue Braga.<br>Paper: <a href=\"https://doi.org/10.1016/j.jtrangeo.2023.103590\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.jtrangeo.202</span><span class=\"invisible\">3.103590</span></a><br>\ud83d\udd13ungated PDF: <a href=\"https://www.urbandemographics.org/publication/2023_jtg_public_transport_travel_time_inaccuracy_variability/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">urbandemographics.org/publicat</span><span class=\"invisible\">ion/2023_jtg_public_transport_travel_time_inaccuracy_variability/</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11564v1",
    "title": "https://arxiv.org/abs/2305.11564v1",
    "latest": "2023-05-22T16:06:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413189526338211",
      "content": "<p>\ud83d\udcdd Decouple Knowledge From Paramters for Plug-and-Play Language Modeling \ud83d\udcda\ud83d\udc7e</p><p>\"The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Hannibal046/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Hannibal046/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11564v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11564v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11438v1",
    "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
    "latest": "2023-05-22T09:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411679334789432",
      "content": "<p>\ud83d\udcdd Phonetic and Prosody-Aware Self-Supervised Learning Approach for Non-Native Fluency Scoring \ud83d\udcda</p><p>\"A self-supervised approach for automatic fluency scoring is proposed that takes into account the phonetic and prosody awareness during both pre-training and fine-tuning stages to improve the performance of the model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11438v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11438v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11426v1",
    "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
    "latest": "2023-05-22T09:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411584964283083",
      "content": "<p>\ud83d\udcdd Post Hoc Explanations of Language Models Can Improve Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"AMPLIFY constructs natural language rationales from post hoc explanations to provide corrective signals to LLMs during in-context learning, resulting in prediction accuracy improvements of about 10-25% over a wide range of tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11426v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11426v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://osf.io/preprints/socarxiv/a7udw/",
    "title": "osf.io/preprints/socarxiv/a7ud...",
    "latest": "2023-05-22T09:02:43+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@rockberta/110411524554763096",
      "content": "<p>Hey there! New preprint where we use contextualized topic models, automated linguistic feature extraction and predictive modeling to look at how online communication by the European Commission has changed over time \ud83d\udcc3\ud83e\udd16\ud83d\udcc8</p><p>TL;DR: we show that EC has radically reshaped its projected online identity, moving towards less technocratic topics &amp; style, defining a more \"unique\" profile among comparable institutions &amp; aligning better with its audience.</p><p><a href=\"https://osf.io/preprints/socarxiv/a7udw/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">osf.io/preprints/socarxiv/a7ud</span><span class=\"invisible\">w/</span></a> <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/polsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>polsci</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@rockberta",
        "display_name": "Roberta Rocca"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.09848",
    "title": "Evaluating Verifiability in Generative Search Engines",
    "latest": "2023-05-22T08:52:24+00:00",
    "last_post": {
      "url": "https://qoto.org/@tedherman/110250616501896621",
      "content": "<p><span class=\"h-card\"><a href=\"https://fediscience.org/@muratdemirbas\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>muratdemirbas</span></a></span> This paper discusses the problem of verifying results of generative transformers - <a href=\"https://arxiv.org/abs/2304.09848\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.09848</span><span class=\"invisible\"></span></a> </p><p>My question: wasn't blockchain supposed to solve all the problems of verifiability?</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@Yarin",
        "display_name": "Yarin :verified: :verified:"
      },
      {
        "url": "https://sigmoid.social/@Riedl",
        "display_name": "Mark Riedl"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://aleph.land/@mtriclot",
        "display_name": "Mathieu Triclot"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11374v1",
    "title": "https://arxiv.org/abs/2305.11374v1",
    "latest": "2023-05-22T08:18:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411349164932293",
      "content": "<p>\ud83d\udcdd Characterizing Tradeoffs Between Teaching via Language and Demonstrations in Multi-Agent Systems \ud83d\udcda</p><p>\"Finds that teaching by demonstration is more effective in the simplest settings, but language is more effective as task difficulty increases, due to its ability to generalize more effectively to unseen scenarios.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/dharakyu/language-and-demos\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/dharakyu/language-a</span><span class=\"invisible\">nd-demos</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11374v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11374v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11364v1",
    "title": "https://arxiv.org/abs/2305.11364v1",
    "latest": "2023-05-22T08:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411301875994659",
      "content": "<p>\ud83d\udcdd Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"Presents LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets, which clusters text along syntactic, lexical, and semantic axes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/PAIR-code/interpretability\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/PAIR-code/interpret</span><span class=\"invisible\">ability</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11364v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11364v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11355v1",
    "title": "https://arxiv.org/abs/2305.11355v1",
    "latest": "2023-05-22T07:54:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411254744145126",
      "content": "<p>\ud83d\udcdd MD3: The Multi-Dialect Dataset of Dialogues \ud83d\udcda</p><p>\"A dataset of conversational speech from three major English-speaking countries: India, Nigeria, and the United States, with more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/nehasinha/Taboo/blob/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/nehasinha/Taboo/blo</span><span class=\"invisible\">b/</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11355v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11355v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09137",
    "title": "https://arxiv.org/abs/2305.09137",
    "latest": "2023-05-22T07:42:47+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381332490398862",
      "content": "<p>Pre-Training to Learn in Context</p><p>Proposes PICL, a framework to enhance the LMs' in-context learning ability by pre-training on \"intrinsic tasks\" in the general plain-text corpus using the simple LM objective.</p><p>repo: <a href=\"https://github.com/thu-coai/PICL\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/thu-coai/PICL</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.09137\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09137</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11334v1",
    "title": "Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs",
    "latest": "2023-05-22T07:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411113215008062",
      "content": "<p>\ud83d\udcdd Writing Your Own Book: A Method for Going From Closed to Open Book QA to Improve Robustness and Performance of Smaller LLMs \ud83d\udcda\ud83d\udc7e</p><p>\"Introduces two novel methods, Tree-Search and Self-contextualizing Question-Answering, designed to enhance the performance of large language models (LLMs) in question-answering tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11334v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11334v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11326v1",
    "title": "Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data",
    "latest": "2023-05-22T06:42:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410971702391662",
      "content": "<p>\ud83d\udcdd Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data \ud83d\udcda</p><p>\"Proposes a method to generate a chatbot from tabular data sources using natural language processing (NLP) and machine learning (ML) techniques, and to deploy it to offer citizens interactive access to public data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11326v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11326v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11317v1",
    "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
    "latest": "2023-05-22T06:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410924603280206",
      "content": "<p>\ud83d\udcdd Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation \ud83d\udcda</p><p>\"A large-scale language model that was trained on an open-domain corpus was used to edit the input prompt to improve the quality and consistency of image outputs by the state-of-the-art Text-to-Image (T2I) models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11317v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11317v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11315v1",
    "title": "https://arxiv.org/abs/2305.11315v1",
    "latest": "2023-05-22T05:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410735679563419",
      "content": "<p>\ud83d\udcdd Improving Toponym Resolution with Better Candidate Generation, Transformer-Based Reranking, and Two-Stage Resolution \ud83d\udcda</p><p>\"GeoNorm first uses information retrieval techniques to generate a list of candidate entries from the geospatial ontology, then reranks the candidate entries using a transformer-based neural network that incorporates information from the ontology such as the entry's population.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clulab/geonorm\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/clulab/geonorm</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11315v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11315v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125",
    "title": "Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity",
    "latest": "2023-05-22T05:27:17+00:00",
    "last_post": {
      "url": "https://mastodon.social/@devezer/110373663709370598",
      "content": "<p>And while epistemic pluralism may not be perfect or guarantee immediate progress, it should protect us from far worse excesses. Incidentally in our 2019 paper our computational model of  scientific process suggests that epistemic diversity is powerful in that it prevents worst-case scenarios and allows for scientific discovery while avoiding many traps. <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0216125</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@FroehlichMarcel",
        "display_name": "Marcel Fr\u00f6hlich"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11255v1",
    "title": "https://arxiv.org/abs/2305.11255v1",
    "latest": "2023-05-22T05:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410641360816775",
      "content": "<p>\ud83d\udcdd Reasoning Implicit Sentiment with Chain-of-Thought Prompting \ud83d\udcda</p><p>\"A three-step prompting principle is designed for THOR to guide the model to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/scofield7419/THOR-ISA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/scofield7419/THOR-I</span><span class=\"invisible\">SA</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11255v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11255v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T04:26:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110410436966499091",
      "content": "<p>Cross-Lingual Supervision improves Large Language Models Pre-training</p><p>Pretraining LLMs on a mixture of a regular LM dataset + cross-lingual parallel data yields models with better in-context learning abilities. </p><p><a href=\"https://arxiv.org/abs/2305.11778\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09612",
    "title": "Large Language Models are Built-in Autoregressive Search Engines",
    "latest": "2023-05-22T03:42:39+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381297041837722",
      "content": "<p>Large Language Models are Built-in Autoregressive Search Engines</p><p>When providing a few Query-URL pairs as in-context demonstrations, LLMs can generate Web URLs where ~90% of the corresponding documents contain correct answers to open-domain questions.</p><p><a href=\"https://arxiv.org/abs/2305.09612\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09612</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2102.07350.pdf",
    "title": "https://arxiv.org/pdf/2102.07350.pdf",
    "latest": "2023-05-22T02:49:21+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110410056412505522",
      "content": "<p>also see \"Prompt Programming for Large Language Models:<br>Beyond the Few-Shot Paradigm\"</p><p> Informed by this more encompassing theory of<br>prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its<br>own natural language prompts for a range of tasks</p><p><a href=\"https://arxiv.org/pdf/2102.07350.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2102.07350.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mastodon.radio/tags/prompts\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>prompts</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.11430.pdf",
    "title": "https://arxiv.org/pdf/2305.11430.pdf",
    "latest": "2023-05-22T02:22:01+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110409948972775616",
      "content": "<p>\"TELeR: A General Taxonomy of LLM Prompts for Benchmarking<br>Complex Tasks\"</p><p><a href=\"https://arxiv.org/pdf/2305.11430.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.11430.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.00118",
    "title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",
    "latest": "2023-05-22T01:44:35+00:00",
    "last_post": {
      "url": "https://paquita.masto.host/@no_tan_incendiario/110343931803514302",
      "content": "<p>Han descubierto que ChatGPT ha \"\"memorizado\"\" m\u00e1s de 100 libros. A ver cu\u00e1ndo las editoriales afilan sus cuchillos. <a href=\"https://arxiv.org/abs/2305.00118\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.00118</span><span class=\"invisible\"></span></a> La lista de libros: <a href=\"https://docs.google.com/spreadsheets/d/1jW7EhsNjIGDMoK2JidyDD7UXH9N0NpEJfWFEj05_LC4/edit#gid=0\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">docs.google.com/spreadsheets/d</span><span class=\"invisible\">/1jW7EhsNjIGDMoK2JidyDD7UXH9N0NpEJfWFEj05_LC4/edit#gid=0</span></a> (fuente: <a href=\"https://github.com/bamman-group/gpt4-books\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/bamman-group/gpt4-b</span><span class=\"invisible\">ooks</span></a> )</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@dbamman",
        "display_name": "David Bamman"
      },
      {
        "url": "https://fediscience.org/@UlrichJunker",
        "display_name": "Ulrich Junker"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch (moderator)"
      },
      {
        "url": "https://mastodon.social/@jose_eduardo",
        "display_name": "Jos\u00e9 Eduardo Gonz\u00e1lez"
      },
      {
        "url": "https://sigmoid.social/@roban",
        "display_name": "Roban Hultman Kramer"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09515",
    "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
    "latest": "2023-05-22T01:42:35+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381269465767328",
      "content": "<p>AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation</p><p>On text summarization, NMT, and common sense generation, AR-Diffusion outperforms existing diffusion LMs and that it can be 100\u00d7 \u223c 600\u00d7 faster when achieving comparable results</p><p><a href=\"https://arxiv.org/abs/2305.09515\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09515</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T01:36:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409769364451473",
      "content": "<p>LIMA: Less Is More for Alignment</p><p>Presents LIMA, a 65B LLaMa language model fine-tuned on only 1k curated samples w/o RLHF. </p><p>Outputs from LIMA are either equivalent or strictly preferred to GPT-4 or Bard in 43% or 58% of cases, respectively.</p><p><a href=\"https://arxiv.org/abs/2305.11206\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01521-z",
    "title": "Humans and algorithms work together \u2014 so study them together",
    "latest": "2023-05-22T01:27:48+00:00",
    "last_post": {
      "url": "https://social.coop/@natematias/110350424967237041",
      "content": "<p>The US Supreme Court will soon decide if YouTube should be held responsible for allegedly permitting the platform\u2019s recommender algorithms to promote content that radicalized terrorists who killed Nohemi Gonzalez.</p><p>Whatever the court concludes, the case highlights an urgent question: how can we govern adaptive algorithms that continually change in response to people\u2019s behavior? The problem? Scientists can't currently answer this question.</p><p>See my new article in <span class=\"h-card\"><a href=\"https://sciencemastodon.com/@nature\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>nature</span></a></span> </p><p><a href=\"https://www.nature.com/articles/d41586-023-01521-z\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01521-z</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://social.coop/@natematias",
        "display_name": "J. Nathan Matias \ud83e\udda3"
      },
      {
        "url": "https://mstdn.social/@kissane",
        "display_name": "Erin Kissane"
      },
      {
        "url": "https://mastodon.social/@markcmarino",
        "display_name": "Mark C Marino"
      },
      {
        "url": "https://mastodon.social/@tiffanycli",
        "display_name": "Tiffany Li"
      },
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      },
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://fosstodon.org/@simon_goRin",
        "display_name": "Simon Gorin"
      },
      {
        "url": "https://hcommons.social/@julsraemy",
        "display_name": "Julien A. Raemy"
      },
      {
        "url": "https://social.coop/@dphiffer",
        "display_name": "Dan Phiffer"
      },
      {
        "url": "https://dair-community.social/@timnitGebru",
        "display_name": "Timnit Gebru (she/her)"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11738",
    "title": "https://arxiv.org/abs/2305.11738",
    "latest": "2023-05-22T01:24:23+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409722329971431",
      "content": "<p>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</p><p>repo: <a href=\"https://github.com/microsoft/ProphetNet/tree/master/CRITIC\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/ProphetNe</span><span class=\"invisible\">t/tree/master/CRITIC</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11738\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11738</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11675",
    "title": "https://arxiv.org/abs/2305.11675",
    "latest": "2023-05-22T01:18:22+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409698691968971",
      "content": "<p>Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity</p><p>proj: <a href=\"https://mind-video.com/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">mind-video.com/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11675\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11675</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11598",
    "title": "Introspective Tips: Large Language Model for In-Context Decision Making",
    "latest": "2023-05-22T00:53:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409600292246753",
      "content": "<p>Introspective Tips: Large Language Model for In-Context Decision Making</p><p><a href=\"https://arxiv.org/abs/2305.11598\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11598</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11841",
    "title": "How Does Generative Retrieval Scale to Millions of Passages?",
    "latest": "2023-05-22T00:40:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409549149527403",
      "content": "<p>How Does Generative Retrieval Scale to Millions of Passages?</p><p>Finds that the use of synthetic queries as a document representation strategy is the only approach that remained effective as they scaled up the corpus size using MS MARCO passages.</p><p><a href=\"https://arxiv.org/abs/2305.11841\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11841</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11846",
    "title": "https://arxiv.org/abs/2305.11846",
    "latest": "2023-05-22T00:37:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409537321914267",
      "content": "<p>Any-to-Any Generation via Composable Diffusion</p><p>Present CoDi, a novel generative model capable of generating any combination of output modalities from any combination of input modalities.</p><p>proj: <a href=\"https://codi-gen.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">codi-gen.github.io/</span><span class=\"invisible\"></span></a> <br>repo: <a href=\"https://github.com/microsoft/i-Code/tree/main/i-Code-V3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/i-Code/tr</span><span class=\"invisible\">ee/main/i-Code-V3</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11846\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11846</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11854",
    "title": "https://arxiv.org/abs/2305.11854",
    "latest": "2023-05-22T00:35:22+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409529572549613",
      "content": "<p>Multimodal Web Navigation with Instruction-Finetuned Foundation Models</p><p>Their 3B model outperforms the SoTA, PaLM-540B, on the WebShop benchmark. </p><p>Will make their 347K high-quality demonstrations publicly available.</p><p>proj: <a href=\"https://sites.google.com/view/mm-webnav/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">sites.google.com/view/mm-webna</span><span class=\"invisible\">v/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11854\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11854</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11863",
    "title": "Scaling laws for language encoding models in fMRI",
    "latest": "2023-05-22T00:30:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409509792875032",
      "content": "<p>Scaling laws for language encoding models in fMRI</p><p>Increasing scale in both models and data yields incredibly effective models of language processing in the brain, enabling better scientific understanding as well as applications such as decoding.</p><p><a href=\"https://arxiv.org/abs/2305.11863\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11863</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01612-x",
    "title": "For chemists, the AI revolution has yet to happen",
    "latest": "2023-05-21T23:45:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110409331655826124",
      "content": "<p>For chemists, the AI revolution has yet to happen - <a href=\"https://www.nature.com/articles/d41586-023-01612-x\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01612-x</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://osf.io/preprints/metaarxiv/fhdbs/",
    "title": "osf.io/preprints/metaarxiv/fhd...",
    "latest": "2023-05-21T23:39:40+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mcaleerp/110407615890521356",
      "content": "<p>Interesting paper in MetaArXiv: \u201cPreregistration in practice: A comparison of preregistered and non-preregistered studies in psychology\u201c. Main finding is whilst PreReg studies are more likely to state power analysis, inconsistent with previous findings, PreReg and non PreReg studies have similar amount of positive findings. Suggesting either PreReg doesn\u2019t reduce issues or issues are reduced in non PreReg studies these days. </p><p>Link: <a href=\"https://osf.io/preprints/metaarxiv/fhdbs/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">osf.io/preprints/metaarxiv/fhd</span><span class=\"invisible\">bs/</span></a></p><p><a href=\"https://mastodon.social/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a> <a href=\"https://mastodon.social/tags/MetaScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MetaScience</span></a> <a href=\"https://mastodon.social/tags/Psychology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Psychology</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.15056",
    "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks",
    "latest": "2023-05-21T22:23:10+00:00",
    "last_post": {
      "url": "https://qoto.org/@tedherman/110409008416499631",
      "content": "<p>Mechanical Turks considered in danger due to LLM/Chat tech. <a href=\"https://arxiv.org/abs/2303.15056\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.15056</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2205.09459",
    "title": "Neural Network Architecture Beyond Width and Depth",
    "latest": "2023-05-21T20:30:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110408564914038079",
      "content": "<p>Neural Network Architecture Beyond Width and Depth - <a href=\"https://arxiv.org/abs/2205.09459\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2205.09459</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s11229-023-04158-7",
    "title": "Epistemic diversity and industrial selection bias - Synthese",
    "latest": "2023-05-21T17:08:11+00:00",
    "last_post": {
      "url": "https://fediscience.org/@ct_bergstrom/110397512520315053",
      "content": "<p>Corporations can influence the directions and conclusions of academic research without corrupting the beliefs of any individual scientist. In a phenomenon known as industry selection bias, companies direct funding and/or data toward scientists who already support the research approaches or technological solutions favored by industry. </p><p>A new paper out in Synthese expands on the earlier Holman and Bruner model of this scenario to illustrate how this process works.</p><p><a href=\"https://link.springer.com/article/10.1007/s11229-023-04158-7\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s11229-023-04158-7</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://nerdculture.de/@dornhaus",
        "display_name": "Anna Dornhaus"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11162",
    "title": "High-Performance Graph Databases That Are Portable, Programmable, and Scale to Hundreds of Thousands of Cores",
    "latest": "2023-05-21T16:30:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110407621156510444",
      "content": "<p>High-Performance Graph Databases, Scaling to Hundreds of Thousands of Cores - <a href=\"https://arxiv.org/abs/2305.11162\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11162</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07759",
    "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
    "latest": "2023-05-21T16:23:02+00:00",
    "last_post": {
      "url": "https://mastodon.online/@jchyip/110386344853645107",
      "content": "<p><a href=\"https://mastodon.online/tags/TinyStories\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>TinyStories</span></a>: How Small Can Language Models Be and Still Speak Coherent English <a href=\"https://arxiv.org/abs/2305.07759\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07759</span><span class=\"invisible\"></span></a> <a href=\"https://mastodon.online/tags/ai\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ai</span></a> <a href=\"https://mastodon.online/tags/llm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>llm</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@bergi",
        "display_name": "Thomas Bergwinkl"
      }
    ]
  },
  {
    "link": "https://frontiersin.org/articles/10.3389/fpubh.2023.1221666/full",
    "title": "frontiersin.org/articles/10.33...",
    "latest": "2023-05-21T15:25:21+00:00",
    "last_post": {
      "url": "https://noc.social/@CaulfieldTim/110402373651558195",
      "content": "<p>Often referenced \"masks do harm\" study retracted.  </p><p>See: <a href=\"https://frontiersin.org/articles/10.3389/fpubh.2023.1221666/full\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">frontiersin.org/articles/10.33</span><span class=\"invisible\">89/fpubh.2023.1221666/full</span></a></p><p>Editors: \"...complaints were valid ... the article does not meet the standards of editorial &amp; scientific soundness...\"</p><p>Will likely live on as a zombie &amp; the retraction will become part of a conspiracy narrative.</p><p><a href=\"https://noc.social/tags/publichealth\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>publichealth</span></a> <a href=\"https://noc.social/tags/masks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>masks</span></a> <a href=\"https://noc.social/tags/scicomm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>scicomm</span></a> <a href=\"https://noc.social/tags/vaccineswork\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>vaccineswork</span></a> <a href=\"https://noc.social/tags/science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>science</span></a> <a href=\"https://noc.social/tags/retraction\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>retraction</span></a></p>"
    },
    "people": [
      {
        "url": "https://twit.social/@glennf",
        "display_name": "Glenn Fleishman"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41586-020-2487-2",
    "title": "Native American gene flow into Polynesia predating Easter Island settlement - Nature",
    "latest": "2023-05-21T15:17:03+00:00",
    "last_post": {
      "url": "https://social.sanfranciscan.org/objects/8971a192-bac6-4122-b369-24fca39e744f",
      "content": "<p>I\u2019ve always considered the sweet potato in Polynesia to be the \u201csmoking gun\u201d when it came to Pre-Columbian contact between the Americas and the \u201cOld World\u201d.</p><p>Now, genetic evidence appears to back up this theory. The article below concludes:</p><p>\u201c[We] find strong genetic evidence for pre-Columbian human trans-Pacific voyaging contact (at the turn of the twelfth century), contemporaneous with the Polynesian voyages of discovery in the remote eastern Pacific.\u201c</p><p><a href=\"https://www.nature.com/articles/s41586-020-2487-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">https://www.nature.com/articles/s41586-020-2487-2</a></p>"
    },
    "people": [
      {
        "url": "https://thefolklore.cafe/@juergen_hubert",
        "display_name": "J\u00fcrgen Hubert"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://vis.social/@mgiraldo",
        "display_name": "@mgiraldo"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s42254-023-00595-y",
    "title": "A new frontier for Hopfield networks - Nature Reviews Physics",
    "latest": "2023-05-21T14:52:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110402192452069793",
      "content": "<p>RT @DimaKrotov@twitter.com</p><p>Recent advances in Hopfield networks of associative memory may be the guiding theoretical principle for designing novel large scale neural architectures. I explain my enthusiasm about these ideas in the article \u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f. Please let me know what you think. <a href=\"https://www.nature.com/articles/s42254-023-00595-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s42254-023</span><span class=\"invisible\">-00595-y</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/DimaKrotov/status/1659526803155714053\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/DimaKrotov/status/</span><span class=\"invisible\">1659526803155714053</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21248/jlcl.36.2023.236",
    "title": "doi.org/10.21248/jlcl.36.2023....",
    "latest": "2023-05-21T14:44:49+00:00",
    "last_post": {
      "url": "https://mastodon.social/@fussballinguist/110406313988183627",
      "content": "<p>Our paper on \"Keyness in Song Lyrics\" is out: Jan Langenhorst, Yannick Frommherz and I discuss keyword analysis as an approach to computational stylistics by the example of song lyrics as highly clumpy data. We show that traditional bag-of-words-approached fail because of the repetitiveness of song lyrics and explore the advances of dispersion-oriented methods. <a href=\"https://mastodon.social/tags/openaccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>openaccess</span></a> <a href=\"https://doi.org/10.21248/jlcl.36.2023.236\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.21248/jlcl.36.2023.</span><span class=\"invisible\">236</span></a>.</p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@sascha_wolfer",
        "display_name": "Sascha Wolfer"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1004226",
    "title": "SARS-CoV-2 transmission with and without mask wearing or air cleaners in schools in Switzerland: A modeling study of epidemiological, environmental, and molecular data",
    "latest": "2023-05-21T14:37:30+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mackayim2022/110406167335561539",
      "content": "<p>SARS-CoV-2 transmission with and without mask wearing or air cleaners in schools in Switzerland: A modeling study of epidemiological, environmental, and molecular data. <br><a href=\"https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1004226\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosmedicine</span><span class=\"invisible\">/article?id=10.1371/journal.pmed.1004226</span></a></p>"
    },
    "people": [
      {
        "url": "https://zirk.us/@mrxmrt",
        "display_name": "Marcus"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11072v1",
    "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
    "latest": "2023-05-21T09:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405994038173144",
      "content": "<p>\ud83d\udcdd Self-Supervised Fine-Tuning for Improved Content Representations by Speaker-Invariant Clustering \ud83d\udcda</p><p>\"Proposes speaker-invariant clustering (Spin) that disentangles speaker information and preserves content representations for pre-trained networks with just 45 minutes of fine-tuning on a single GPU.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11072v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11072v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11068v1",
    "title": "ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph",
    "latest": "2023-05-21T09:12:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405899711320680",
      "content": "<p>\ud83d\udcdd ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph \ud83d\udcda\ud83d\udc7e</p><p>\"Orkg-Leaderboards uses a combination of machine-learning and rule-based approaches to extract Task-Dataset-Metric tuples from scholarly papers in AI domain.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11068v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11068v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11038v1",
    "title": "https://arxiv.org/abs/2305.11038v1",
    "latest": "2023-05-21T08:12:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405663906727945",
      "content": "<p>\ud83d\udcdd Learning in-Context Learning for Named Entity Recognition \ud83d\udcda</p><p>\"A new extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, and a meta-function pre-training algorithm is proposed to pre-train PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/chen700564/metaner-icl\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/chen700564/metaner-</span><span class=\"invisible\">icl</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11038v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11038v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08596",
    "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
    "latest": "2023-05-21T07:55:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110405596165938435",
      "content": "<p>DarkBERT: A Language Model for the Dark Side of the Internet - <a href=\"https://arxiv.org/abs/2305.08596\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08596</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11082",
    "title": "Fundamental Limitations of Alignment in Large Language Models",
    "latest": "2023-05-21T07:01:50+00:00",
    "last_post": {
      "url": "https://scholar.social/@joakinen/110266044734622865",
      "content": "<p>\"We propose a theoretical approach called Behavior Expectation Bounds (BEB) to investigate several inherent characteristics and limitations of <a href=\"https://scholar.social/tags/alignment\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>alignment</span></a> in <a href=\"https://scholar.social/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a>. We prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt.\"</p><p>Fundamental Limitations of Alignment in Large Language Models<br><a href=\"https://arxiv.org/abs/2304.11082\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11082</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11029v1",
    "title": "https://arxiv.org/abs/2305.11029v1",
    "latest": "2023-05-21T07:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405380817866757",
      "content": "<p>\ud83d\udcdd Uncertainty Guided Label Denoising for Document-Level Distant Relation Extraction \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a document-level distant relation extraction framework, UGDRE, with uncertainty guided label denoising (UGLD) for reducing noise and retaining high-quality labels.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/QiSun123/UGDRE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/QiSun123/UGDRE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11029v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11029v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/full/10.1080/0950236X.2012.638759",
    "title": "tandfonline.com/doi/full/10.10...",
    "latest": "2023-05-21T06:37:28+00:00",
    "last_post": {
      "url": "https://ravenation.club/@mpe/110405291140999411",
      "content": "<p>It's not open access, but the Textual Practice issue on Amis's _Money_ from a few years ago, edited by my colleague Joe Brooker, is worth a read. <br><a href=\"https://www.tandfonline.com/doi/full/10.1080/0950236X.2012.638759\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/full/10.10</span><span class=\"invisible\">80/0950236X.2012.638759</span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/epdf/10.1080/17513472.2023.2191572",
    "title": "tandfonline.com/doi/epdf/10.10...",
    "latest": "2023-05-21T06:36:16.343000+00:00",
    "last_post": {
      "url": "https://ciberlandia.pt/@villares/110123876632324473",
      "content": "<p>Research Article<br>Knitted origami<br>Elizabeth L. Wilmer</p><p>\"Techniques are presented for embedding horizontal, vertical, and45\u25e6diagonal crease lines into garter stitch knitted fabric. While theseare mostly based on standard lace knitting stitches, the horizon-tal creases use double-cable-crossed elongated stitches in a non-standard way. This crease library suffices to knit a model of a squaretwist, a foundational origami tessellation unit.\"</p><p><a href=\"https://www.tandfonline.com/doi/epdf/10.1080/17513472.2023.2191572\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/epdf/10.10</span><span class=\"invisible\">80/17513472.2023.2191572</span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@AlgoCompSynth",
        "display_name": "AlgoCompSynth by znmeb #MaskUp"
      },
      {
        "url": "https://vis.social/@Justin_Lind",
        "display_name": "Justin Lind"
      },
      {
        "url": "https://dair-community.social/@trochee",
        "display_name": "Jeremy Kahn"
      },
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      },
      {
        "url": "https://mstdn.social/@felwert",
        "display_name": "Frederik Elwert"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11023v1",
    "title": "https://arxiv.org/abs/2305.11023v1",
    "latest": "2023-05-21T06:24:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405239283755794",
      "content": "<p>\ud83d\udcdd Generalized Multiple Intent Conditioned Slot Filling \ud83d\udcda</p><p>\"Casts slot filling as a JSON generation task and approach it using a language model trained on pre-training datasets and an in-domain dataset generated with GPT-3.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/reinfer/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/reinfer/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11023v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11023v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11016v1",
    "title": "https://arxiv.org/abs/2305.11016v1",
    "latest": "2023-05-21T05:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405097640223081",
      "content": "<p>\ud83d\udcdd Silver Syntax Pre-Training for Cross-Domain Relation Extraction \ud83d\udcda</p><p>\"Exploits the affinity between syntactic structure and semantic RE, and identify the syntactic relations which are closely related to RE by being on the shortest dependency path between two entities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mainlp/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mainlp/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11016v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11016v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11000v1",
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "latest": "2023-05-21T05:12:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404955920701559",
      "content": "<p>\ud83d\udcdd SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities \ud83d\udcda</p><p>\"A three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11000v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11000v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10998v1",
    "title": "The Web Can Be Your Oyster for Improving Large Language Models",
    "latest": "2023-05-21T04:36:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404814488183160",
      "content": "<p>\ud83d\udcdd The Web Can Be Your Oyster for Improving Large Language Models \ud83d\udcda</p><p>\"Proposes a web-augmented LLM UNIWEB which learns over 16 knowledge-intensive tasks in a unified text-to-text format.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10998v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10998v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10992v1",
    "title": "How does the task complexity of masked pretraining objectives affect downstream performance?",
    "latest": "2023-05-21T04:00:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404672752845374",
      "content": "<p>\ud83d\udcdd How Does the Task Complexity of Masked Pretraining Objectives Affect Downstream Performance? \ud83d\udcda\ud83d\udc7e</p><p>\"Pretraining a model with a masked language modeling task and fine-tuning it on downstream tasks such as sentiment analysis or natural language understanding tasks shows the state-of-the-art result.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10992v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10992v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10991v1",
    "title": "Less is More! A slim architecture for optimal language translation",
    "latest": "2023-05-21T03:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404436705280754",
      "content": "<p>\ud83d\udcdd Less Is More! A Slim Architecture for Optimal Language Translation \ud83d\udcda\ud83d\udc7e</p><p>\"A gating mechanism to remove excess parameters in softmax attention, and a hierarchical word embedding scheme to boost performance of the embedding layer while also reducing parameter counts by a factor of 3.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10991v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10991v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10985v1",
    "title": "https://arxiv.org/abs/2305.10985v1",
    "latest": "2023-05-21T02:48:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404389675586755",
      "content": "<p>\ud83d\udcdd Multi-CrossRE a Multi-Lingual Multi-Domain Dataset for Relation Extraction \ud83d\udcda</p><p>\"Multi-CrossRE is a multi-lingual RE dataset created by translating CrossRE (Bassignana and Plank, 2022) into different languages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mainlp/CrossRE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mainlp/CrossRE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10985v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10985v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10951v1",
    "title": "https://arxiv.org/abs/2305.10951v1",
    "latest": "2023-05-21T01:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404153958070478",
      "content": "<p>\ud83d\udcdd Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation \ud83d\udcda</p><p>\"Self-training uses an existing ASR system to generate transcriptions of untranscribed speech, which are then combined with existing human-transcribed speech to improve performance.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Bartelds/asr-augmentation\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/Bartelds/asr-augmen</span><span class=\"invisible\">tation</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10951v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10951v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s00239-019-09896-2",
    "title": "Ageing Throughout History: The Evolution of Human Lifespan - Journal of Molecular Evolution",
    "latest": "2023-05-21T01:25:49+00:00",
    "last_post": {
      "url": "https://mastodon.social/@freakonometrics/110404065675005494",
      "content": "<p>\"Ageing Throughout History: The Evolution of Human Lifespan\" <a href=\"https://link.springer.com/article/10.1007/s00239-019-09896-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s00239-019-09896-2</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@freakonometrics",
        "display_name": "Arthur Charpentier"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10930v1",
    "title": "https://arxiv.org/abs/2305.10930v1",
    "latest": "2023-05-21T01:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403965242101491",
      "content": "<p>\ud83d\udcdd On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation \ud83d\udcda\ud83d\udc7e</p><p>\"Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/chenllliang/Off-Target-MNMT}{https://github.com/chenllliang/Off-Target-MNMT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/chenllliang/Off-Tar</span><span class=\"invisible\">get-MNMT}{https://github.com/chenllliang/Off-Target-MNMT</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10930v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10930v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10927v1",
    "title": "Causal Document-Grounded Dialogue Pre-training",
    "latest": "2023-05-21T00:36:18+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403870937916053",
      "content": "<p>\ud83d\udcdd Causal Document-Grounded Dialogue Pre-Training \ud83d\udcda</p><p>\"A causally-complete dataset construction strategy and a causally-perturbed pre-training strategy to capture the causal relationships between variables and optimize the overall casual effect, achieving considerable and consistent improvements in the fully-supervised, low-resource, few-shot, and zero-shot settings.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10927v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10927v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281010",
    "title": "The king\u2019s spice cabinet\u2013Plant remains from Gribshunden, a 15th century royal shipwreck in the Baltic Sea",
    "latest": "2023-05-20T23:27:00+00:00",
    "last_post": {
      "url": "https://xoxo.zone/@lauraehall/110397333505819975",
      "content": "<p>1) The shipwreck of the 1495 medieval Danish warship Gribshunden turned out to have incredibly well-preserved plant remains, including expensive spices like saffron, peppercorns, ginger, and almond.</p><p>It's a \u201csubstantially complete royal medieval pantry\u201d and is \"[one of] the most fabulous discoveries of spices in any archaeological context, on land or sea\"</p><p>View the paper here: <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281010\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0281010</span></a></p><p><a href=\"https://xoxo.zone/tags/Medieval\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Medieval</span></a> <a href=\"https://xoxo.zone/tags/Gribshunden\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Gribshunden</span></a> <a href=\"https://xoxo.zone/tags/Shipwreck\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Shipwreck</span></a> <a href=\"https://xoxo.zone/tags/Archaeology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Archaeology</span></a> <a href=\"https://xoxo.zone/tags/MaritimeArchaeology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MaritimeArchaeology</span></a> <a href=\"https://xoxo.zone/tags/Viking\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Viking</span></a> <a href=\"https://xoxo.zone/tags/Vikings\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Vikings</span></a> <a href=\"https://xoxo.zone/tags/Histodons\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Histodons</span></a></p>"
    },
    "people": [
      {
        "url": "https://xoxo.zone/@lauraehall",
        "display_name": "Laura E. Hall"
      },
      {
        "url": "https://kolektiva.social/@aredridel",
        "display_name": "Mx. Aria Stewart"
      },
      {
        "url": "https://everything.happens.horse/@vruba",
        "display_name": "Charlie Loyd"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01705-7",
    "title": "China overtakes United States on contribution to research in Nature Index",
    "latest": "2023-05-20T23:25:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110403590743895835",
      "content": "<p>China overtakes United States on contribution to research in Nature Index - <a href=\"https://www.nature.com/articles/d41586-023-01705-7\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01705-7</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10920v1",
    "title": "https://arxiv.org/abs/2305.10920v1",
    "latest": "2023-05-20T23:12:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403540459568637",
      "content": "<p>\ud83d\udcdd Emergent Communication with Attention \ud83d\udcda\ud83d\udc7e</p><p>\"Attention allows the agents to learn to focus on particular concepts in the environment and align with their partner on the concepts they focus on, which leads to a more compositional and interpretable emergent language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/zalandoresearch/fashion-mnist\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/zalandoresearch/fas</span><span class=\"invisible\">hion-mnist</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10920v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10920v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07922",
    "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
    "latest": "2023-05-20T23:09:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110375897971452835",
      "content": "<p>CodeT5+: Open Code Large Language Models for Code Understanding and Generation</p><p>The instruction-tuned CodeT5+ 16B achieves new SoTA of 35.0% pass@1 on the HumanEval code generation task against other open code LLMs.</p><p><a href=\"https://arxiv.org/abs/2305.07922\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07922</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10907v1",
    "title": "Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation",
    "latest": "2023-05-20T22:24:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403351800700620",
      "content": "<p>\ud83d\udcdd Take a Break in the Middle: Investigating Subgoals Towards Hierarchical Script Generation \ud83d\udcda</p><p>\"The goal is first decomposed into a few steps and then each step is further decomposed into subgoals, until the leaf nodes (i,e, primitive actions) are reached.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10907v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10907v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10866v1",
    "title": "TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition",
    "latest": "2023-05-20T21:24:18+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403115970890544",
      "content": "<p>\ud83d\udcdd TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition \ud83d\udcda</p><p>\"Proposed to design auxiliary tasks with enlightened prompt learning for Implicit Discourse Relation Recognition task based on the joint training of three prompt learning tasks with shared argument representation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10866v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10866v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10847v1",
    "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
    "latest": "2023-05-20T20:00:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402785326190004",
      "content": "<p>\ud83d\udcdd Large Language Models Can Be Guided to Evade AI-Generated Text Detection \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts, which enables LLMs to evade existing plagiarism detectors.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10847v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10847v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10468",
    "title": "Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence",
    "latest": "2023-05-20T19:46:43+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110402732285772796",
      "content": "<p>[R] Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence</p><p><a href=\"https://arxiv.org/abs/2305.10468\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10468</span><span class=\"invisible\"></span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://arxiv.org/abs/2305.10468\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://arxiv.org/a</span><span class=\"invisible\">bs/2305.10468</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08596?utm_source=tldrai",
    "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
    "latest": "2023-05-20T19:30:02+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110402666666468743",
      "content": "<p>RT @neuroecology@twitter.com</p><p>\"We need to solve the alignment problem before a MegaCorp creates an AI that turns us all into paperclips\"</p><p>Meanwhile, academics:</p><p><a href=\"https://arxiv.org/abs/2305.08596?utm_source=tldrai\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">arxiv.org/abs/2305.08596?utm_s</span><span class=\"invisible\">ource=tldrai</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/neuroecology/status/1659998392817266690\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/neuroecology/statu</span><span class=\"invisible\">s/1659998392817266690</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10845v1",
    "title": "https://arxiv.org/abs/2305.10845v1",
    "latest": "2023-05-20T19:24:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402644030334523",
      "content": "<p>\ud83d\udcdd TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model \ud83d\udcda</p><p>\"Consists of two passes: the first pass uses a non-incremental model, and the second pass uses an incremental policy to revise its prediction based on the first pass.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/pkhdipraja/tapir\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/pkhdipraja/tapir</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10845v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10845v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10833v1",
    "title": "Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants",
    "latest": "2023-05-20T18:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402455092024406",
      "content": "<p>\ud83d\udcdd Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants \ud83d\udcda</p><p>\"Works on a dataset of flowers and plants with the help of different transformer-based models to distinguish between metaphoric and literal flower and plant names using transfer learning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10833v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10833v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10819v1",
    "title": "https://arxiv.org/abs/2305.10819v1",
    "latest": "2023-05-20T18:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402313723964201",
      "content": "<p>\ud83d\udcdd CLEME: Debiasing Multi-Reference Evaluation for Grammatical Error Correction \ud83d\udcda</p><p>\"CLEME first constructs consistent chunk sequences for each sentence, then computes F$_{0,5}$ scores based on the chunks, where the boundaries of grammatical errors are determined automatically.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/yejh123/CLEME\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/yejh123/CLEME</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10819v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10819v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/zpgdf/",
    "title": "http://osf.io/zpgdf/",
    "latest": "2023-05-20T17:56:43+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110402299702954366",
      "content": "<p>Measuring Backsliding with Observables: Observable-to-Subjective Score Mapping (OSM) <a href=\"http://osf.io/zpgdf/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/zpgdf/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2203.02155.pdf",
    "title": "https://arxiv.org/pdf/2203.02155.pdf",
    "latest": "2023-05-20T17:28:51+00:00",
    "last_post": {
      "url": "https://mastodon.online/@slashdottir/110402189991015772",
      "content": "<p>nvm, I think I found it</p><p><a href=\"https://arxiv.org/pdf/2203.02155.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2203.02155.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv ML / AI"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10786v1",
    "title": "https://arxiv.org/abs/2305.10786v1",
    "latest": "2023-05-20T17:12:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402124888334936",
      "content": "<p>\ud83d\udcdd Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings \ud83d\udcda</p><p>\"Diagonal Attention Pooling (Ditto) alleviates the anisotropy problem and improve pre-trained models on STS tasks with model-based importance estimations and weighted average of word representations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/princeton-nlp/SimCSE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/princeton-nlp/SimCS</span><span class=\"invisible\">E</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10786v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10786v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08298",
    "title": "Symbol tuning improves in-context learning in language models",
    "latest": "2023-05-20T16:51:30+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110375823308376698",
      "content": "<p>Symbol tuning improves in-context learning in language models</p><p>Presents symbol tuning\u2014finetuning LMs on in-context input\u2013label pairs where natural language labels (e.g., \u201cpositive sentiment\u201d) are replaced with arbitrary symbols (e.g., \u201cfoo/bar\u201d).</p><p><a href=\"https://arxiv.org/abs/2305.08298\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08298</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10736v1",
    "title": "Counterfactual Debiasing for Generating Factually Consistent Text Summaries",
    "latest": "2023-05-20T16:48:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402030532597530",
      "content": "<p>\ud83d\udcdd Counterfactual Debiasing for Generating Factually Consistent Text Summaries \ud83d\udcda\ud83d\udc7e</p><p>\"Constructs causal graphs for abstractive text summarization and identify the intrinsic causes of factual inconsistency, i,e, language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10736v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10736v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.15004.pdf",
    "title": "https://arxiv.org/pdf/2304.15004.pdf",
    "latest": "2023-05-20T16:20:16+00:00",
    "last_post": {
      "url": "https://dair-community.social/@timnitGebru/110398594256377809",
      "content": "<p>\"we present an al-<br>ternative explanation for emergent abilities: that for a particular task and model<br>family, when analyzing fixed model outputs, one can choose a metric which leads<br>to the inference of an emergent ability or another...which does not..our alternative suggests that..claims of emergent abilities are creations of the researcher\u2019s analyses, not fundamental changes in model behavior on specific tasks with scale\"</p><p>Rylan Schaeffer, Brando Miranda, &amp; Sanmi Koyejo</p><p><a href=\"https://arxiv.org/pdf/2304.15004.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.15004.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@eliocamp",
        "display_name": "Elio Campitelli"
      },
      {
        "url": "https://masto.ai/@cheng",
        "display_name": "Cheng Soon Ong"
      },
      {
        "url": "https://dair-community.social/@timnitGebru",
        "display_name": "Timnit Gebru (she/her)"
      },
      {
        "url": "https://vis.social/@futuraprime",
        "display_name": "Evan Hensleigh"
      },
      {
        "url": "https://wandering.shop/@janellecshane",
        "display_name": "Janelle Shane"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10824v1",
    "title": "https://arxiv.org/abs/2305.10824v1",
    "latest": "2023-05-20T16:16:26+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_ir/110401841834778788",
      "content": "<p>\ud83d\udcdd Integrating Item Relevance in Training Loss for Sequential Recommender Systems \ud83d\udcbf\ud83d\udc7e</p><p>\"Proposes a relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise in user interactions by leveraging the relevance among them.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/IR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>IR</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/pmixer/SASRec.pytorch\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/pmixer/SASRec.pytor</span><span class=\"invisible\">ch</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10824v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10824v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://recsys.social/@karlhigley",
        "display_name": "Karl Higley"
      }
    ]
  }
]