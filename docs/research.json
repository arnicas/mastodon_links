[
  {
    "link": "https://arxiv.org/abs/2305.13888v1",
    "title": "PaD: Program-aided Distillation Specializes Large Models in Reasoning",
    "latest": "2023-05-25T09:57:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428727161918699",
      "content": "<p>\ud83d\udcdd PaD: Program-Aided Distillation Specializes Large Models in Reasoning \ud83d\udcda</p><p>\"We distill LLMs to specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking, which is a general-purpose approach.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13888v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13888v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41559-023-02008-w",
    "title": "Better incentives are needed to reward academic software development - Nature Ecology & Evolution",
    "latest": "2023-05-25T09:29:58+00:00",
    "last_post": {
      "url": "https://datasci.social/@mszll/110428618663085148",
      "content": "<p>Better incentives are needed to reward academic software development<br><a href=\"https://www.nature.com/articles/s41559-023-02008-w\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41559-023</span><span class=\"invisible\">-02008-w</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      },
      {
        "url": "https://fediscience.org/@tschfflr",
        "display_name": "Tatjana Scheffler"
      },
      {
        "url": "https://mstdn.social/@felwert",
        "display_name": "Frederik Elwert"
      },
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      },
      {
        "url": "https://fosstodon.org/@mdsumner",
        "display_name": "mdsumner"
      },
      {
        "url": "https://vis.social/@jhilden",
        "display_name": "Jonatan Hild\u00e9n"
      },
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13857v1",
    "title": "https://arxiv.org/abs/2305.13857v1",
    "latest": "2023-05-25T09:27:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428609258089204",
      "content": "<p>\ud83d\udcdd Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation \ud83d\udcda</p><p>\"Conducts an interactive user study to unveil the vulnerabilities of task-oriented dialogue (TOD) systems against realistic scenarios that have not been explored before, in which users do not follow instructions precisely due to their lack of understanding.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/budzianowski/multiwoz\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/budzianowski/multiw</span><span class=\"invisible\">oz</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13857v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13857v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13850v1",
    "title": "Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document",
    "latest": "2023-05-25T09:07:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428530401311286",
      "content": "<p>\ud83d\udcdd Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document \ud83d\udcda\ud83d\udc7e</p><p>\"Given an image of the document, GOSE firstly generates the preliminary relation predictions on entity pairs; secondly, it captures the global structure knowledge based on the prediction result of the previous iteration and further incorporates global structure knowledge into entity representations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13850v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13850v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13844v1",
    "title": "https://arxiv.org/abs/2305.13844v1",
    "latest": "2023-05-25T08:57:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428491092824948",
      "content": "<p>\ud83d\udcdd Arukikata Travelogue Dataset with Geographic Entity Mention, Coreference, and Link Annotation \ud83d\udcda</p><p>\"The dataset is created by annotating travelogue documents in Japanese with mentions, coreference clusters, and geo-entities based on geo-databases such as GeoNames and Geonames.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/megagonlabs/ginza\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/megagonlabs/ginza</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13844v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13844v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13829v1",
    "title": "Learn from Mistakes through Cooperative Interaction with Study Assistant",
    "latest": "2023-05-25T08:37:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428412538266960",
      "content": "<p>\ud83d\udcdd Learn From Mistakes Through Cooperative Interaction with Study Assistant \ud83d\udcda</p><p>\"SALAM collects mistakes from previous responses in the training phase and provides general guidance during inference based on the mistake collections, so the LLM can avoid making similar mistakes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13829v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13829v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13820v1",
    "title": "An Open Dataset and Model for Language Identification",
    "latest": "2023-05-25T08:07:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428294254662007",
      "content": "<p>\ud83d\udcdd An Open Dataset and Model for Language Identification \ud83d\udcda</p><p>\"A language identification model which achieves a macro-average F1 score of 0-93 across 201 languages by training on a curated dataset of monolingual data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13820v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13820v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13812v1",
    "title": "https://arxiv.org/abs/2305.13812v1",
    "latest": "2023-05-25T07:47:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428216056252125",
      "content": "<p>\ud83d\udcdd Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality \ud83d\udcda\ud83d\udd2d</p><p>\"Scene graph decomposition and augmentation along with coarse-to-fine contrastive learning that aligns sentences of various complexities to the same image, along with novel negative mining techniques in the scene graph space.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/vacancy/SceneGraphParser\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/vacancy/SceneGraphP</span><span class=\"invisible\">arser</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13812v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13812v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13808v1",
    "title": "Asking Clarification Questions to Handle Ambiguity in Open-Domain QA",
    "latest": "2023-05-25T07:37:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428176693835208",
      "content": "<p>\ud83d\udcdd Asking Clarification Questions to Handle Ambiguity in Open-Domain QA \ud83d\udcda</p><p>\"Proposes an approach based on GPT-2 to automatically generate clarification questions given an ambiguous question, a set of passages that contain the answer, and a set of possible answers.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13808v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13808v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13805v1",
    "title": "Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path",
    "latest": "2023-05-25T07:17:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428097967246318",
      "content": "<p>\ud83d\udcdd Towards Zero-Shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path \ud83d\udcda</p><p>\"ReXMiner encodes the shortest relative paths in the Document Object Model (DOM) tree which is a more accurate and efficient signal for key-value pair extraction within a web page.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13805v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13805v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13794v1",
    "title": "Personalized Predictive ASR for Latency Reduction in Voice Assistants",
    "latest": "2023-05-25T06:57:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428019342090959",
      "content": "<p>\ud83d\udcdd Personalized Predictive ASR for Latency Reduction in Voice Assistants \ud83d\udcda</p><p>\"Predictive automatic speech recognition (ASR) involves predicting the full utterance from a partially observed utterance and passing the predicted utterance to downstream systems in order to prefetch and cache a response.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13794v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13794v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13788v1",
    "title": "Can Large Language Models Infer and Disagree Like Humans?",
    "latest": "2023-05-25T06:37:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427940521509751",
      "content": "<p>\ud83d\udcdd Can Large Language Models Infer and Disagree Like Humans? \ud83d\udcda\ud83d\udc7e</p><p>\"Uses two different techniques, Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR), to test LLMs\u2019 ability and alignment with human disagreement distribution on NLI.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13788v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13788v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13785v1",
    "title": "Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation",
    "latest": "2023-05-25T06:27:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427901326705894",
      "content": "<p>\ud83d\udcdd Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation \ud83d\udcda</p><p>\"Optimizes few-shot text classification without accessing the gradients of the underlying LLMs through prompt-based data augmentation, treating the black-box model as a feature extractor and training a classifier on the extracted features.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13785v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13785v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13782v1",
    "title": "https://arxiv.org/abs/2305.13782v1",
    "latest": "2023-05-25T06:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427862076106745",
      "content": "<p>\ud83d\udcdd Images in Language Space: Exploring the Suitability of Large Language Models for Vision &amp; Language Tasks \ud83d\udcda</p><p>\"Uses separate verbalisation models to verbalise visual information into text before using it to solve visual-linguistic tasks with a language model like GPT-3.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clp-research/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/clp-research/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13782v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13782v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13776v1",
    "title": "Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation",
    "latest": "2023-05-25T06:07:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427822770223273",
      "content": "<p>\ud83d\udcdd Counterspeeches Up My Sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation \ud83d\udcda\ud83d\udc7e</p><p>\"A two stage framework for intent-conditioned counterspeech generation that leverages vector-quantized representations learned for each intent category along with PerFuMe, a novel fusion module to incorporate intent-specific information into the model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13776v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13776v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13755v1",
    "title": "https://arxiv.org/abs/2305.13755v1",
    "latest": "2023-05-25T05:47:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427744116928214",
      "content": "<p>\ud83d\udcdd Topic-Driven Distant Supervision Framework for Macro-Level Discourse Parsing \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a distant supervision framework that leverages the relations between topic structure and rhetorical structure to narrow the gap between in-domain and out-of-domain tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/fjiangAI/TDSF_DP\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/fjiangAI/TDSF_DP</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13755v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13755v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.32388/ZNWI7T",
    "title": "https://doi.org/10.32388/ZNWI7T",
    "latest": "2023-05-25T05:20:09.312000+00:00",
    "last_post": {
      "url": "https://scholar.social/@egonw/110425122353070829",
      "content": "<p>Jente Houweling and I wrote up the idea (based on discussions, what we read, etc) that resulted from her research that we should replace \"data management\" with \"research output management\". Because we have a very narrow view of what data is, which does not reflect the research process.</p><p>We wrote up a definition and are looking for your peer review! Please be Reviewer of this short output: <a href=\"https://doi.org/10.32388/ZNWI7T\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.32388/ZNWI7T</span><span class=\"invisible\"></span></a></p><p><a href=\"https://scholar.social/tags/peerWanted\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>peerWanted</span></a></p>"
    },
    "people": [
      {
        "url": "https://scholar.social/@telliott",
        "display_name": "Tom Elliott"
      },
      {
        "url": "https://mstdn.social/@felwert",
        "display_name": "Frederik Elwert"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13751v1",
    "title": "Challenges in Context-Aware Neural Machine Translation",
    "latest": "2023-05-25T05:17:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427626029957175",
      "content": "<p>\ud83d\udcdd Challenges in Context-Aware Neural Machine Translation \ud83d\udcda</p><p>\"Collects a new dataset of Chinese-English novels with sentence and document-level segmentation to facilitate more realistic research on context-aware machine translation for the document-level setting.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13751v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13751v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13749v1",
    "title": "https://arxiv.org/abs/2305.13749v1",
    "latest": "2023-05-25T04:57:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427547542304940",
      "content": "<p>\ud83d\udcdd Goal-Driven Explainable Clustering via Language Descriptions \ud83d\udcda</p><p>\"We first prompt a language model with \"[corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ZihanWangKi/GoalEx\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/ZihanWangKi/GoalEx</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13749v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13749v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13735v1",
    "title": "Aligning Large Language Models through Synthetic Feedback",
    "latest": "2023-05-25T04:17:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427390385698115",
      "content": "<p>\ud83d\udcdd Aligning Large Language Models Through Synthetic Feedback \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Our 7B-sized model outperforms the 12-13B-sized models with about 75% winning rate on average on the A/B tests, which use GPT-4 as the judge.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13735v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13735v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13738v1",
    "title": "i-Code Studio: A Configurable and Composable Framework for Integrative AI",
    "latest": "2023-05-25T04:07:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427350926832179",
      "content": "<p>\ud83d\udcdd I-Code Studio: A Configurable and Composable Framework for Integrative AI \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"Works by orchestrating multiple pre-trained models in a fine-tuning-free fashion, and is able to conduct complex multimodal tasks such as video-to-text retrieval and visual question answering.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13738v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13738v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://academic.oup.com/jxb/advance-article/doi/10.1093/jxb/erad167/7173266",
    "title": "design of synthetic gene circuits in plants: new components, old challenges",
    "latest": "2023-05-25T03:53:43+00:00",
    "last_post": {
      "url": "https://genomic.social/@jamespblloyd/110427196190556368",
      "content": "<p>A really great primer on the current state of plant synthetic biology, with a focus on synthetic gene circuits. </p><p>A must read for anyone interested in plant synbio. </p><p>The design of synthetic gene circuits in plants: new components, old challenges</p><p><a href=\"https://academic.oup.com/jxb/advance-article/doi/10.1093/jxb/erad167/7173266\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">academic.oup.com/jxb/advance-a</span><span class=\"invisible\">rticle/doi/10.1093/jxb/erad167/7173266</span></a></p>"
    },
    "people": [
      {
        "url": "https://bayes.club/@akhilrao",
        "display_name": "edgeworth boxlord"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13733v1",
    "title": "https://arxiv.org/abs/2305.13733v1",
    "latest": "2023-05-25T03:27:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427193570555033",
      "content": "<p>\ud83d\udcdd Self-Critique Prompting with Large Language Models for Inductive Instructions \ud83d\udcda</p><p>\"Self-Critique prompting enables models to not only self-criticize but also self-correct, which shows promising results in handling inductive instructions under zero-shot and few-shot settings.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/DevoAllen/INDust\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/DevoAllen/INDust</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13733v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13733v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13723v1",
    "title": "PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training",
    "latest": "2023-05-25T03:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427154322898362",
      "content": "<p>\ud83d\udcdd PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training \ud83d\udcda</p><p>\"PromptClass is composed of: (A) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13723v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13723v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2302.06716v3.pdf",
    "title": "https://arxiv.org/pdf/2302.06716v3.pdf",
    "latest": "2023-05-25T02:53:29+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110427059641306942",
      "content": "<p>\"Machine Learning Model Attribution Challenge\"</p><p><a href=\"https://arxiv.org/pdf/2302.06716v3.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2302.06716v3.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mastodon.radio/tags/forensics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>forensics</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.13812.pdf",
    "title": "https://arxiv.org/pdf/2305.13812.pdf",
    "latest": "2023-05-25T02:48:33+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110427040198458594",
      "content": "<p>RT @Harman26Singh@twitter.com</p><p>\ud83d\udea8New paper\ud83d\udea8<br>Contrastively trained Vision-Language Model\u2019s (like CLIP) are poor at compositional reasoning</p><p>Our new paper improves this:<br>\u201cCoarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality\u201d</p><p><a href=\"https://arxiv.org/pdf/2305.13812.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.13812.pdf</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Harman26Singh/status/1661335953175355392\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Harman26Singh/stat</span><span class=\"invisible\">us/1661335953175355392</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13721v1",
    "title": "Continual Dialogue State Tracking via Example-Guided Question Answering",
    "latest": "2023-05-25T02:47:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427036167124444",
      "content": "<p>\ud83d\udcdd Continual Dialogue State Tracking via Example-Guided Question Answering \ud83d\udcda\ud83d\udc7e</p><p>\"A model trained on DST is reformulated as an example-guided QA framework, where a model is prompted to predict the change in dialogue state given a dialogue history and an in-context example.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13721v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13721v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14992",
    "title": "Reasoning with Language Model is Planning with World Model",
    "latest": "2023-05-25T02:36:29+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426992790607723",
      "content": "<p>Reasoning with Language Model is Planning with World Model</p><p>- Proposes RAP, a new LLM reasoning framework</p><p>- PAP on LLaMA33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting</p><p><a href=\"https://arxiv.org/abs/2305.14992\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14992</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2303.12132v1.pdf",
    "title": "https://arxiv.org/pdf/2303.12132v1.pdf",
    "latest": "2023-05-25T02:32:00+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110426975159173620",
      "content": "<p>\"Fundamentals of Generative Large Language Models<br>and Perspectives in Cyber-Defense\"</p><p>This review aims to provide a<br>brief overview of the history, state of the art, and implications of Generative Language Models in<br>terms of their principles, abilities, limitations, and future prospects \u2013 especially in the context of<br>cyber-defense, with a focus on the Swiss operational environment.</p><p><a href=\"https://arxiv.org/pdf/2303.12132v1.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2303.12132v1.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a>  <a href=\"https://mastodon.radio/tags/infosec\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>infosec</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15328",
    "title": "https://arxiv.org/abs/2305.15328",
    "latest": "2023-05-25T02:28:36+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426961764083786",
      "content": "<p>Visual Programming for Text-to-Image Generation and Evaluation</p><p>Presents VPGen, an interpretable step-by-step T2I generation framework that decomposes the task into multiple steps.</p><p>proj: <a href=\"https://vp-t2i.github.io\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">vp-t2i.github.io</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.15328\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15328</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13718v1",
    "title": "https://arxiv.org/abs/2305.13718v1",
    "latest": "2023-05-25T02:27:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426957620405347",
      "content": "<p>\ud83d\udcdd LogicLLM: Exploring Self-Supervised Logic-Enhanced Training for Large Language Models \ud83d\udcda</p><p>\"The proposed LogicLLM first incorporates logical knowledge through self-supervised post-training and activates it via in-context learning with an auto-regressive objective variant of MERIt.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/SparkJiao/MERIt-v2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/SparkJiao/MERIt-v2</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13718v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13718v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15080",
    "title": "Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
    "latest": "2023-05-25T02:18:29+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426921961551530",
      "content": "<p>Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models</p><p>Significantly outperforms the existing SotA models on visual document understanding</p><p><a href=\"https://arxiv.org/abs/2305.15080\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15080</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13712v1",
    "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
    "latest": "2023-05-25T02:07:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426878834385383",
      "content": "<p>\ud83d\udcdd Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"Collects a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty, as well as a semantic evaluation method to quantify the uncertainty expressed in the answers.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13712v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13712v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15074",
    "title": "https://arxiv.org/abs/2305.15074",
    "latest": "2023-05-25T02:00:32+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426851381371703",
      "content": "<p>Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models</p><p>Presents JEEBench, a benchmark w/ 450 pre-engineering math, physics and chemistry problems from the IIT JEE-Advanced Exam</p><p>repo: <a href=\"https://github.com/hgaurav2k/JEEBench\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/hgaurav2k/JEEBench</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.15074\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15074</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13711v1",
    "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
    "latest": "2023-05-25T01:57:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426839490239854",
      "content": "<p>\ud83d\udcdd LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"LLM-Eval is a unified evaluation framework consisting of 4 dimensions for assessing open-domain conversation quality, including relevance, informativeness, knowledgeability, and engagingness.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13711v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13711v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13710v1",
    "title": "Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems",
    "latest": "2023-05-25T01:37:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426761010404911",
      "content": "<p>\ud83d\udcdd Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems \ud83d\udcda</p><p>\"An end-to-end task-oriented dialogue system with an interactive textual interface is trained to process external knowledge through natural language understanding and reasoning with an attention mechanism.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13710v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13710v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15324",
    "title": "Model evaluation for extreme risks",
    "latest": "2023-05-25T01:32:30+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426741170838224",
      "content": "<p>Model evaluation for extreme risks</p><p><a href=\"https://arxiv.org/abs/2305.15324\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15324</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13703v1",
    "title": "MemeCap: A Dataset for Captioning and Interpreting Memes",
    "latest": "2023-05-25T01:17:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426682416685071",
      "content": "<p>\ud83d\udcdd MemeCap: A Dataset for Captioning and Interpreting Memes \ud83d\udcda</p><p>\"Proposes a new task of meme captioning and release a new dataset of 6300 memes along with captions, visual metaphors and literal captions.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13703v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13703v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15334",
    "title": "https://arxiv.org/abs/2305.15334",
    "latest": "2023-05-25T01:15:29+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426674246413641",
      "content": "<p>Gorilla: Large Language Model Connected with Massive APIs</p><p>Releases Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls.</p><p>proj: <a href=\"https://gorilla.cs.berkeley.edu/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">gorilla.cs.berkeley.edu/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.15334\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15334</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15348",
    "title": "READ: Recurrent Adaptation of Large Transformers",
    "latest": "2023-05-25T01:10:30+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426654638888250",
      "content": "<p>READ: Recurrent Adaptation of Large Transformers</p><p>Presents READ, a parameter-efficient transfer learning method that outperforms other methods while consuming significantly lower energy.</p><p><a href=\"https://arxiv.org/abs/2305.15348\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15348</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13698v1",
    "title": "https://arxiv.org/abs/2305.13698v1",
    "latest": "2023-05-25T01:07:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426643096373835",
      "content": "<p>\ud83d\udcdd Exploring Large Language Models for Classical Philology \ud83d\udcda</p><p>\"Creates four language models for Ancient Greek that vary along two dimensions: Explores encoder-only and encoder-decoder architectures using two model types, RoBERTa and T5.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Heidelberg-NLP/ancient-language-models\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/Heidelberg-NLP/anci</span><span class=\"invisible\">ent-language-models</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13698v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13698v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15393",
    "title": "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models",
    "latest": "2023-05-25T01:04:29+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426630991534389",
      "content": "<p>LayoutGPT: Compositional Visual Planning and Generation with Large Language Models</p><p>Outperforms text-to-image models/systems by 20-40% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness.</p><p><a href=\"https://arxiv.org/abs/2305.15393\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15393</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13697v1",
    "title": "UNIMO-3: Multi-granularity Interaction for Vision-Language Representation Learning",
    "latest": "2023-05-25T00:47:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426564383579553",
      "content": "<p>\ud83d\udcdd UNIMO-3: Multi-Granularity Interaction for Vision-Language Representation Learning \ud83d\udcda</p><p>\"UNIMO-3 model can establish effective connections between different layers in a cross-modal encoder, and adaptively capture the interaction between two modalities at different levels, which leads to the cross-modal interaction collapsing into a limited multi-modal semantic information interaction.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13697v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13697v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13685v1",
    "title": "Causal Intervention for Abstractive Related Work Generation",
    "latest": "2023-05-25T00:07:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426407155051218",
      "content": "<p>\ud83d\udcdd Causal Intervention for Abstractive Related Work Generation \ud83d\udcda</p><p>\"Causal Intervention Module for Related Work Generation (CaM) to effectively capture causalities in the generation process and improve the quality and coherence of the generated related works.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13685v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13685v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13691v1",
    "title": "Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis",
    "latest": "2023-05-24T23:57:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426367857397099",
      "content": "<p>\ud83d\udcdd Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis \ud83d\udcda</p><p>\"Built upon the data generation functions parameterized by LLMs and prompts, which requires minimal hand-crafted features or heuristics to be defined for multi-hop QA and fact verification tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13691v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13691v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05928",
    "title": "WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia",
    "latest": "2023-05-24T23:53:04+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110426350203980803",
      "content": "<p>\"WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia\" WikiSQE has about 3.4 M sentences with 153 quality labels</p><p>(Ando et al, 2023)</p><p><a href=\"https://arxiv.org/abs/2305.05928\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05928</span><span class=\"invisible\"></span></a> <a href=\"https://twitter.com/WikiResearch/status/1658042531643367424\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1658042531643367424</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13684v1",
    "title": "mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models",
    "latest": "2023-05-24T23:27:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426249942645112",
      "content": "<p>\ud83d\udcdd mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models \ud83d\udcda</p><p>\"Capable of selecting better source language by inducing language similarities from mPLMs using multi-parallel corpora, resulting in 1%-2% improvement on cross-lingual transfer.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13684v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13684v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13683v1",
    "title": "https://arxiv.org/abs/2305.13683v1",
    "latest": "2023-05-24T23:07:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426170945203225",
      "content": "<p>\ud83d\udcdd Error Detection for Text-to-SQL Semantic Parsing \ud83d\udcda</p><p>\"Based on pre-trained language model of code and is enhanced with structural features learned by graph neural networks to detect erroneous SQL query predictions on the fly without access to the parser.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/antlr/grammars-v4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/antlr/grammars-v4</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13683v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13683v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11675",
    "title": "https://arxiv.org/abs/2305.11675",
    "latest": "2023-05-24T23:05:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110426161345853731",
      "content": "<p>High-Quality Video Reconstruction from Brain Activity - <a href=\"https://arxiv.org/abs/2305.11675\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11675</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14314",
    "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "latest": "2023-05-24T23:05:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110426161336484099",
      "content": "<p>QLoRA: Efficient Finetuning of Quantized LLMs - <a href=\"https://arxiv.org/abs/2305.14314\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14314</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@bergi",
        "display_name": "Thomas Bergwinkl"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13669v1",
    "title": "Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment",
    "latest": "2023-05-24T21:57:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425895873376639",
      "content": "<p>\ud83d\udcdd Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment \ud83d\udcda\ud83d\udc7e</p><p>\"MixAlign is a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13669v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13669v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13668v1",
    "title": "Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations",
    "latest": "2023-05-24T21:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425817274498814",
      "content": "<p>\ud83d\udcdd Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations \ud83d\udcda\ud83e\udde0</p><p>\"Learns a similarity metric for each object, such that when the similarity between object A and a given verb is higher than the similarity between object B and that verb, the agent will more likely choose object A than object B in a given context.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13668v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13668v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13667v1",
    "title": "https://arxiv.org/abs/2305.13667v1",
    "latest": "2023-05-24T21:27:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425777781700447",
      "content": "<p>\ud83d\udcdd Optimizing Non-Autoregressive Transformers with Contrastive Learning \ud83d\udcda</p><p>\"Proposed to ease the difficulty of modality learning via sampling from the model distribution instead of the data distribution, and derive contrastive constraints to stabilize the training process and integrate this resulting objective with the state-of-the-art NAT architecture DA-Transformer.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/thu-coai/DA-Transformer\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/thu-coai/DA-Transfo</span><span class=\"invisible\">rmer</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13667v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13667v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14296",
    "title": "USB: A Unified Summarization Benchmark Across Tasks and Domains",
    "latest": "2023-05-24T21:18:38+00:00",
    "last_post": {
      "url": "https://hci.social/@jbigham/110425742959387131",
      "content": "<p>My students and collaborators just released a new summarization benchmark that forefronts factuality and evidence extraction -- </p><p>\u26a1Introducing USB: A Unified Summarization Benchmark Across Tasks and Domains!</p><p><a href=\"https://arxiv.org/abs/2305.14296\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14296</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@jbigham",
        "display_name": "Jeff Bigham"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13658v1",
    "title": "Understanding compositional data augmentation in automatic morphological inflection",
    "latest": "2023-05-24T21:07:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425699476851475",
      "content": "<p>\ud83d\udcdd Understanding Compositional Data Augmentation in Automatic Morphological Inflection \ud83d\udcda</p><p>\"A data augmentation method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples for automatic morphological inflection generation tasks such as lemmatization and morphological analysis.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13658v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13658v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.09848?s=09",
    "title": "Evaluating Verifiability in Generative Search Engines",
    "latest": "2023-05-24T20:48:37+00:00",
    "last_post": {
      "url": "https://mastodon.cloud/@aarontay/110276375288247354",
      "content": "<p>[Read] Evaluating Verifiability in Generative Search Engines - finally a paper that assesses how reliable Search+Large language models (LLM) type systems like Bing Chat, perplexity.ai, you chat, neevaai <a href=\"https://arxiv.org/abs/2304.09848?s=09\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.09848?s=09</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13657v1",
    "title": "ChatGPT as your Personal Data Scientist",
    "latest": "2023-05-24T20:47:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425620529901551",
      "content": "<p>\ud83d\udcdd ChatGPT as Your Personal Data Scientist \ud83d\udcda</p><p>\"ChatGPT acts as an intelligent agent that assists users in their data science tasks through intuitive, natural conversations and without requiring in-depth knowledge of the underlying ML processes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13657v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13657v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14327",
    "title": "https://arxiv.org/abs/2305.14327",
    "latest": "2023-05-24T20:37:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110425579685882242",
      "content": "<p>RT @Wade_Yin9712@twitter.com</p><p>Introducing \ud83e\udd96Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation! <br>Dynosaur contains 800K generated instruction-tuning data and can \u2b06\ufe0f**dynamically** grow!</p><p>Project page: <a href=\"https://dynosaur-it.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">dynosaur-it.github.io/</span><span class=\"invisible\"></span></a><br>Preprint: <a href=\"https://arxiv.org/abs/2305.14327\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14327</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/WadeYin9712/Dynosaur\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/WadeYin9712/Dynosau</span><span class=\"invisible\">r</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Wade_Yin9712/status/1661237791920132098\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Wade_Yin9712/statu</span><span class=\"invisible\">s/1661237791920132098</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1093/scipol/scac081",
    "title": "https://doi.org/10.1093/scipol/scac081",
    "latest": "2023-05-24T19:54:47+00:00",
    "last_post": {
      "url": "https://hcommons.social/@jcpeyssard/110425413242180989",
      "content": "<p>Mikael Laakso , Anna-Maija Multas, European scholarly journals from small- and mid-size publishers: mapping journals and public funding mechanisms, Science and Public Policy, 2023 <a href=\"https://doi.org/10.1093/scipol/scac081\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.1093/scipol/scac081</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@RonaldSnijder",
        "display_name": "Ronald Snijder"
      },
      {
        "url": "https://hcommons.social/@jcpeyssard",
        "display_name": "Jean-Christophe Peyssard"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13654v1",
    "title": "Understanding and Mitigating Spurious Correlations in Text Classification",
    "latest": "2023-05-24T19:47:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425384605481370",
      "content": "<p>\ud83d\udcdd Understanding and Mitigating Spurious Correlations in Text Classification \ud83d\udcda</p><p>\"Prevents models from learning certain spurious features by introducing a regularization loss during fine-tuning process of pre-trained language models (PLM), so that the model will not rely on those spurious features and generalize on out-of-distribution data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13654v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13654v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13048",
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "latest": "2023-05-24T19:38:07+00:00",
    "last_post": {
      "url": "https://arvr.social/@mpesce/110417099505946407",
      "content": "<p>During my keynote this morning I noted that in the three weeks I'd been keynoting <a href=\"https://arvr.social/tags/pwctheoutside\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>pwctheoutside</span></a> the field of artificial intelligence had advanced at least three years.</p><p>Here's today's massive research breakthrough. It might be the AI breakthrough of the year -- in any other year.</p><p><a href=\"https://arxiv.org/abs/2305.13048\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13048</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@davidmortensen",
        "display_name": "David Mortensen"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13648v1",
    "title": "Non-parametric, Nearest-neighbor-assisted Fine-tuning for Neural Machine Translation",
    "latest": "2023-05-24T19:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425345363936146",
      "content": "<p>\ud83d\udcdd Non-Parametric, Nearest-Neighbor-Assisted Fine-Tuning for Neural Machine Translation \ud83d\udcda\ud83d\udc7e</p><p>\"Uses a non-parametric kNN method to improve machine translation by incorporating statistics into the gradient update of a fine-tuned baseline NMT model with a gating mechanism, the ground truth probability from the kNN model, and a reinforcement learning method.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13648v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13648v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14337",
    "title": "Anchor Prediction: Automatic Refinement of Internet Links",
    "latest": "2023-05-24T19:19:34+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110425274766638680",
      "content": "<p>RT @nelsonfliu@twitter.com</p><p>Most links on the Internet are unanchored\u2014they simply point to a target webpage as a whole.</p><p>We define and explore the task of *anchor prediction*: given a link in its source context, can we directly point readers to relevant parts of the target webpage?</p><p><a href=\"https://arxiv.org/abs/2305.14337\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14337</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/nelsonfliu/status/1661265876912582658\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/nelsonfliu/status/</span><span class=\"invisible\">1661265876912582658</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13645v1",
    "title": "mPMR: A Multilingual Pre-trained Machine Reader at Scale",
    "latest": "2023-05-24T19:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425266881405590",
      "content": "<p>\ud83d\udcdd mPMR: A Multilingual Pre-Trained Machine Reader at Scale \ud83d\udcda</p><p>\"MPMR pre-trains mPLMs with multilingual machine reading comprehension data and fine-tunes mPLMs with downstream tasks to perform sequence classification and span extraction, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13645v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13645v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.14342",
    "title": "http://arxiv.org/abs/2305.14342",
    "latest": "2023-05-24T19:15:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110425258953264039",
      "content": "<p>RT @tengyuma@twitter.com</p><p>Adam, a 9-yr old optimizer, is the go-to for training LLMs (eg, GPT-3, OPT, LLAMA).</p><p>Introducing Sophia, a new optimizer that is 2x faster than Adam on LLMs. Just a few more lines of code could cut your costs from $2M to $1M (if scaling laws hold).</p><p><a href=\"http://arxiv.org/abs/2305.14342\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.14342</span><span class=\"invisible\"></span></a> \ud83e\uddf5\u2b07\ufe0f</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/tengyuma/status/1661412995430219786\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/tengyuma/status/16</span><span class=\"invisible\">61412995430219786</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13641v1",
    "title": "https://arxiv.org/abs/2305.13641v1",
    "latest": "2023-05-24T18:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425188057946085",
      "content": "<p>\ud83d\udcdd AxomiyaBERTa: A Phonologically-Aware Transformer Model for Assamese \ud83d\udcda</p><p>\"The training procedure of AxomiyaBERTa is different from standard BERT models: it only uses the masked language modeling training objective, while standard models use both masked language modeling as well as the next-sentence prediction objective.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/csu-signal/axomiyaberta\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/csu-signal/axomiyab</span><span class=\"invisible\">erta</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13641v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13641v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1057/s41304-023-00431-y",
    "title": "Alternative metrics, traditional problems? Assessing gender dynamics in the altmetrics of political science - European Political Science",
    "latest": "2023-05-24T18:49:32+00:00",
    "last_post": {
      "url": "https://fediscience.org/@petersuber/110425156663480927",
      "content": "<p>Update. In political science, \"journal articles authored exclusively by female scholars score 27% lower on average [on Altmetric Attention Scores, AAS] than exclusively male-authored outputs. However, men are also more likely to write articles with an AAS of zero. These patterns are shaped by the presence of high-scoring male 'superstars' whose research attracts much online attention.\"<br><a href=\"https://link.springer.com/article/10.1057/s41304-023-00431-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">057/s41304-023-00431-y</span></a></p><p><a href=\"https://fediscience.org/tags/Altmetrics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Altmetrics</span></a> <a href=\"https://fediscience.org/tags/Gender\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Gender</span></a> <a href=\"https://fediscience.org/tags/PoliticalScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PoliticalScience</span></a></p>"
    },
    "people": [
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      },
      {
        "url": "https://fediscience.org/@petersuber",
        "display_name": "petersuber"
      }
    ]
  },
  {
    "link": "https://journals.sagepub.com/doi/10.1177/1532673X231175625",
    "title": "journals.sagepub.com/doi/10.11...",
    "latest": "2023-05-24T18:41:38+00:00",
    "last_post": {
      "url": "https://mastodon.social/@owasow/110425125607399404",
      "content": "<p>New study finds people in Iowa \u201cliving closer to BLM protests show greater support for BLM movement and, to a lesser extent, for defunding the police. Results suggest protests may affect public opinion, but only within a very narrow range of a few miles.\u201d <a href=\"https://journals.sagepub.com/doi/10.1177/1532673X231175625\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.sagepub.com/doi/10.11</span><span class=\"invisible\">77/1532673X231175625</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@owasow",
        "display_name": "Omar Wasow"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13632v1",
    "title": "https://arxiv.org/abs/2305.13632v1",
    "latest": "2023-05-24T18:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425109452118197",
      "content": "<p>\ud83d\udcdd Detecting and Mitigating Hallucinations in Multilingual Summarisation \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"MFACT is a novel metric for the faithfulness evaluation of generated summaries on non-English language, which transfers existing English metric FACT using multi-lingual pretrained model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/yfqiu-nlp/mfact-summ\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/yfqiu-nlp/mfact-sum</span><span class=\"invisible\">m</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13632v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13632v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.acalib.2023.102734",
    "title": "doi.org/10.1016/j.acalib.2023....",
    "latest": "2023-05-24T18:28:24+00:00",
    "last_post": {
      "url": "https://fediscience.org/@petersuber/110425073539653825",
      "content": "<p>More on the <a href=\"https://fediscience.org/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> citation advantage (<a href=\"https://fediscience.org/tags/OACA\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OACA</span></a>). <br><a href=\"https://doi.org/10.1016/j.acalib.2023.102734\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.acalib.2023.</span><span class=\"invisible\">102734</span></a></p><p>\"Articles under the hybrid gold modality are cited on average twice as much as those in the gold modality, regardless of funding\u2026Funded articles generally obtain 50% more citations than unfunded ones within the same publication modality. Open access <a href=\"https://fediscience.org/tags/repositories\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>repositories</span></a> significantly increase citations, particularly for articles w/o funding\u2026Articles in OA repositories receive 50% more citations than paywalled ones.\"</p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@petersuber",
        "display_name": "petersuber"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13637v1",
    "title": "IdEALS: Idiomatic Expressions for Advancement of Language Skills",
    "latest": "2023-05-24T18:27:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425070148083340",
      "content": "<p>\ud83d\udcdd IdEALS: Idiomatic Expressions for Advancement of Language Skills \ud83d\udcda</p><p>\"We curate training and testing corpora from real-world data, then evaluate various approaches and compare their performance against human experts and existing GEC tools, including Grammarly, LanguageTool and Grammarly's Customized Grammar Assistant (GCA).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13637v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13637v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13628v1",
    "title": "Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning",
    "latest": "2023-05-24T18:07:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424991511924398",
      "content": "<p>\ud83d\udcdd Improving Self-Training for Cross-Lingual Named Entity Recognition with Contrastive and Prototype Learning \ud83d\udcda</p><p>\"Contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13628v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13628v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05363",
    "title": "https://doi.org/10.21105/joss.05363",
    "latest": "2023-05-24T18:06:15+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110424986421050772",
      "content": "<p>Just published in JOSS: 'sptotal: an R package for predicting totals and weighted sums from spatial data' <a href=\"https://doi.org/10.21105/joss.05363\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05363</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13627v1",
    "title": "https://arxiv.org/abs/2305.13627v1",
    "latest": "2023-05-24T17:57:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424952229757790",
      "content": "<p>\ud83d\udcdd Instruct-Align: Teaching Novel Languages with to LLMs Through Alignment-Based Cross-Lingual Instruction \ud83d\udcda\ud83d\udc7e</p><p>\"By learning cross-lingual alignment between unseen and previously learned languages via alignment-based cross-lingual instruction tuning and experience replay of previously learned languages via continual instruction tuning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/IndoNLP/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/IndoNLP/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13627v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13627v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05226",
    "title": "https://doi.org/10.21105/joss.05226",
    "latest": "2023-05-24T17:44:33+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110424901150947173",
      "content": "<p>Just published in JOSS: 'osiris: An R package to process climate impacts on agricultural yields for the Global Change Analysis Model' <a href=\"https://doi.org/10.21105/joss.05226\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05226</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13626v1",
    "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
    "latest": "2023-05-24T17:37:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424873715485077",
      "content": "<p>\ud83d\udcdd Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-Guided, and Non-Collaboration \ud83d\udcda</p><p>\"A Proactive Chain-of-Thought prompting scheme is proposed to trigger the goal planning capability over descriptive reasoning chains and equip LLMs with proactivity, especially, for three aspects: clarification, target-guided, and non-collaborative dialogues.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13626v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13626v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13617v1",
    "title": "https://arxiv.org/abs/2305.13617v1",
    "latest": "2023-05-24T17:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424795004115201",
      "content": "<p>\ud83d\udcdd SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"SPEECH first models complex dependency among event structured components with energy-based modeling, and then represents event classes with simple hyperspheres, where the hyperspheres can be learned with only a few annotated event instances.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/zjunlp/SPEECH\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/zjunlp/SPEECH</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13617v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13617v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13945",
    "title": "Wikipedia and open access",
    "latest": "2023-05-24T17:05:12+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110424746369594426",
      "content": "<p>RT by <span class=\"h-card\"><a href=\"https://mastodon.social/@wikiresearch\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>wikiresearch</span></a></span>: Great to see this new study that found that <a href=\"https://mastodon.social/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> articles are extensively more cited in <span class=\"h-card\"><a href=\"https://mastodon.social/@Wikipedia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Wikipedia</span></a></span> &amp; show a 15% higher likelihood of being cited in Wikipedia when compared to paywalled articles <a href=\"https://arxiv.org/abs/2305.13945\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13945</span><span class=\"invisible\"></span></a></p><p>Our collaborations are paying off! @jimmy_wales <a href=\"https://twitter.com/melhagemann/status/1661395869332062208\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/melhagemann/status</span><span class=\"invisible\">/1661395869332062208</span></a></p>"
    },
    "people": [
      {
        "url": "https://dair-community.social/@DrVeronikaCH",
        "display_name": "Veronika Cheplygina"
      },
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41586-023-06094-5",
    "title": "Walking naturally after spinal cord injury using a brain\u2013spine interface - Nature",
    "latest": "2023-05-24T17:00:04+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110424726190746469",
      "content": "<p>Walking naturally after spinal cord injury using a brain\u2013spine interface - <a href=\"https://www.nature.com/articles/s41586-023-06094-5\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41586-023</span><span class=\"invisible\">-06094-5</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13602v1",
    "title": "https://arxiv.org/abs/2305.13602v1",
    "latest": "2023-05-24T16:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424716178234241",
      "content": "<p>\ud83d\udcdd ReSee: Responding Through Seeing Fine-Grained Visual Knowledge in Open-Domain Dialogue \ud83d\udcda</p><p>\"Provides new paradigm to construct multimodal dialogue datasets from text-only dialogues, and a simple but effective dialogue model ReSee to add visual representation into vanilla dialogue models by modality concatenations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Maluuba/nlg-eval\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Maluuba/nlg-eval</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13602v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13602v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.trd.2023.103757",
    "title": "doi.org/10.1016/j.trd.2023.103...",
    "latest": "2023-05-24T16:49:53+00:00",
    "last_post": {
      "url": "https://datasci.social/@UrbanDemog/110424592523320991",
      "content": "<p>Super glad to see our study \"Estimating public transport emissions from GTFS data\" finally published on Transp. Research Part D. Amazing collaboration with Joao Bazzo &amp; P. Andrade <br>\ud83d\udcd1Paper: <a href=\"https://doi.org/10.1016/j.trd.2023.103757\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.trd.2023.103</span><span class=\"invisible\">757</span></a> <br>\ud83d\udd13ungated PDF: <a href=\"https://urbandemographics.org/publication/2023_trpd_public_transport_emissions_gtfs2emis/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">urbandemographics.org/publicat</span><span class=\"invisible\">ion/2023_trpd_public_transport_emissions_gtfs2emis/</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      },
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13589v1",
    "title": "BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases",
    "latest": "2023-05-24T16:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424637611065719",
      "content": "<p>\ud83d\udcdd BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases \ud83d\udcda</p><p>\"BiasX is a framework that enables content moderators to provide free-text explanations in support of toxicity decisions, alongside their binary (toxic or not) decisions.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13589v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13589v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41893-023-01132-6",
    "title": "Quantifying the human cost of global warming - Nature Sustainability",
    "latest": "2023-05-24T16:12:04+00:00",
    "last_post": {
      "url": "https://botsin.space/@kottke/110424537435653767",
      "content": "<p>Quantifying the human cost of global warming: because of climate change, over 600 million people currently live outside the \"human climate niche\". That could rise to more than 1/3 of the total global population by the end of the century. <a href=\"https://www.nature.com/articles/s41893-023-01132-6\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41893-023</span><span class=\"invisible\">-01132-6</span></a></p>"
    },
    "people": [
      {
        "url": "https://occult.institute/@maya",
        "display_name": "maya \ud83e\uded0"
      },
      {
        "url": "https://botsin.space/@kottke",
        "display_name": "kottke.org"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12972",
    "title": "VanillaNet: the Power of Minimalism in Deep Learning",
    "latest": "2023-05-24T16:00:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110424490192297674",
      "content": "<p>VanillaNet: The Power of Minimalism in Deep Learning - <a href=\"https://arxiv.org/abs/2305.12972\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12972</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686",
    "title": "Cross-validation: what does it estimate and how well does it do it?",
    "latest": "2023-05-24T15:59:23+00:00",
    "last_post": {
      "url": "https://sciences.social/@klauspforr/110424448086940405",
      "content": "<p>wow <a href=\"https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/full/10.10</span><span class=\"invisible\">80/01621459.2023.2197686</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13585v1",
    "title": "Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs",
    "latest": "2023-05-24T15:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424480274780649",
      "content": "<p>\ud83d\udcdd Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs \ud83d\udcda</p><p>\"Proposes a structure-modeled textual encoding framework for inductive logical reasoning over KGs to find answers for complex logical queries with the structure-modeled instructions and the pre-trained encoder.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13585v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13585v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13583v1",
    "title": "Cross-Attention is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition",
    "latest": "2023-05-24T15:47:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424441061550977",
      "content": "<p>\ud83d\udcdd Cross-Attention Is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition \ud83d\udcda</p><p>\"Proposes a lightweight model via Hierarchical Crossmodal Transformer with Modality Gating (HCT-MG), which determines a primary modality according to its contribution to the target task and then hierarchically incorporates auxiliary modalities to alleviate inter-modal incongruity and reduce information redundancy.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13583v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13583v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13582v1",
    "title": "Better Low-Resource Entity Recognition Through Translation and Annotation Fusion",
    "latest": "2023-05-24T15:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424401688948635",
      "content": "<p>\ud83d\udcdd Better Low-Resource Entity Recognition Through Translation and Annotation Fusion \ud83d\udcda</p><p>\"TransFusion is a framework which translates text into a high-resource language for annotation using fully supervised models, and then fuses those annotations back into the low-resource language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13582v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13582v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.17709",
    "title": "Teru Teru B\u014dzu: Defensive Raincloud Plots",
    "latest": "2023-05-24T15:19:24+00:00",
    "last_post": {
      "url": "https://vis.social/@EuroVis/110423556294707191",
      "content": "<p>\ud83d\udcdc&nbsp; Raincloud plots show distributions in multiple ways. But are they more than the sum of their parts?<br>\u270d\ufe0f <span class=\"h-card\"><a href=\"https://jorts.horse/@Birdbassador\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Birdbassador</span></a></span><br>\ud83d\udc49 <a href=\"https://arxiv.org/abs/2303.17709\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.17709</span><span class=\"invisible\"></span></a><br><a href=\"https://vis.social/tags/Fullpaper\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Fullpaper</span></a> <a href=\"https://vis.social/tags/EuroVis\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>EuroVis</span></a> <a href=\"https://vis.social/tags/Eurovis2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Eurovis2023</span></a></p>"
    },
    "people": [
      {
        "url": "https://vis.social/@lane",
        "display_name": "Lane Harrison"
      },
      {
        "url": "https://vis.social/@mcnutt",
        "display_name": "Andrew McNutt"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13571v1",
    "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
    "latest": "2023-05-24T15:17:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424323106007438",
      "content": "<p>\ud83d\udcdd Latent Positional Information Is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings \ud83d\udcda</p><p>\"Transformer language models encode strong positional information without positional embeddings through shrinkage of the self-attention variance in each layer, which is not affected by gradient updates during pretraining.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13571v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13571v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14334",
    "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence",
    "latest": "2023-05-24T09:58:54+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110423070110153900",
      "content": "<p>Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence</p><p>Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski  Trevor Darrell</p><p>tl;dr: diffusion features are good descriptors for semantic corrs, if aggregated among timesteps.</p><p><a href=\"https://arxiv.org/abs/2305.14334\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14334</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41467-023-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D",
    "title": "Spatially-optimized urban greening for reduction of population exposure to land surface temperature extremes | Nature Communications",
    "latest": "2023-05-24T09:55:34+00:00",
    "last_post": {
      "url": "https://datasci.social/@mszll/110423057006850663",
      "content": "<p>Spatially-optimized urban greening for reduction of population exposure to land surface temperature extremes</p><p><a href=\"https://www.nature.com/articles/s41467-023-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41467-023</span><span class=\"invisible\">-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D</span></a></p>"
    },
    "people": [
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13705",
    "title": "DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models",
    "latest": "2023-05-24T09:52:59+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110423046869119371",
      "content": "<p>DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models</p><p>Lijun Li, Li'an Zhuo, Bang Zhang, Liefeng Bo, Chen Chen</p><p>tl;dr: diffusion models can do mesh reconstruction.<br><a href=\"https://arxiv.org/abs/2305.13705\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13705</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13455v1",
    "title": "https://arxiv.org/abs/2305.13455v1",
    "latest": "2023-05-24T09:37:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422986361169984",
      "content": "<p>\ud83d\udcdd Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents \ud83d\udcda</p><p>\"Presents a general framework for implementing and evaluating games with LLMs, including five concrete games as an exemplary set of settings to probe a range of capabilities (e.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clp-research/clembench\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/clp-research/clembe</span><span class=\"invisible\">nch</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13455v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13455v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13417v1",
    "title": "https://arxiv.org/abs/2305.13417v1",
    "latest": "2023-05-24T09:27:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422947048051474",
      "content": "<p>\ud83d\udcdd Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT \ud83d\udcda</p><p>\"Our visualization tool is created on top of the attention mechanism to visualize the flow of information inside the model by analyzing the tokens projected from the attention heads and memory values.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/shacharKZ/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/shacharKZ/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13417v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13417v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13413v1",
    "title": "Syntactic Knowledge via Graph Attention with BERT in Machine Translation",
    "latest": "2023-05-24T08:57:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422828823261253",
      "content": "<p>\ud83d\udcdd Syntactic Knowledge via Graph Attention with BERT in Machine Translation \ud83d\udcda</p><p>\"GAT and BERT jointly represent syntactic dependency feature as explicit knowledge of the source language to enrich source language representations and guide target language generation, which can improve translation quality across machine translation tasks without sacrificing BLEU scores.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13413v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13413v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1080/03080188.2022.2150807",
    "title": "doi.org/10.1080/03080188.2022....",
    "latest": "2023-05-24T08:42:33+00:00",
    "last_post": {
      "url": "https://mastodon.social/@lakens/110422769862919296",
      "content": "<p>Lovely paper on facts and objectivity by Philippe Stamenkovic <a href=\"https://doi.org/10.1080/03080188.2022.2150807\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1080/03080188.2022.</span><span class=\"invisible\">2150807</span></a>. Extremely accessible introduction to this topic. Some quotes: </p><p>\u201cObjectivity is the source of the authority which science enjoys in society, and a precondition of public trust in science: it is one of the main reasons (alongside truth) why we value science.\u201d</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      },
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13412v1",
    "title": "https://arxiv.org/abs/2305.13412v1",
    "latest": "2023-05-24T08:37:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422750300473291",
      "content": "<p>\ud83d\udcdd Element-Aware Summarization with Large Language Models: Expert-Aligned Evaluation and Chain-of-Thought Method \ud83d\udcda</p><p>\"Proposes a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with human writing mindset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Alsace08/SumCoT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Alsace08/SumCoT</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13412v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13412v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/00031224231168074",
    "title": "doi.org/10.1177/00031224231168...",
    "latest": "2023-05-24T08:22:15+00:00",
    "last_post": {
      "url": "https://fediscience.org/@MarkRubin/110417275690043774",
      "content": "<p>New study finds \u201carticles that develop and present a new method tend to be more disruptive.\u201d <a href=\"https://doi.org/10.1177/00031224231168074\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/00031224231168</span><span class=\"invisible\">074</span></a></p><p>Reminds me of Greenwald (2012): \u201cAn analysis of the recent history of Nobel Prizes in science unexpectedly revealed that these awards were given much more often for creation of methods and for method-based discoveries than for developments of new theory\u201d: <a href=\"https://doi.org/10.1177/1745691611434210\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/17456916114342</span><span class=\"invisible\">10</span></a> </p><p><a href=\"https://fediscience.org/tags/Science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Science</span></a><br><a href=\"https://fediscience.org/tags/Theory\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Theory</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/1745691611434210",
    "title": "doi.org/10.1177/17456916114342...",
    "latest": "2023-05-24T08:22:15+00:00",
    "last_post": {
      "url": "https://fediscience.org/@MarkRubin/110417275690043774",
      "content": "<p>New study finds \u201carticles that develop and present a new method tend to be more disruptive.\u201d <a href=\"https://doi.org/10.1177/00031224231168074\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/00031224231168</span><span class=\"invisible\">074</span></a></p><p>Reminds me of Greenwald (2012): \u201cAn analysis of the recent history of Nobel Prizes in science unexpectedly revealed that these awards were given much more often for creation of methods and for method-based discoveries than for developments of new theory\u201d: <a href=\"https://doi.org/10.1177/1745691611434210\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/17456916114342</span><span class=\"invisible\">10</span></a> </p><p><a href=\"https://fediscience.org/tags/Science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Science</span></a><br><a href=\"https://fediscience.org/tags/Theory\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Theory</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.09676",
    "title": "Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications",
    "latest": "2023-05-24T08:08:02+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mymarkup/110422625514652656",
      "content": "<p>Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications</p><p>\u201dBroadly, our findings suggest that though multidisciplinary venues intend to cater to more general audiences, some fields' writing norms may act as barriers rather than bridges, and thus impede the dispersion of scholarly ideas.\u201d</p><p><a href=\"https://arxiv.org/abs/2212.09676\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2212.09676</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.social/tags/preprintwatch\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>preprintwatch</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13403v1",
    "title": "https://arxiv.org/abs/2305.13403v1",
    "latest": "2023-05-24T08:07:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422632150721880",
      "content": "<p>\ud83d\udcdd GATology for Linguistics: What Syntactic Dependencies It Knows \ud83d\udcda</p><p>\"A graph neural network which is a strategy for modeling and representing explicit syntactic knowledge, and can work with pre-trained models such as BERT in downstream tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/UniversalDependencies/UD_\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/UniversalDependenci</span><span class=\"invisible\">es/UD_</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13403v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13403v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/25152459211007467",
    "title": "doi.org/10.1177/25152459211007...",
    "latest": "2023-05-24T07:56:53+00:00",
    "last_post": {
      "url": "https://ecoevo.social/@sortee/110422354226072945",
      "content": "<p>Evidence from psychology shows that <a href=\"https://ecoevo.social/tags/registeredreports\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>registeredreports</span></a> have a lower positive result rate than the standard literature &amp; likely reduces <a href=\"https://ecoevo.social/tags/publicationbias\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>publicationbias</span></a> &amp; QRPs </p><p>\ud83d\udcf0by <span class=\"h-card\"><a href=\"https://mastodon.online/@annescheel\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>annescheel</span></a></span> Mitchell Schijen &amp; <span class=\"h-card\"><a href=\"https://mastodon.social/@lakens\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lakens</span></a></span>  <a href=\"https://doi.org/10.1177/25152459211007467\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/25152459211007</span><span class=\"invisible\">467</span></a></p><p>Time for eco &amp; evo to support Registered Reports!</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13401v1",
    "title": "https://arxiv.org/abs/2305.13401v1",
    "latest": "2023-05-24T07:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422514226278980",
      "content": "<p>\ud83d\udcdd A Study of Conceptual Language Similarity: Comparison and Evaluation \ud83d\udcda</p><p>\"The conceptual similarity is based on the hypothesis that two languages are similar if they encode the meanings of concepts using similar linguistic structures, such as word order and verbal inflection.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/DianDYu/language\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/DianDYu/language</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13401v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13401v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13386v1",
    "title": "Can LLMs facilitate interpretation of pre-trained language models?",
    "latest": "2023-05-24T07:27:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422474828156047",
      "content": "<p>\ud83d\udcdd Can LLMs Facilitate Interpretation of Pre-Trained Language Models? \ud83d\udcda</p><p>\"A: Proposes using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models (LM).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13386v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13386v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://doi.org/10.18452/25440",
    "title": "http://doi.org/10.18452/25440",
    "latest": "2023-05-24T07:26:43+00:00",
    "last_post": {
      "url": "https://mastodon.social/@hauschke/110422471735229407",
      "content": "<p>Ulrike K\u00fcsters vom IRB stellt das Positionspapier der <span class=\"h-card\"><a href=\"https://openbiblio.social/@dini\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>dini</span></a></span> AG FIS  vor:</p><p>Management von Forschungsinformationen in Hochschulen und Forschungseinrichtungen. Eine Standortbestimmung (2022)</p><p><a href=\"http://doi.org/10.18452/25440\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">doi.org/10.18452/25440</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.social/tags/111bibliocon\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>111bibliocon</span></a> <a href=\"https://mastodon.social/tags/FIS\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>FIS</span></a> <a href=\"https://mastodon.social/tags/Forschungsberichterstattung\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Forschungsberichterstattung</span></a> <a href=\"https://mastodon.social/tags/Forschungsinformationssystem\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Forschungsinformationssystem</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@hauschke",
        "display_name": "hauschke"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280902",
    "title": "The efficacy of interventions in reducing belief in conspiracy theories: A systematic review",
    "latest": "2023-05-24T06:30:50+00:00",
    "last_post": {
      "url": "https://mastodon.social/@matthewfacciani/110395774887850512",
      "content": "<p>Meta-analysis reveals that teaching people how to think critically (i.e. prebunking) combats misinformation more effectively than trying to debunk false claims. <br><a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280902\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0280902</span></a><br><a href=\"https://mastodon.social/tags/misinformation\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>misinformation</span></a> <a href=\"https://mastodon.social/tags/MisinfoResearch\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MisinfoResearch</span></a> <a href=\"https://mastodon.social/tags/prebunking\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>prebunking</span></a> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/sociology\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>sociology</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/psychology\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>psychology</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/communicationscholars\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>communicationscholars</span></a></span></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@dougholton",
        "display_name": "Doug Holton"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.11140.pdf",
    "title": "https://arxiv.org/pdf/2305.11140.pdf",
    "latest": "2023-05-24T06:21:09+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@fbk_mt/110422213868329357",
      "content": "<p>Our pick of the week by Beatrice Savoldi: \"Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model\" Amrhein et al., 2023</p><p><a href=\"https://arxiv.org/pdf/2305.11140.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.11140.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://sigmoid.social/tags/pickoftheweek\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>pickoftheweek</span></a> <a href=\"https://sigmoid.social/tags/gender\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>gender</span></a> <a href=\"https://sigmoid.social/tags/bias\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>bias</span></a> <a href=\"https://sigmoid.social/tags/fair\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>fair</span></a> <a href=\"https://sigmoid.social/tags/fairness\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>fairness</span></a> <a href=\"https://sigmoid.social/tags/debiasing\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>debiasing</span></a> <a href=\"https://sigmoid.social/tags/translation\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>translation</span></a> <a href=\"https://sigmoid.social/tags/MT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MT</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@fbk_mt",
        "display_name": "MT Group at FBK"
      }
    ]
  },
  {
    "link": "https://academic.oup.com/pnasnexus/article/2/3/pgad051/7059318?login=false",
    "title": "Complex systems of secrecy: the offshore networks of oligarchs",
    "latest": "2023-05-24T06:01:16+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mikeolson/110422135730518945",
      "content": "<p>Kleptocracy depends on good infrastructure for money laundering. That infrastructure operates worldwide, but the US and UK provide a great deal of it. The most effective way to sanction criminals, make money laundering much harder and penalize corruption and kleptocracy would be to impose know-your-customer and reporting obligations on lawyers and accountants.</p><p>This post comes to you thanks to this excellent PNAS paper:</p><p><a href=\"https://academic.oup.com/pnasnexus/article/2/3/pgad051/7059318?login=false\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">academic.oup.com/pnasnexus/art</span><span class=\"invisible\">icle/2/3/pgad051/7059318?login=false</span></a></p><p>Came my way via 3QD:</p><p><a href=\"https://3quarksdaily.com/3quarksdaily/2023/05/sean-carrolls-mindscape-podcast-brooke-harrington-on-offshore-wealth-as-a-complex-system.html\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">3quarksdaily.com/3quarksdaily/</span><span class=\"invisible\">2023/05/sean-carrolls-mindscape-podcast-brooke-harrington-on-offshore-wealth-as-a-complex-system.html</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@mikeolson",
        "display_name": "Mike Olson"
      }
    ]
  },
  {
    "link": "https://doi.org/10.36253/jlis.it-498",
    "title": "https://doi.org/10.36253/jlis.it-498",
    "latest": "2023-05-24T05:53:57+00:00",
    "last_post": {
      "url": "https://hcommons.social/@jcpeyssard/110422106908824416",
      "content": "<p>Snijder, Ronald. 2023. \u201cBooks in a bubble.: Assessing the OAPEN Library Collection\u201d. JLIS.It 14 (2):75-92. <a href=\"https://doi.org/10.36253/jlis.it-498\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.36253/jlis.it-498</span><span class=\"invisible\"></span></a>.</p>"
    },
    "people": [
      {
        "url": "https://hcommons.social/@jcpeyssard",
        "display_name": "Jean-Christophe Peyssard"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2210.06927",
    "title": "Prediction can be safely used as a proxy for explanation in causally consistent Bayesian generalized linear models",
    "latest": "2023-05-24T04:20:36+00:00",
    "last_post": {
      "url": "https://fediscience.org/@scholzmx/110417237170725257",
      "content": "<p>Updated preprint is up on arxiv. It is now focused on just the prediction as proxy for explanation question. Finding remains that prediction is a valid and reliable proxy for explanation in causally unbiased GLMs when comparing models with the same linear predictor term.<br><a href=\"https://arxiv.org/abs/2210.06927\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2210.06927</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.08167.pdf",
    "title": "https://arxiv.org/pdf/2304.08167.pdf",
    "latest": "2023-05-24T03:49:39+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618137001202",
      "content": "<p>\"Classification of news spreading barriers\" an approach to barrier classification where the semantics of news articles are inferred through Wikipedia concepts. </p><p>(Sittar. et al, 2023)</p><p><a href=\"https://arxiv.org/pdf/2304.08167.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.08167.pdf</span><span class=\"invisible\"></span></a> <a href=\"https://twitter.com/WikiResearch/status/1660675562862518272\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1660675562862518272</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/pdf/10.1145/3579517",
    "title": "dl.acm.org/doi/pdf/10.1145/357...",
    "latest": "2023-05-24T03:49:39+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618163571101",
      "content": "<p>\"Queer Identities, Normative Databases: Challenges to Capturing Queerness On @Wikidata\", showing inherent and unaddressed frictions when translating queer identities to the confines of a structured database. </p><p>( Weathington and Brubaker 2023)</p><p><a href=\"https://dl.acm.org/doi/pdf/10.1145/3579517\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/357</span><span class=\"invisible\">9517</span></a> <a href=\"https://twitter.com/WikiResearch/status/1661060518772195335\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1661060518772195335</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09497",
    "title": "Curious Rhythms: Temporal Regularities of Wikipedia Consumption",
    "latest": "2023-05-24T03:49:38+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618084830656",
      "content": "<p>\"Curious Rhythms: Temporal Regularities of <span class=\"h-card\"><a href=\"https://mastodon.social/@Wikipedia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Wikipedia</span></a></span> Consumption\" a large-scale analysis of billions of timezone-corrected page requests mined from English Wikipedia's server logs.</p><p>(Piccardi et al, 2023)</p><p><a href=\"https://arxiv.org/abs/2305.09497\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09497</span><span class=\"invisible\"></span></a> <a href=\"https://twitter.com/WikiResearch/status/1658871787692736512\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1658871787692736512</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.05403.pdf",
    "title": "https://arxiv.org/pdf/2305.05403.pdf",
    "latest": "2023-05-24T03:49:37+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618061046604",
      "content": "<p>\"Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey\" including fundamental methodologies and practice-oriented recommendations on how to choose between different approaches for a specific problem</p><p>(Razniewski et al, 2023)</p><p><a href=\"https://arxiv.org/pdf/2305.05403.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.05403.pdf</span><span class=\"invisible\"></span></a><br>@Bosch_AI <a href=\"https://twitter.com/WikiResearch/status/1658472419185786880\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1658472419185786880</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13047",
    "title": "Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media",
    "latest": "2023-05-24T02:18:03+00:00",
    "last_post": {
      "url": "https://mastodon.social/@andreskarjus/110417263335240534",
      "content": "<p>Preprint w Mark Mets <span class=\"h-card\"><a href=\"https://mastodon.social/@IndrekIbrus\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>IndrekIbrus</span></a></span> <span class=\"h-card\"><a href=\"https://mastodon.social/@schichmax\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>schichmax</span></a></span> <br>\"Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media\"<br><a href=\"https://arxiv.org/abs/2305.13047\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13047</span><span class=\"invisible\"></span></a><br>We compare 2 corpora, a mainstream publisher &amp; a rightwing populist online outlet. Rightwingers are ofc more negative &amp;talk more about immigration - but now we can quantify. Compared multiple LLMs &amp;ChatGPT3.5, which gets same F1 as a finetuned RoBERTa. But zero-shot! That's pretty awesome.</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14224",
    "title": "mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations",
    "latest": "2023-05-24T02:06:24+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421212155208290",
      "content": "<p>mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations</p><p>Outperforms mT5 at the same parameter sizes by a large margin on NLU and NLG tasks in 40+ languages.</p><p><a href=\"https://arxiv.org/abs/2305.14224\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14224</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14177",
    "title": "ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry",
    "latest": "2023-05-24T01:58:09+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421179739560551",
      "content": "<p>ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry</p><p>Presents a set of open-source RL environments, ChemGymRL, based on the Open AI Gym template</p><p><a href=\"https://arxiv.org/abs/2305.14177\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14177</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13840",
    "title": "https://arxiv.org/abs/2305.13840",
    "latest": "2023-05-24T01:54:16+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421164483028998",
      "content": "<p>Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models</p><p>proj: <a href=\"https://controlavideo.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">controlavideo.github.io/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.13840\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13840</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14330",
    "title": "https://arxiv.org/abs/2305.14330",
    "latest": "2023-05-24T01:33:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421081480423378",
      "content": "<p>Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation</p><p>repo: <a href=\"https://github.com/KU-CVLAB/DirecT2V\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/KU-CVLAB/DirecT2V</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14330\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14330</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14233",
    "title": "https://arxiv.org/abs/2305.14233",
    "latest": "2023-05-24T01:30:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421069670056432",
      "content": "<p>Enhancing Chat Language Models by Scaling High-quality Instructional Conversations</p><p>- UltraChat contains 1.5M high-quality, diverse multi-turn dialogues<br>- UltraLLaMA outperforms the SotA open-source model, Vicuna </p><p>repo: <a href=\"https://github.com/thunlp/UltraChat\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/thunlp/UltraChat</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14233\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14233</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14325",
    "title": "https://arxiv.org/abs/2305.14325",
    "latest": "2023-05-24T01:12:11+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420998965406370",
      "content": "<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate</p><p>proj: <a href=\"https://composable-models.github.io/llm_debate/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">composable-models.github.io/ll</span><span class=\"invisible\">m_debate/</span></a><br>repo: <a href=\"https://github.com/composable-models/llm_multiagent_debate\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/composable-models/l</span><span class=\"invisible\">lm_multiagent_debate</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14325\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14325</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.00740",
    "title": "Inspecting and Editing Knowledge Representations in Language Models",
    "latest": "2023-05-24T01:06:13+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420975523967772",
      "content": "<p>RT @evanqed@twitter.com</p><p>You\u2019ve heard of model editing, now get ready for \ud83d\udcdd representation editing \ud83d\udcdd</p><p>In our new paper, we find directions in LM rep space that make the LM assert a fact is true.</p><p>Paper: <a href=\"https://arxiv.org/abs/2304.00740\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.00740</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/evandez/REMEDI\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/evandez/REMEDI</span><span class=\"invisible\"></span></a><br>w/ @belindazli@twitter.com @jacobandreas@twitter.com</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/evanqed/status/1661172005083791364\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/evanqed/status/166</span><span class=\"invisible\">1172005083791364</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13534",
    "title": "How Language Model Hallucinations Can Snowball",
    "latest": "2023-05-24T01:01:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420956304706540",
      "content": "<p>How Language Model Hallucinations Can Snowball</p><p><a href=\"https://arxiv.org/abs/2305.13534\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13534</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13735",
    "title": "Aligning Large Language Models through Synthetic Feedback",
    "latest": "2023-05-24T00:59:23+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420948643044011",
      "content": "<p>Aligning Large Language Models through Synthetic Feedback</p><p>Proposes a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs.</p><p><a href=\"https://arxiv.org/abs/2305.13735\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13735</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/1706.03762",
    "title": "https://arxiv.org/abs/1706.03762",
    "latest": "2023-05-23T22:49:13+00:00",
    "last_post": {
      "url": "https://mastodon.gamedev.place/@jay/110420242881451785",
      "content": "<p>GPT - Generatively Pre-trained Transformer <br> <br>\"Let's Build GPT. From Scratch. In Code. Spelled Out\" by Andrej Karpathy<br><a href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">youtube.com/watch?v=kCc8FmEb1n</span><span class=\"invisible\">Y</span></a><br>(Coding starts at 42:13 mark) <br> <br>\"ChatGPT... is trained on a good chunk of the Internet\" (3:26 mark)  \ud83e\udd14 &lt;-- Meet Mr. Copyright \u00a9 <br> <br>Andrej Karpathy<br><a href=\"https://build.microsoft.com/en-US/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8?source=/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">build.microsoft.com/en-US/spea</span><span class=\"invisible\">kers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8?source=/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8</span></a> <br> <br>\"Attention Is All You Need\" Seminal Paper on AI Transformers<br><a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/1706.03762</span><span class=\"invisible\"></span></a> <br> <br><a href=\"https://mastodon.gamedev.place/tags/Microsoft\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Microsoft</span></a> <a href=\"https://mastodon.gamedev.place/tags/MicrosoftBuild\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MicrosoftBuild</span></a> <a href=\"https://mastodon.gamedev.place/tags/Build2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Build2023</span></a> <a href=\"https://mastodon.gamedev.place/tags/ChatGPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ChatGPT</span></a> <a href=\"https://mastodon.gamedev.place/tags/GPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT</span></a> <a href=\"https://mastodon.gamedev.place/tags/OpenAI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAI</span></a> <a href=\"https://mastodon.gamedev.place/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://mastodon.gamedev.place/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <a href=\"https://mastodon.gamedev.place/tags/Copyright\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Copyright</span></a> <a href=\"https://mastodon.gamedev.place/tags/Infringement\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Infringement</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13252",
    "title": "\"According to ...\" Prompting Language Models Improves Quoting from Pre-Training Data",
    "latest": "2023-05-23T22:27:54+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420123288249295",
      "content": "<p>RT @orionweller@twitter.com</p><p>Can we guide LLMs to quote text from their pre-training data using prefixes like \"According To ..\", improving grounding and reducing hallucination? We discovered that LLMs do have this capability and can increase or decrease quoting on request \ud83e\udd2f</p><p>\ud83d\udcdd:<a href=\"https://arxiv.org/abs/2305.13252\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13252</span><span class=\"invisible\"></span></a> 1/5</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/orionweller/status/1660994241773047810\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/orionweller/status</span><span class=\"invisible\">/1660994241773047810</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://osf.io/fawj3",
    "title": "https://osf.io/fawj3",
    "latest": "2023-05-23T20:30:24+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@ElineVG/110419463003101195",
      "content": "<p>I had a wonderful time this morning presenting my poster on an efficient Bayesian observer model predicting attractive and repulsive temporal context effects @VSSMtg <a href=\"https://fosstodon.org/tags/VSS2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>VSS2023</span></a>! If you missed it and are interested, find the poster here: <a href=\"https://osf.io/fawj3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">osf.io/fawj3</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://scicomm.xyz/@sharoz",
        "display_name": "Steve Haroz (@sharoz on \ud83d\udc24)"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/20319525231167983",
    "title": "doi.org/10.1177/20319525231167...",
    "latest": "2023-05-23T20:19:14+00:00",
    "last_post": {
      "url": "https://someone.elses.computer/@mikarv/110258344105065543",
      "content": "<p>In case you\u2019ve been hearing about the draft EU <a href=\"https://someone.elses.computer/tags/PlatformWorkDirective\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PlatformWorkDirective</span></a> and wanted to learn more, our paper on its <a href=\"https://someone.elses.computer/tags/algorithmicManagement\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>algorithmicManagement</span></a> provisions (w Six Silberman &amp; <span class=\"h-card\"><a href=\"https://someone.elses.computer/@RDBinns\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>RDBinns</span></a></span>) now out in the European Labour Law Journal. <br>tl;dr: generally v well drafted, novel aspects worth studying, may create unintended effects, could benefit from some tweaks to avoid tensions and to shore up its complexities in a situation of high power asymmetries. <a href=\"https://doi.org/10.1177/20319525231167983\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/20319525231167</span><span class=\"invisible\">983</span></a> <a href=\"https://someone.elses.computer/tags/gigEconomy\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>gigEconomy</span></a> <a href=\"https://someone.elses.computer/tags/platformWork\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>platformWork</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.scot/@karengregory",
        "display_name": "Karen Gregory"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05322",
    "title": "https://doi.org/10.21105/joss.05322",
    "latest": "2023-05-23T20:08:48+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110419806006542749",
      "content": "<p>Just published in JOSS: 'SarcGraph: A Python package for analyzing the contractile behavior of pluripotent stem cell-derived cardiomyocytes' <a href=\"https://doi.org/10.21105/joss.05322\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05322</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13281",
    "title": "LM vs LM: Detecting Factual Errors via Cross Examination",
    "latest": "2023-05-23T19:28:39+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110419648178699377",
      "content": "<p>RT @johnjnay@twitter.com</p><p>LLM vs LLM: Detecting Errors via Cross Examining Agents</p><p>-An incorrect claim is likely to result in inconsistency w/ other claims<br>-Multi-turn interactions between LLM that generated claim and Examiner LLM</p><p>-Outperforms baselines across factual benchmarks</p><p><a href=\"https://arxiv.org/abs/2305.13281\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13281</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/johnjnay/status/1660830409276440579\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/johnjnay/status/16</span><span class=\"invisible\">60830409276440579</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09781",
    "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification",
    "latest": "2023-05-23T18:55:24+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110387182088605932",
      "content": "<p>SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification</p><p>Significantly reduces the end-to-end latency while provably preserving model quality</p><p><a href=\"https://arxiv.org/abs/2305.09781\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09781</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.sagepub.com/doi/10.1177/15554120221115393",
    "title": "journals.sagepub.com/doi/10.11...",
    "latest": "2023-05-23T18:55:08+00:00",
    "last_post": {
      "url": "https://scholar.social/@electricarchaeo/110419516378183749",
      "content": "<p>hell ya. I always give a little cheer when I see things that we put up on Epoiesen cited in other articles. Today, a reflection on writing history through games cites work that McCall published with us on Epoiesen: <a href=\"https://journals.sagepub.com/doi/10.1177/15554120221115393\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.sagepub.com/doi/10.11</span><span class=\"invisible\">77/15554120221115393</span></a></p>"
    },
    "people": [
      {
        "url": "https://scholar.social/@telliott",
        "display_name": "Tom Elliott"
      },
      {
        "url": "https://scholar.social/@electricarchaeo",
        "display_name": "Shawn Graham"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12182",
    "title": "https://arxiv.org/abs/2305.12182",
    "latest": "2023-05-23T18:42:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419465347473367",
      "content": "<p>Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages</p><p>repo: <a href=\"https://github.com/cisnlp/Glot500\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/cisnlp/Glot500</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.12182\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12182</span><span class=\"invisible\"></span></a><br>HuggingFace: <a href=\"https://huggingface.co/cis-lmu/glot500-base\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">huggingface.co/cis-lmu/glot500</span><span class=\"invisible\">-base</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13016",
    "title": "Iterative Forward Tuning Boosts In-context Learning in Language Models",
    "latest": "2023-05-23T18:41:59+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464644045437",
      "content": "<p>RT @huybery@twitter.com</p><p>In-context learning as the mysterious ability in LMs, and some works points to a link with gradient descent.<br>This inspired us to propose \ud83e\udd14deep-thinking, which boosts ICL by iterative forward tuning.<br>It is possible to tune LMs without backpropagation!</p><p><a href=\"https://arxiv.org/abs/2305.13016\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13016</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/huybery/status/1660835495310499840\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/huybery/status/166</span><span class=\"invisible\">0835495310499840</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13230",
    "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis",
    "latest": "2023-05-23T18:41:56+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464459802469",
      "content": "<p>To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis</p><p>Studies what would happen if we train LLM with repeated data and how we can alleviate the LLM mult-epoch degradation.</p><p><a href=\"https://arxiv.org/abs/2305.13230\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13230</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13144",
    "title": "Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline",
    "latest": "2023-05-23T18:41:53+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464272109741",
      "content": "<p>RT @zhengzangw@twitter.com</p><p>Can we use <a href=\"https://sigmoid.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> to speed up <a href=\"https://sigmoid.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> inference? \ud83d\udd25Excited to share our new work: Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline.\ud83e\uddf54</p><p>ArXiv: <a href=\"https://arxiv.org/abs/2305.13144\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13144</span><span class=\"invisible\"></span></a><br>Blog: <a href=\"https://zhengzangw.github.io/blogs/seqsch/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">zhengzangw.github.io/blogs/seq</span><span class=\"invisible\">sch/</span></a><br>Code: <a href=\"https://github.com/zhengzangw/Sequence-Scheduling\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/zhengzangw/Sequence</span><span class=\"invisible\">-Scheduling</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/zhengzangw/status/1660833269972074496\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/zhengzangw/status/</span><span class=\"invisible\">1660833269972074496</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13245",
    "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    "latest": "2023-05-23T18:41:50+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464061647289",
      "content": "<p>RT @michielsdj@twitter.com</p><p>New paper! Multi-query attention trades quality for speed and requires training a new model. Instead uptrain improved MQ variant from existing multi-head model! </p><p>Work with Joshua Ainslie, James Lee-Thorp, @_theopompus@twitter.com, Federico Lebron, Sumit Sanghai.</p><p><a href=\"https://arxiv.org/abs/2305.13245\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13245</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/michielsdj/status/1660839037903536129\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/michielsdj/status/</span><span class=\"invisible\">1660839037903536129</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12524",
    "title": "TheoremQA: A Theorem-driven Question Answering dataset",
    "latest": "2023-05-23T18:41:49+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463995882500",
      "content": "<p>RT @WenhuChen@twitter.com</p><p>New Arxiv: <a href=\"https://arxiv.org/abs/2305.12524\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12524</span><span class=\"invisible\"></span></a></p><p>GPT-4/PaLM-2 have both shown almost perfect performance on existing grade school math dataset. What about more challenging STEM questions, especially the ones which require specific theorems, like Stoke's theorem, Wiener Process, etc?</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/WenhuChen/status/1660832837715611648\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WenhuChen/status/1</span><span class=\"invisible\">660832837715611648</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13009",
    "title": "https://arxiv.org/abs/2305.13009",
    "latest": "2023-05-23T18:41:41+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463458618269",
      "content": "<p>Textually Pretrained Speech Language Models</p><p>Presents the largest SpeechLM both in terms of number of parameters and training data.</p><p>proj: <a href=\"https://pages.cs.huji.ac.il/adiyoss-lab/twist/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">pages.cs.huji.ac.il/adiyoss-la</span><span class=\"invisible\">b/twist/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.13009\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13009</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13035",
    "title": "Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design",
    "latest": "2023-05-23T18:41:38+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463271311196",
      "content": "<p>Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design</p><p>Their shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute.</p><p><a href=\"https://arxiv.org/abs/2305.13035\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13035</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://doi.org/10.54900/g0qks-tcz98",
    "title": "https://doi.org/10.54900/g0qks-tcz98",
    "latest": "2023-05-23T17:24:14+00:00",
    "last_post": {
      "url": "https://ravenation.club/@mpe/110419158940172271",
      "content": "<p>Attempts at automating journal subject classification by my @CrossrefOrg colleague, Esha Datta<br><a href=\"https://doi.org/10.54900/g0qks-tcz98\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.54900/g0qks-tcz98</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12234",
    "title": "The Damage to Lunar Orbiting Spacecraft Caused by the Ejecta of Lunar Landers",
    "latest": "2023-05-23T17:10:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110419103143950353",
      "content": "<p>The Damage to Lunar Orbiting Spacecraft Caused by the Ejecta of Lunar Landers - <a href=\"https://arxiv.org/abs/2305.12234\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12234</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://bayes.club/@akhilrao",
        "display_name": "edgeworth boxlord"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://doi.org/10.5281/zenodo.5526634",
    "title": "https://doi.org/10.5281/zenodo.5526634",
    "latest": "2023-05-23T17:01:30+00:00",
    "last_post": {
      "url": "https://mastodon.social/@brembs/110418324534008701",
      "content": "<p>Could this be the paradigm shift all of <a href=\"https://mastodon.social/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a> has been waiting for?</p><p>Council of the EU adopts new principles:<br>\"interoperable, not-for-profit infrastructures for publishing based on open source software and open standards\"<br><a href=\"https://data.consilium.europa.eu/doc/document/ST-8827-2023-INIT/en/pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">data.consilium.europa.eu/doc/d</span><span class=\"invisible\">ocument/ST-8827-2023-INIT/en/pdf</span></a></p><p>and now ten major research organizations support the proposal:<br><a href=\"https://www.coalition-s.org/wp-content/uploads/2023/05/JointResponse2CouncilScholCommConclusions.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">coalition-s.org/wp-content/upl</span><span class=\"invisible\">oads/2023/05/JointResponse2CouncilScholCommConclusions.pdf</span></a></p><p>What they propose is nearly identical to our proposal:<br><a href=\"https://doi.org/10.5281/zenodo.5526634\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.5281/zenodo.5526634</span><span class=\"invisible\"></span></a></p><p>Does this now get the ball rolling, or is it just words on paper?</p>"
    },
    "people": [
      {
        "url": "https://ecoevo.social/@cboettig",
        "display_name": "Carl Boettiger"
      },
      {
        "url": "https://ciberlandia.pt/@villares",
        "display_name": "Alexandre B A Villares \ud83d\udc0d \u2614"
      },
      {
        "url": "https://mastodon.social/@edzer",
        "display_name": "Edzer Pebesma"
      },
      {
        "url": "https://mastodon.social/@igor",
        "display_name": "Igor Brigadir"
      },
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://hci.social/@jbigham",
        "display_name": "Jeff Bigham"
      },
      {
        "url": "https://bbq.snoot.com/@ProgGrrl",
        "display_name": "ProgGrrl"
      },
      {
        "url": "https://datasci.social/@yy",
        "display_name": "YY Ahn"
      },
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://econtwitter.net/@ito",
        "display_name": "Tim Gushue"
      },
      {
        "url": "https://sfba.social/@morgandawn",
        "display_name": "morgandawn"
      },
      {
        "url": "https://mastodon.social/@tiffanycli",
        "display_name": "Tiffany Li"
      },
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      },
      {
        "url": "https://mas.to/@enridaga",
        "display_name": "enridaga"
      },
      {
        "url": "https://starbase80.wtf/@5ciFiGirl",
        "display_name": "Sci-Fi Girl"
      },
      {
        "url": "https://fediscience.org/@mario_angst_sci",
        "display_name": "Mario Angst"
      },
      {
        "url": "https://h4.io/@joshisanonymous",
        "display_name": "Joshua McNeill"
      }
    ]
  },
  {
    "link": "https://doi.org/10.54337/jovi.v1i1.7782",
    "title": "doi.org/10.54337/jovi.v1i1.778...",
    "latest": "2023-05-23T16:55:55+00:00",
    "last_post": {
      "url": "https://hci.social/@jovi/110418478552305809",
      "content": "<p>JoVI's first publication is now live! See <a href=\"https://doi.org/10.54337/jovi.v1i1.7782\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.54337/jovi.v1i1.778</span><span class=\"invisible\">2</span></a> for our inaugural editorial &amp; mission statement.</p><p>/cc <span class=\"h-card\"><a href=\"https://hci.social/@khornbaek\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>khornbaek</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@amelia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>amelia</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@arvind\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>arvind</span></a></span> <span class=\"h-card\"><a href=\"https://mastodon.social/@scheidegger\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>scheidegger</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@jschwabish\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>jschwabish</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@elm\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>elm</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@lane\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lane</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@lace\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lace</span></a></span> <span class=\"h-card\"><a href=\"https://hci.social/@floe\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>floe</span></a></span> <span class=\"h-card\"><a href=\"https://fediscience.org/@mjskay\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>mjskay</span></a></span> <span class=\"h-card\"><a href=\"https://mstdn.science/@lonnibesancon\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lonnibesancon</span></a></span></p>"
    },
    "people": [
      {
        "url": "https://vis.social/@elm",
        "display_name": "Niklas Elmqvist"
      },
      {
        "url": "https://vis.social/@dr_tj",
        "display_name": "Dr. T.J. Jankun-Kelly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12212v1",
    "title": "Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT",
    "latest": "2023-05-23T16:30:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110418946391923185",
      "content": "<p>\ud83d\udcdd Prompt ChatGPT in MNER: Improved Multimodal Named Entity Recognition Method Based on Auxiliary Refining Knowledge From ChatGPT \ud83d\udcda</p><p>\"Prompt ChatGPT In MNER (PGIM) utilizes ChatGPT, an implicit knowledge engine, to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12212v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12212v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12074v1",
    "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
    "latest": "2023-05-23T10:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417412880049300",
      "content": "<p>\ud83d\udcdd DisCo: Distilled Student Models Co-Training for Semi-Supervised Text Mining \ud83d\udcda</p><p>\"A novel co-training technique to optimize multiple small student models generated from a large PLM using knowledge distillation under diversified views: model and data views produced by different distillation strategies and input augmentations, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12074v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12074v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12147",
    "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4",
    "latest": "2023-05-23T10:00:05+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110417412101749441",
      "content": "<p>LogiCoT: Logical Chain-of-Thought instruction-tuning data collection with GPT-4. ~ Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang. <a href=\"https://arxiv.org/abs/2305.12147\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12147</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/GPT4\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT4</span></a> <a href=\"https://mathstodon.xyz/tags/Logic\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Logic</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11252v1",
    "title": "Brain-inspired learning in artificial neural networks: a review",
    "latest": "2023-05-23T09:47:20+00:00",
    "last_post": {
      "url": "https://mastodon.social/@achterbrain/110417362350083227",
      "content": "<p>New <a href=\"https://mastodon.social/tags/review\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>review</span></a> led by Samuel Schmidgall:<br>'Brain-inspired learning in artificial neural networks'<br><a href=\"https://arxiv.org/abs/2305.11252v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11252v1</span><span class=\"invisible\"></span></a></p><p>This was a great <a href=\"https://mastodon.social/tags/collaboration\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>collaboration</span></a> supported by OpenBioML's new <a href=\"https://mastodon.social/tags/NeuroAI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuroAI</span></a> initiative! Looking forward to many more projects coming out of this <a href=\"https://mastodon.social/tags/open\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>open</span></a> collaborative <a href=\"https://mastodon.social/tags/research\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>research</span></a> community!</p><p><a href=\"https://mastodon.social/tags/neuroscience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>neuroscience</span></a> <a href=\"https://mastodon.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@achterbrain",
        "display_name": "Jascha Achterberg"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12057v1",
    "title": "https://arxiv.org/abs/2305.12057v1",
    "latest": "2023-05-23T09:40:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417334069138419",
      "content": "<p>\ud83d\udcdd Accurate Knowledge Distillation with N-Best Reranking \ud83d\udcda</p><p>\"Leverages a diverse set of models, including publicly available large pretrained models, to provide more accurate pseudo-labels for training student models via n-best reranking.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/fairseq/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/fa</span><span class=\"invisible\">irseq/</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12057v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12057v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.12073v1.pdf",
    "title": "https://arxiv.org/pdf/2305.12073v1.pdf",
    "latest": "2023-05-23T09:14:35+00:00",
    "last_post": {
      "url": "https://mastodon.social/@tiago_ribeiro/110417142228641639",
      "content": "<p>\"GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance\"</p><p>\"Our findings reinforce the exceptional performance of the GELU activation function, which attains the highest test accuracy and lowest test loss among the activation functions investigated. Other activation functions, such as Hardswish and ReLU6, exhibit commendable performance as well...\"</p><p><a href=\"https://mastodon.social/tags/GELU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GELU</span></a> <a href=\"https://mastodon.social/tags/ReLU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReLU</span></a> <a href=\"https://mastodon.social/tags/HardShrink\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>HardShrink</span></a> <a href=\"https://mastodon.social/tags/leakyReLU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>leakyReLU</span></a> <a href=\"https://mastodon.social/tags/ReLU6\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReLU6</span></a></p><p>\ud83d\udd17<a href=\"https://arxiv.org/pdf/2305.12073v1.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.12073v1.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12029v1",
    "title": "https://arxiv.org/abs/2305.12029v1",
    "latest": "2023-05-23T09:00:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417177104587367",
      "content": "<p>\ud83d\udcdd MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Introduces two models based on BERT and RoBERTa as baselines on this dataset, which can be used as a benchmark for future research to tackle this task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/huashen218/MultiTurnCleanup.git\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/huashen218/MultiTur</span><span class=\"invisible\">nCleanup.git</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12029v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12029v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12027v1",
    "title": "Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings",
    "latest": "2023-05-23T08:30:12+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417059049321121",
      "content": "<p>\ud83d\udcdd Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings \ud83d\udcda\ud83d\udc7e</p><p>\"DUCK infuses prior knowledge on entity types in entity representations by learning to place entities of similar type close to each other in the embedding space, using boxes and hyperspheres.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12027v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12027v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12018v1",
    "title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases",
    "latest": "2023-05-23T08:00:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416940920589699",
      "content": "<p>\ud83d\udcdd BOLT: Fast Energy-Based Controlled Text Generation with Tunable Biases \ud83d\udcda</p><p>\"Proposes BOLT which relies on tunable biases to adjust output logits directly to achieve fast convergence in controlled generation while maintaining the generator's autoregressive nature to assert a strong control on token-wise conditional dependencies and fluent output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12018v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12018v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12001v1",
    "title": "https://arxiv.org/abs/2305.12001v1",
    "latest": "2023-05-23T07:40:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416862157122454",
      "content": "<p>\ud83d\udcdd OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models \ud83d\udcda</p><p>\"Entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/metaseq\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/me</span><span class=\"invisible\">taseq</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12001v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12001v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12196",
    "title": "Experimental results from applying GPT-4 to an unpublished formal language",
    "latest": "2023-05-23T07:27:50+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110416776676268021",
      "content": "<p>Experimental results from applying GPT-4 to an unpublished formal language. ~ Gregor vom Scheidt (@GregorVScheidt). <a href=\"https://arxiv.org/abs/2305.12196\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12196</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/ChatGPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ChatGPT</span></a> <a href=\"https://mathstodon.xyz/tags/GPT4\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT4</span></a> <a href=\"https://mathstodon.xyz/tags/FunctionalProgramming\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>FunctionalProgramming</span></a> <a href=\"https://mathstodon.xyz/tags/Logic\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Logic</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11326",
    "title": "Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data",
    "latest": "2023-05-23T07:27:07+00:00",
    "last_post": {
      "url": "https://fediscience.org/@JordiCabot/110416810949851518",
      "content": "<p>Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data</p><p>&lt;- From a CSV to a fully functional chatbot </p><p>Paper: <a href=\"https://arxiv.org/abs/2305.11326\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11326</span><span class=\"invisible\"></span></a><br>Summary: <a href=\"https://livablesoftware.com/chatbots-open-data-project/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">livablesoftware.com/chatbots-o</span><span class=\"invisible\">pen-data-project/</span></a></p><p><a href=\"https://fediscience.org/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://fediscience.org/tags/csv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>csv</span></a> <a href=\"https://fediscience.org/tags/OpenData\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenData</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@JordiCabot",
        "display_name": "Jordi Cabot"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12000v1",
    "title": "Deep Learning Approaches to Lexical Simplification: A Survey",
    "latest": "2023-05-23T07:10:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416744250292836",
      "content": "<p>\ud83d\udcdd Deep Learning Approaches to Lexical Simplification: A Survey \ud83d\udcda</p><p>\"LS is the lexical component of Text Simplification (TS) with the aim of making texts more accessible to various target populations such as children, people with language and learning disorders, foreigners or people with low literacy.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12000v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12000v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2109.06096",
    "title": "https://arxiv.org/abs/2109.06096",
    "latest": "2023-05-23T06:38:47+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416620928970772",
      "content": "<p>As always a small model behaves just like an undertrained model<br><a href=\"https://twitter.com/LChoshen/status/1506245430912430091\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/LChoshen/status/15</span><span class=\"invisible\">06245430912430091</span></a><br><a href=\"https://arxiv.org/abs/2109.06096\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2109.06096</span><span class=\"invisible\"></span></a><br>So you can also pick an undertrained model, which would be better than a trained model in detecting who generated the text.</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2301.11305",
    "title": "https://arxiv.org/abs/2301.11305",
    "latest": "2023-05-23T06:33:49+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416601360081415",
      "content": "<p>First the problem, given a text you want to know whether a human wrote it. You've been in NLP lately I am sure a teacher, sister, nephew etc. called and told you they suspect someone handed them a GPT text.<br>Problem: how can you tell<br>The approach<br>Randomly replace words<br>Then see how much it changed the sentence probability\\likelihood</p><p>presented by <br><a href=\"https://arxiv.org/abs/2301.11305\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2301.11305</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09859",
    "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
    "latest": "2023-05-23T06:33:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416598590927915",
      "content": "<p>Opposite scaling law: detection of machine-generated text is done better by smaller models</p><p>Everyone (outside <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a>...) is afraid GPT would cheat for them, which pushes for detection methods</p><p><a href=\"https://arxiv.org/abs/2305.09859\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09859</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <a href=\"https://sigmoid.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11993v1",
    "title": "https://arxiv.org/abs/2305.11993v1",
    "latest": "2023-05-23T06:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416586994729304",
      "content": "<p>\ud83d\udcdd Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis \ud83d\udcda</p><p>\"A specialised Flan-T5 language model is trained to take usage examples and a usage cluster as input and output a natural language definition of that usage cluster as its output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ltgoslo/definition_modeling\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/ltgoslo/definition_</span><span class=\"invisible\">modeling</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11993v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11993v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11979v1",
    "title": "https://arxiv.org/abs/2305.11979v1",
    "latest": "2023-05-23T05:30:12+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416351236599275",
      "content": "<p>\ud83d\udcdd A Weak Supervision Approach for Few-Shot Aspect Based Sentiment \ud83d\udcda</p><p>\"Uses weak supervision on unlabeled data using the output of a simple baseline, and then fine-tune a pre-trained sequence-to-sequence model on the generated dataset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/robertvacareanu/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/robertvacareanu/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11979v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11979v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.10559",
    "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
    "latest": "2023-05-23T05:19:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416307592130037",
      "content": "<p>We also have seen claims ICL is quite similar to taking a gradient step<br>arxiv.org/abs/2211.15661<br><a href=\"https://arxiv.org/abs/2212.10559\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2212.10559</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2202.12837",
    "title": "http://arxiv.org/abs/2202.12837",
    "latest": "2023-05-23T05:18:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416304404784912",
      "content": "<p>So we have seen papers showing that models gain a lot from seeing examples (ICL) with random labels<br><a href=\"http://arxiv.org/abs/2202.12837\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2202.12837</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09731",
    "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
    "latest": "2023-05-23T05:17:36+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416301704135043",
      "content": "<p>In-Context-Learning == gradient descent or disregards labels completely?!<br>Why not both?</p><p>Models recognize the task but also learn it<br>&amp; The benefits of actual learning grow with # examples and model size</p><p> <br><a href=\"https://arxiv.org/abs/2305.09731\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09731</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11952v1",
    "title": "Self-QA: Unsupervised Knowledge Guided Language Model Alignment",
    "latest": "2023-05-23T05:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416233204460998",
      "content": "<p>\ud83d\udcdd Self-Qa: Unsupervised Knowledge Guided Language Model Alignment \ud83d\udcda</p><p>\"Self-QA replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge, enabling the model to generate a larger quantity of correct and domain-specific instruction data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11952v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11952v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11938v1",
    "title": "https://arxiv.org/abs/2305.11938v1",
    "latest": "2023-05-23T04:40:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416154477268328",
      "content": "<p>\ud83d\udcdd XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages \ud83d\udcda</p><p>\"XTREME-UP is a benchmark defined by three criteria: its focus on the scarce-data scenario rather than zero-shot (rather than few-shot); its focus on user-centric tasks rather than linguistic probing tasks; and its focus on under-represented languages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/google-research/xtreme-up\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/google-research/xtr</span><span class=\"invisible\">eme-up</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11938v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11938v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.02819",
    "title": "GPT detectors are biased against non-native English writers",
    "latest": "2023-05-23T04:32:53+00:00",
    "last_post": {
      "url": "https://river.group.lt/@rgb/110408082519926313",
      "content": "<p><a href=\"https://arxiv.org/abs/2304.02819\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.02819</span><span class=\"invisible\"></span></a><br>GPT detectors are biased against non-native English writers</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11916v1",
    "title": "F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks",
    "latest": "2023-05-23T04:20:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416075706791889",
      "content": "<p>\ud83d\udcdd F-Pabee: Flexible-Patience-Based Early Exiting for Single-Label and Multi-Label Text Classification Tasks \ud83d\udcda</p><p>\"A Flexible-Patience-Based Early Exiting Method (F-PABEE) has been proposed to alleviate the problems mentioned above for single-label classification (SLC) and multi-label classification (MLC) tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11916v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11916v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11862v1",
    "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models",
    "latest": "2023-05-23T03:42:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415926410383122",
      "content": "<p>\ud83d\udcdd Reducing Sequence Length by Predicting Edit Operations with Large Language Models \ud83d\udcda</p><p>\"A local sequence transduction task is represented as a sequence of edit operations on the input text, which is learned by instruction tuning for large language models (LLMs).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11862v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11862v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11826v1",
    "title": "STOAT: Structured Data to Analytical Text With Controls",
    "latest": "2023-05-23T03:22:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415847795639417",
      "content": "<p>\ud83d\udcdd STOAT: Structured Data to Analytical Text with Controls \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a reasoning category aware table-to-text generation model with vector quantization to infuse the reasoning category in the output descriptions and also to control the reasoning category generation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11826v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11826v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11626v1",
    "title": "CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search",
    "latest": "2023-05-23T02:12:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415572481058584",
      "content": "<p>\ud83d\udcdd CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search \ud83d\udcda</p><p>\"A combination of GraphCodeBERT pre-training and cross-consistency training of a language model across different programming languages using XCD dataset as supervision signal.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11626v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11626v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11625v1",
    "title": "Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets",
    "latest": "2023-05-23T02:02:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415533142570361",
      "content": "<p>\ud83d\udcdd Searching by Code: A New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets \ud83d\udcda</p><p>\"SnippeR uses a single encoder for both the code snippet and the natural language query, and learns to rank the candidate answer by the cosine similarity of the two.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11625v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11625v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11744v1",
    "title": "Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval",
    "latest": "2023-05-23T01:52:58+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_ir/110415139983596635",
      "content": "<p>\ud83d\udcdd Inference-Time Re-Ranker Relevance Feedback for Neural Information Retrieval \ud83d\udcbf\ud83d\udcda</p><p>\"We update the retriever's query vector using a lightweight inference-time distillation of the re-ranker's prediction for that instance and perform a second retrieval with the updated query vector.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/IR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>IR</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11744v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11744v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://recsys.social/@karlhigley",
        "display_name": "Karl Higley"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11596v1",
    "title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation",
    "latest": "2023-05-23T01:32:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415415119788630",
      "content": "<p>\ud83d\udcdd Mitigating Backdoor Poisoning Attacks Through the Lens of Spurious Correlation \ud83d\udcda</p><p>\"Works by filtering out instances with potentially problematic correlations between features and labels, and using a subset of the rest to retrain the models and remove backdoor behaviours.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CR</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11596v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11596v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11553v1",
    "title": "https://arxiv.org/abs/2305.11553v1",
    "latest": "2023-05-23T01:12:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415336656025295",
      "content": "<p>\ud83d\udcdd Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information \ud83d\udcda</p><p>\"Considers each abstract as a recurrent cycle of sentences and place segmentation boundaries by greedily optimizing the NMI score between premises and conclusions in each cycle of sentences.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mediacloud/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mediacloud/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11553v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11553v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13304",
    "title": "https://arxiv.org/abs/2305.13304",
    "latest": "2023-05-23T01:12:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415336394152761",
      "content": "<p>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text</p><p>Generates a paragraph at each timestep and updates its language-based long-short term memory stored on the hard drive and the prompt, respectively. </p><p>repo: <a href=\"https://github.com/aiwaves-cn/RecurrentGPT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/aiwaves-cn/Recurren</span><span class=\"invisible\">tGPT</span></a> <br>abs: <a href=\"https://arxiv.org/abs/2305.13304\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13304</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13292",
    "title": "VideoLLM: Modeling Video Sequence with Large Language Models",
    "latest": "2023-05-23T01:03:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415301110279075",
      "content": "<p>VideoLLM: Modeling Video Sequence with Large Language Models</p><p>Leverages the sequence reasoning capabilities of pre-trained LLMs from NLP for video sequence understanding.</p><p><a href=\"https://arxiv.org/abs/2305.13292\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13292</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13301",
    "title": "Training Diffusion Models with Reinforcement Learning",
    "latest": "2023-05-23T01:00:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415289384934263",
      "content": "<p>Training Diffusion Models with Reinforcement Learning</p><p>Presents an RL-based framework for training denoising diffusion models to directly optimize a variety of reward functions</p><p><a href=\"https://arxiv.org/abs/2305.13301\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13301</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2303.12712.pdf",
    "title": "https://arxiv.org/pdf/2303.12712.pdf",
    "latest": "2023-05-23T00:42:46+00:00",
    "last_post": {
      "url": "https://techhub.social/@freemanwd/110415146468249466",
      "content": "<p>Microsoft researchers wonder if GPT4 is a form of AGI based on several experiments <br><a href=\"https://arxiv.org/pdf/2303.12712.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2303.12712.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@andrey",
        "display_name": "Andrey Kurenkov"
      },
      {
        "url": "https://fosstodon.org/@amueller",
        "display_name": "Andreas Mueller"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://genart.social/@tca",
        "display_name": "tiago"
      },
      {
        "url": "https://mastodon.social/@bruces",
        "display_name": "Bruce Sterling @bruces"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11449v1",
    "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast",
    "latest": "2023-05-23T00:32:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415179239334587",
      "content": "<p>\ud83d\udcdd Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-Tuning Slow and Fast \ud83d\udcda</p><p>\"Proposes a method named Fine-tuning slow and fast with four training policies to improve the performance gap for multilingual tasks, by reducing forgetting and improving the fine-tuning process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11449v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11449v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11251v1",
    "title": "Computational thematics: Comparing algorithms for clustering the genres of literary fiction",
    "latest": "2023-05-23T00:12:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415100611809059",
      "content": "<p>\ud83d\udcdd Computational Thematics: Comparing Algorithms for Clustering the Genres of Literary Fiction \ud83d\udcda</p><p>\"S are applied to a corpus of books belonging to four pre-tagged genres of fiction (Fantasy, Horror, Science Fiction, and Romance), and are then validated against the \"ground truth\" genre labels.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11251v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11251v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11243v1",
    "title": "Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses",
    "latest": "2023-05-23T00:02:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415061229184234",
      "content": "<p>\ud83d\udcdd Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses \ud83d\udcda\ud83d\udc7e</p><p>\"LaMDA is a large language model (LM), a type of AModels trained to predict the next word given a sequence of words, that was trained on a large number of webpages and books.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11243v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11243v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11231v1",
    "title": "Recent Trends in Unsupervised Summarization",
    "latest": "2023-05-22T23:22:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414903863283183",
      "content": "<p>\ud83d\udcdd Recent Trends in Unsupervised Summarization \ud83d\udcda</p><p>\"Unsupervised approaches learn to summarize from the input data and do not need labeled datasets for training, unlike supervised approaches that require labeled datasets for training the models to summarize the content.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11231v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11231v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2302.12173",
    "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
    "latest": "2023-05-22T23:05:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110414836730125554",
      "content": "<p>Compromising LLM-Integrated Applications with Indirect Prompt Injection - <a href=\"https://arxiv.org/abs/2302.12173\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2302.12173</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@raphaelmilliere",
        "display_name": "Rapha\u00ebl Milli\u00e8re"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206v1",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T23:02:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414825637860258",
      "content": "<p>\ud83d\udcdd LIMA: Less Is More for Alignment \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"LIMA is a 65B parameter language model trained for 150B tokens, using a supervised loss, from 1,000 carefully curated prompts and their responses.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11206v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11186v1",
    "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
    "latest": "2023-05-22T22:52:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414785973568218",
      "content": "<p>\ud83d\udcdd Compress, Then Prompt: Improving Accuracy-Efficiency Trade-Off of LLM Inference with Transferable Prompt \ud83d\udcda\ud83e\udde0</p><p>\"A prompt learning paradigm that cultivates an additive prompt over a compressed LLM to bolster their accuracy is presented and empirically tested in this work, shedding light on new possibilities for enhancing the balance between accuracy and efficiency in LLM inference.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11186v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11186v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11719v1",
    "title": "https://arxiv.org/abs/2305.11719v1",
    "latest": "2023-05-22T21:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414463541864356",
      "content": "<p>\ud83d\udcdd Information Screening Whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling \ud83d\udd2d\ud83d\udcda</p><p>\"Represents the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ChocoWu/MRE-ISE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/ChocoWu/MRE-ISE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11719v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11719v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/74djc/",
    "title": "http://osf.io/74djc/",
    "latest": "2023-05-22T21:04:05+00:00",
    "last_post": {
      "url": "https://botsin.space/@EdArXivBot/110395685386849135",
      "content": "<p>Unlocking Financial Success: Empowering Higher Ed Students and Developing Financial Literacy Interventions at Scale <a href=\"http://osf.io/74djc/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/74djc/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@dougholton",
        "display_name": "Doug Holton"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11845v1",
    "title": "https://arxiv.org/abs/2305.11845v1",
    "latest": "2023-05-22T20:18:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414180509844595",
      "content": "<p>\ud83d\udcdd RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"RxnScribe is an end-to-end model that takes as input a chemical reaction diagram and outputs a structured representation in a machine-readable format.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/thomas0809/RxnScribe\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/thomas0809/RxnScrib</span><span class=\"invisible\">e</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11845v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11845v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11806v1",
    "title": "https://arxiv.org/abs/2305.11806v1",
    "latest": "2023-05-22T19:54:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414086009856181",
      "content": "<p>\ud83d\udcdd The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics \ud83d\udcda</p><p>\"Develops and compare several explanation methods and demonstrate their effectiveness for interpreting state-of-the-art neural metrics based on BERT and ROBERTA fine-tuned on sentence-level human judgments from the WMT Metrics Shared Task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Unbabel/COMET\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Unbabel/COMET</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11806v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11806v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11790v1",
    "title": "Prompting with Pseudo-Code Instructions",
    "latest": "2023-05-22T19:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413944317609304",
      "content": "<p>\ud83d\udcdd Prompting with Pseudo-Code Instructions \ud83d\udcda</p><p>\"A pre-trained language model is fine-tuned using a prompt in the form of pseudocode instructions for a task in addition to the natural language instruction for the task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11790v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11790v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11789v1",
    "title": "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach",
    "latest": "2023-05-22T19:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413897105704111",
      "content": "<p>\ud83d\udcdd Solving NLP Problems Through Human-System Collaboration: A Discussion-Based Approach \ud83d\udcda</p><p>\"Creates a dataset with a human-in-the-loop setup where an annotator can discuss a natural language inference task with a chatbot and receive feedback.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11789v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11789v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41593-023-01333-4",
    "title": "Addressing the ethical and societal challenges posed by genome-wide association studies of behavioral and brain-related traits - Nature Neuroscience",
    "latest": "2023-05-22T18:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413833974408684",
      "content": "<p>Ethical, societal implications of genome-wide behavioral, brain-related traits - <a href=\"https://www.nature.com/articles/s41593-023-01333-4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41593-023</span><span class=\"invisible\">-01333-4</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778v1",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T18:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413802737748919",
      "content": "<p>\ud83d\udcdd Cross-Lingual Supervision Improves Large Language Models Pre-Training \ud83d\udcda\ud83e\udde0</p><p>\"A pre-training objective mixing a self-supervised language modeling and a supervised machine translation objectives, including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11778v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11761v1",
    "title": "ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation",
    "latest": "2023-05-22T18:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413755777159926",
      "content": "<p>\ud83d\udcdd ReSeTOX: Re-Learning Attention Weights for Toxicity Mitigation in Machine Translation \ud83d\udcda</p><p>\"Works by dynamically adjusting the key-value self-attention weights and re-evaluates the beam search hypotheses in case of identified added toxicity during the inference process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11761v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11761v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/u26ze/",
    "title": "http://osf.io/u26ze/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645261557408",
      "content": "<p>Gender and retention patterns among U.S. faculty <a href=\"http://osf.io/u26ze/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u26ze/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/u5kd2/",
    "title": "http://osf.io/u5kd2/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645213145496",
      "content": "<p>Power users: Canadian sex workers' use of technology post COVID <a href=\"http://osf.io/u5kd2/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u5kd2/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/7s9g4/",
    "title": "http://osf.io/7s9g4/",
    "latest": "2023-05-22T18:02:01+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645169835083",
      "content": "<p>Intelligent transport design with a dual focus <a href=\"http://osf.io/7s9g4/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/7s9g4/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  }
]