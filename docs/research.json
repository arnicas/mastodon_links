[
  {
    "link": "https://arxiv.org/abs/2305.13669v1",
    "title": "Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment",
    "latest": "2023-05-24T21:57:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425895873376639",
      "content": "<p>\ud83d\udcdd Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment \ud83d\udcda\ud83d\udc7e</p><p>\"MixAlign is a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13669v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13669v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13668v1",
    "title": "Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations",
    "latest": "2023-05-24T21:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425817274498814",
      "content": "<p>\ud83d\udcdd Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations \ud83d\udcda\ud83e\udde0</p><p>\"Learns a similarity metric for each object, such that when the similarity between object A and a given verb is higher than the similarity between object B and that verb, the agent will more likely choose object A than object B in a given context.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13668v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13668v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13667v1",
    "title": "https://arxiv.org/abs/2305.13667v1",
    "latest": "2023-05-24T21:27:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425777781700447",
      "content": "<p>\ud83d\udcdd Optimizing Non-Autoregressive Transformers with Contrastive Learning \ud83d\udcda</p><p>\"Proposed to ease the difficulty of modality learning via sampling from the model distribution instead of the data distribution, and derive contrastive constraints to stabilize the training process and integrate this resulting objective with the state-of-the-art NAT architecture DA-Transformer.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/thu-coai/DA-Transformer\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/thu-coai/DA-Transfo</span><span class=\"invisible\">rmer</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13667v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13667v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14296",
    "title": "USB: A Unified Summarization Benchmark Across Tasks and Domains",
    "latest": "2023-05-24T21:18:38+00:00",
    "last_post": {
      "url": "https://hci.social/@jbigham/110425742959387131",
      "content": "<p>My students and collaborators just released a new summarization benchmark that forefronts factuality and evidence extraction -- </p><p>\u26a1Introducing USB: A Unified Summarization Benchmark Across Tasks and Domains!</p><p><a href=\"https://arxiv.org/abs/2305.14296\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14296</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@jbigham",
        "display_name": "Jeff Bigham"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13658v1",
    "title": "Understanding compositional data augmentation in automatic morphological inflection",
    "latest": "2023-05-24T21:07:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425699476851475",
      "content": "<p>\ud83d\udcdd Understanding Compositional Data Augmentation in Automatic Morphological Inflection \ud83d\udcda</p><p>\"A data augmentation method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples for automatic morphological inflection generation tasks such as lemmatization and morphological analysis.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13658v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13658v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.09848?s=09",
    "title": "Evaluating Verifiability in Generative Search Engines",
    "latest": "2023-05-24T20:48:37+00:00",
    "last_post": {
      "url": "https://mastodon.cloud/@aarontay/110276375288247354",
      "content": "<p>[Read] Evaluating Verifiability in Generative Search Engines - finally a paper that assesses how reliable Search+Large language models (LLM) type systems like Bing Chat, perplexity.ai, you chat, neevaai <a href=\"https://arxiv.org/abs/2304.09848?s=09\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.09848?s=09</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13657v1",
    "title": "ChatGPT as your Personal Data Scientist",
    "latest": "2023-05-24T20:47:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425620529901551",
      "content": "<p>\ud83d\udcdd ChatGPT as Your Personal Data Scientist \ud83d\udcda</p><p>\"ChatGPT acts as an intelligent agent that assists users in their data science tasks through intuitive, natural conversations and without requiring in-depth knowledge of the underlying ML processes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13657v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13657v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14327",
    "title": "https://arxiv.org/abs/2305.14327",
    "latest": "2023-05-24T20:37:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110425579685882242",
      "content": "<p>RT @Wade_Yin9712@twitter.com</p><p>Introducing \ud83e\udd96Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation! <br>Dynosaur contains 800K generated instruction-tuning data and can \u2b06\ufe0f**dynamically** grow!</p><p>Project page: <a href=\"https://dynosaur-it.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">dynosaur-it.github.io/</span><span class=\"invisible\"></span></a><br>Preprint: <a href=\"https://arxiv.org/abs/2305.14327\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14327</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/WadeYin9712/Dynosaur\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/WadeYin9712/Dynosau</span><span class=\"invisible\">r</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Wade_Yin9712/status/1661237791920132098\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Wade_Yin9712/statu</span><span class=\"invisible\">s/1661237791920132098</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1093/scipol/scac081",
    "title": "https://doi.org/10.1093/scipol/scac081",
    "latest": "2023-05-24T19:54:47+00:00",
    "last_post": {
      "url": "https://hcommons.social/@jcpeyssard/110425413242180989",
      "content": "<p>Mikael Laakso , Anna-Maija Multas, European scholarly journals from small- and mid-size publishers: mapping journals and public funding mechanisms, Science and Public Policy, 2023 <a href=\"https://doi.org/10.1093/scipol/scac081\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.1093/scipol/scac081</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@RonaldSnijder",
        "display_name": "Ronald Snijder"
      },
      {
        "url": "https://hcommons.social/@jcpeyssard",
        "display_name": "Jean-Christophe Peyssard"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13654v1",
    "title": "Understanding and Mitigating Spurious Correlations in Text Classification",
    "latest": "2023-05-24T19:47:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425384605481370",
      "content": "<p>\ud83d\udcdd Understanding and Mitigating Spurious Correlations in Text Classification \ud83d\udcda</p><p>\"Prevents models from learning certain spurious features by introducing a regularization loss during fine-tuning process of pre-trained language models (PLM), so that the model will not rely on those spurious features and generalize on out-of-distribution data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13654v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13654v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13048",
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "latest": "2023-05-24T19:38:07+00:00",
    "last_post": {
      "url": "https://arvr.social/@mpesce/110417099505946407",
      "content": "<p>During my keynote this morning I noted that in the three weeks I'd been keynoting <a href=\"https://arvr.social/tags/pwctheoutside\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>pwctheoutside</span></a> the field of artificial intelligence had advanced at least three years.</p><p>Here's today's massive research breakthrough. It might be the AI breakthrough of the year -- in any other year.</p><p><a href=\"https://arxiv.org/abs/2305.13048\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13048</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@davidmortensen",
        "display_name": "David Mortensen"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13648v1",
    "title": "Non-parametric, Nearest-neighbor-assisted Fine-tuning for Neural Machine Translation",
    "latest": "2023-05-24T19:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425345363936146",
      "content": "<p>\ud83d\udcdd Non-Parametric, Nearest-Neighbor-Assisted Fine-Tuning for Neural Machine Translation \ud83d\udcda\ud83d\udc7e</p><p>\"Uses a non-parametric kNN method to improve machine translation by incorporating statistics into the gradient update of a fine-tuned baseline NMT model with a gating mechanism, the ground truth probability from the kNN model, and a reinforcement learning method.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13648v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13648v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14337",
    "title": "Anchor Prediction: Automatic Refinement of Internet Links",
    "latest": "2023-05-24T19:19:34+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110425274766638680",
      "content": "<p>RT @nelsonfliu@twitter.com</p><p>Most links on the Internet are unanchored\u2014they simply point to a target webpage as a whole.</p><p>We define and explore the task of *anchor prediction*: given a link in its source context, can we directly point readers to relevant parts of the target webpage?</p><p><a href=\"https://arxiv.org/abs/2305.14337\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14337</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/nelsonfliu/status/1661265876912582658\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/nelsonfliu/status/</span><span class=\"invisible\">1661265876912582658</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13645v1",
    "title": "mPMR: A Multilingual Pre-trained Machine Reader at Scale",
    "latest": "2023-05-24T19:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425266881405590",
      "content": "<p>\ud83d\udcdd mPMR: A Multilingual Pre-Trained Machine Reader at Scale \ud83d\udcda</p><p>\"MPMR pre-trains mPLMs with multilingual machine reading comprehension data and fine-tunes mPLMs with downstream tasks to perform sequence classification and span extraction, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13645v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13645v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.14342",
    "title": "http://arxiv.org/abs/2305.14342",
    "latest": "2023-05-24T19:15:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110425258953264039",
      "content": "<p>RT @tengyuma@twitter.com</p><p>Adam, a 9-yr old optimizer, is the go-to for training LLMs (eg, GPT-3, OPT, LLAMA).</p><p>Introducing Sophia, a new optimizer that is 2x faster than Adam on LLMs. Just a few more lines of code could cut your costs from $2M to $1M (if scaling laws hold).</p><p><a href=\"http://arxiv.org/abs/2305.14342\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.14342</span><span class=\"invisible\"></span></a> \ud83e\uddf5\u2b07\ufe0f</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/tengyuma/status/1661412995430219786\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/tengyuma/status/16</span><span class=\"invisible\">61412995430219786</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13641v1",
    "title": "https://arxiv.org/abs/2305.13641v1",
    "latest": "2023-05-24T18:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425188057946085",
      "content": "<p>\ud83d\udcdd AxomiyaBERTa: A Phonologically-Aware Transformer Model for Assamese \ud83d\udcda</p><p>\"The training procedure of AxomiyaBERTa is different from standard BERT models: it only uses the masked language modeling training objective, while standard models use both masked language modeling as well as the next-sentence prediction objective.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/csu-signal/axomiyaberta\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/csu-signal/axomiyab</span><span class=\"invisible\">erta</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13641v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13641v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.32388/ZNWI7T",
    "title": "https://doi.org/10.32388/ZNWI7T",
    "latest": "2023-05-24T18:54:35+00:00",
    "last_post": {
      "url": "https://scholar.social/@egonw/110425122353070829",
      "content": "<p>Jente Houweling and I wrote up the idea (based on discussions, what we read, etc) that resulted from her research that we should replace \"data management\" with \"research output management\". Because we have a very narrow view of what data is, which does not reflect the research process.</p><p>We wrote up a definition and are looking for your peer review! Please be Reviewer of this short output: <a href=\"https://doi.org/10.32388/ZNWI7T\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.32388/ZNWI7T</span><span class=\"invisible\"></span></a></p><p><a href=\"https://scholar.social/tags/peerWanted\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>peerWanted</span></a></p>"
    },
    "people": [
      {
        "url": "https://scholar.social/@telliott",
        "display_name": "Tom Elliott"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1057/s41304-023-00431-y",
    "title": "Alternative metrics, traditional problems? Assessing gender dynamics in the altmetrics of political science - European Political Science",
    "latest": "2023-05-24T18:49:32+00:00",
    "last_post": {
      "url": "https://fediscience.org/@petersuber/110425156663480927",
      "content": "<p>Update. In political science, \"journal articles authored exclusively by female scholars score 27% lower on average [on Altmetric Attention Scores, AAS] than exclusively male-authored outputs. However, men are also more likely to write articles with an AAS of zero. These patterns are shaped by the presence of high-scoring male 'superstars' whose research attracts much online attention.\"<br><a href=\"https://link.springer.com/article/10.1057/s41304-023-00431-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">057/s41304-023-00431-y</span></a></p><p><a href=\"https://fediscience.org/tags/Altmetrics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Altmetrics</span></a> <a href=\"https://fediscience.org/tags/Gender\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Gender</span></a> <a href=\"https://fediscience.org/tags/PoliticalScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PoliticalScience</span></a></p>"
    },
    "people": [
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      },
      {
        "url": "https://fediscience.org/@petersuber",
        "display_name": "petersuber"
      }
    ]
  },
  {
    "link": "https://journals.sagepub.com/doi/10.1177/1532673X231175625",
    "title": "journals.sagepub.com/doi/10.11...",
    "latest": "2023-05-24T18:41:38+00:00",
    "last_post": {
      "url": "https://mastodon.social/@owasow/110425125607399404",
      "content": "<p>New study finds people in Iowa \u201cliving closer to BLM protests show greater support for BLM movement and, to a lesser extent, for defunding the police. Results suggest protests may affect public opinion, but only within a very narrow range of a few miles.\u201d <a href=\"https://journals.sagepub.com/doi/10.1177/1532673X231175625\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.sagepub.com/doi/10.11</span><span class=\"invisible\">77/1532673X231175625</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@owasow",
        "display_name": "Omar Wasow"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13632v1",
    "title": "https://arxiv.org/abs/2305.13632v1",
    "latest": "2023-05-24T18:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425109452118197",
      "content": "<p>\ud83d\udcdd Detecting and Mitigating Hallucinations in Multilingual Summarisation \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"MFACT is a novel metric for the faithfulness evaluation of generated summaries on non-English language, which transfers existing English metric FACT using multi-lingual pretrained model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/yfqiu-nlp/mfact-summ\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/yfqiu-nlp/mfact-sum</span><span class=\"invisible\">m</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13632v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13632v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.acalib.2023.102734",
    "title": "doi.org/10.1016/j.acalib.2023....",
    "latest": "2023-05-24T18:28:24+00:00",
    "last_post": {
      "url": "https://fediscience.org/@petersuber/110425073539653825",
      "content": "<p>More on the <a href=\"https://fediscience.org/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> citation advantage (<a href=\"https://fediscience.org/tags/OACA\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OACA</span></a>). <br><a href=\"https://doi.org/10.1016/j.acalib.2023.102734\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.acalib.2023.</span><span class=\"invisible\">102734</span></a></p><p>\"Articles under the hybrid gold modality are cited on average twice as much as those in the gold modality, regardless of funding\u2026Funded articles generally obtain 50% more citations than unfunded ones within the same publication modality. Open access <a href=\"https://fediscience.org/tags/repositories\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>repositories</span></a> significantly increase citations, particularly for articles w/o funding\u2026Articles in OA repositories receive 50% more citations than paywalled ones.\"</p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@petersuber",
        "display_name": "petersuber"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13637v1",
    "title": "IdEALS: Idiomatic Expressions for Advancement of Language Skills",
    "latest": "2023-05-24T18:27:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425070148083340",
      "content": "<p>\ud83d\udcdd IdEALS: Idiomatic Expressions for Advancement of Language Skills \ud83d\udcda</p><p>\"We curate training and testing corpora from real-world data, then evaluate various approaches and compare their performance against human experts and existing GEC tools, including Grammarly, LanguageTool and Grammarly's Customized Grammar Assistant (GCA).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13637v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13637v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13628v1",
    "title": "Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning",
    "latest": "2023-05-24T18:07:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424991511924398",
      "content": "<p>\ud83d\udcdd Improving Self-Training for Cross-Lingual Named Entity Recognition with Contrastive and Prototype Learning \ud83d\udcda</p><p>\"Contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13628v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13628v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05363",
    "title": "https://doi.org/10.21105/joss.05363",
    "latest": "2023-05-24T18:06:15+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110424986421050772",
      "content": "<p>Just published in JOSS: 'sptotal: an R package for predicting totals and weighted sums from spatial data' <a href=\"https://doi.org/10.21105/joss.05363\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05363</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13627v1",
    "title": "https://arxiv.org/abs/2305.13627v1",
    "latest": "2023-05-24T17:57:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424952229757790",
      "content": "<p>\ud83d\udcdd Instruct-Align: Teaching Novel Languages with to LLMs Through Alignment-Based Cross-Lingual Instruction \ud83d\udcda\ud83d\udc7e</p><p>\"By learning cross-lingual alignment between unseen and previously learned languages via alignment-based cross-lingual instruction tuning and experience replay of previously learned languages via continual instruction tuning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/IndoNLP/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/IndoNLP/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13627v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13627v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05226",
    "title": "https://doi.org/10.21105/joss.05226",
    "latest": "2023-05-24T17:44:33+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110424901150947173",
      "content": "<p>Just published in JOSS: 'osiris: An R package to process climate impacts on agricultural yields for the Global Change Analysis Model' <a href=\"https://doi.org/10.21105/joss.05226\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05226</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13626v1",
    "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
    "latest": "2023-05-24T17:37:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424873715485077",
      "content": "<p>\ud83d\udcdd Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-Guided, and Non-Collaboration \ud83d\udcda</p><p>\"A Proactive Chain-of-Thought prompting scheme is proposed to trigger the goal planning capability over descriptive reasoning chains and equip LLMs with proactivity, especially, for three aspects: clarification, target-guided, and non-collaborative dialogues.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13626v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13626v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13617v1",
    "title": "https://arxiv.org/abs/2305.13617v1",
    "latest": "2023-05-24T17:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424795004115201",
      "content": "<p>\ud83d\udcdd SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"SPEECH first models complex dependency among event structured components with energy-based modeling, and then represents event classes with simple hyperspheres, where the hyperspheres can be learned with only a few annotated event instances.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/zjunlp/SPEECH\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/zjunlp/SPEECH</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13617v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13617v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13945",
    "title": "Wikipedia and open access",
    "latest": "2023-05-24T17:05:12+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110424746369594426",
      "content": "<p>RT by <span class=\"h-card\"><a href=\"https://mastodon.social/@wikiresearch\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>wikiresearch</span></a></span>: Great to see this new study that found that <a href=\"https://mastodon.social/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> articles are extensively more cited in <span class=\"h-card\"><a href=\"https://mastodon.social/@Wikipedia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Wikipedia</span></a></span> &amp; show a 15% higher likelihood of being cited in Wikipedia when compared to paywalled articles <a href=\"https://arxiv.org/abs/2305.13945\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13945</span><span class=\"invisible\"></span></a></p><p>Our collaborations are paying off! @jimmy_wales <a href=\"https://twitter.com/melhagemann/status/1661395869332062208\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/melhagemann/status</span><span class=\"invisible\">/1661395869332062208</span></a></p>"
    },
    "people": [
      {
        "url": "https://dair-community.social/@DrVeronikaCH",
        "display_name": "Veronika Cheplygina"
      },
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41586-023-06094-5",
    "title": "Walking naturally after spinal cord injury using a brain\u2013spine interface - Nature",
    "latest": "2023-05-24T17:00:04+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110424726190746469",
      "content": "<p>Walking naturally after spinal cord injury using a brain\u2013spine interface - <a href=\"https://www.nature.com/articles/s41586-023-06094-5\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41586-023</span><span class=\"invisible\">-06094-5</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13602v1",
    "title": "https://arxiv.org/abs/2305.13602v1",
    "latest": "2023-05-24T16:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424716178234241",
      "content": "<p>\ud83d\udcdd ReSee: Responding Through Seeing Fine-Grained Visual Knowledge in Open-Domain Dialogue \ud83d\udcda</p><p>\"Provides new paradigm to construct multimodal dialogue datasets from text-only dialogues, and a simple but effective dialogue model ReSee to add visual representation into vanilla dialogue models by modality concatenations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Maluuba/nlg-eval\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Maluuba/nlg-eval</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13602v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13602v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.trd.2023.103757",
    "title": "doi.org/10.1016/j.trd.2023.103...",
    "latest": "2023-05-24T16:49:53+00:00",
    "last_post": {
      "url": "https://datasci.social/@UrbanDemog/110424592523320991",
      "content": "<p>Super glad to see our study \"Estimating public transport emissions from GTFS data\" finally published on Transp. Research Part D. Amazing collaboration with Joao Bazzo &amp; P. Andrade <br>\ud83d\udcd1Paper: <a href=\"https://doi.org/10.1016/j.trd.2023.103757\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.trd.2023.103</span><span class=\"invisible\">757</span></a> <br>\ud83d\udd13ungated PDF: <a href=\"https://urbandemographics.org/publication/2023_trpd_public_transport_emissions_gtfs2emis/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">urbandemographics.org/publicat</span><span class=\"invisible\">ion/2023_trpd_public_transport_emissions_gtfs2emis/</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      },
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13589v1",
    "title": "BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases",
    "latest": "2023-05-24T16:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424637611065719",
      "content": "<p>\ud83d\udcdd BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases \ud83d\udcda</p><p>\"BiasX is a framework that enables content moderators to provide free-text explanations in support of toxicity decisions, alongside their binary (toxic or not) decisions.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13589v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13589v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41893-023-01132-6",
    "title": "Quantifying the human cost of global warming - Nature Sustainability",
    "latest": "2023-05-24T16:12:04+00:00",
    "last_post": {
      "url": "https://botsin.space/@kottke/110424537435653767",
      "content": "<p>Quantifying the human cost of global warming: because of climate change, over 600 million people currently live outside the \"human climate niche\". That could rise to more than 1/3 of the total global population by the end of the century. <a href=\"https://www.nature.com/articles/s41893-023-01132-6\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41893-023</span><span class=\"invisible\">-01132-6</span></a></p>"
    },
    "people": [
      {
        "url": "https://occult.institute/@maya",
        "display_name": "maya \ud83e\uded0"
      },
      {
        "url": "https://botsin.space/@kottke",
        "display_name": "kottke.org"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12972",
    "title": "VanillaNet: the Power of Minimalism in Deep Learning",
    "latest": "2023-05-24T16:00:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110424490192297674",
      "content": "<p>VanillaNet: The Power of Minimalism in Deep Learning - <a href=\"https://arxiv.org/abs/2305.12972\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12972</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686",
    "title": "Cross-validation: what does it estimate and how well does it do it?",
    "latest": "2023-05-24T15:59:23+00:00",
    "last_post": {
      "url": "https://sciences.social/@klauspforr/110424448086940405",
      "content": "<p>wow <a href=\"https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/full/10.10</span><span class=\"invisible\">80/01621459.2023.2197686</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13585v1",
    "title": "Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs",
    "latest": "2023-05-24T15:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424480274780649",
      "content": "<p>\ud83d\udcdd Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs \ud83d\udcda</p><p>\"Proposes a structure-modeled textual encoding framework for inductive logical reasoning over KGs to find answers for complex logical queries with the structure-modeled instructions and the pre-trained encoder.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13585v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13585v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13583v1",
    "title": "Cross-Attention is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition",
    "latest": "2023-05-24T15:47:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424441061550977",
      "content": "<p>\ud83d\udcdd Cross-Attention Is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition \ud83d\udcda</p><p>\"Proposes a lightweight model via Hierarchical Crossmodal Transformer with Modality Gating (HCT-MG), which determines a primary modality according to its contribution to the target task and then hierarchically incorporates auxiliary modalities to alleviate inter-modal incongruity and reduce information redundancy.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13583v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13583v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13582v1",
    "title": "Better Low-Resource Entity Recognition Through Translation and Annotation Fusion",
    "latest": "2023-05-24T15:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424401688948635",
      "content": "<p>\ud83d\udcdd Better Low-Resource Entity Recognition Through Translation and Annotation Fusion \ud83d\udcda</p><p>\"TransFusion is a framework which translates text into a high-resource language for annotation using fully supervised models, and then fuses those annotations back into the low-resource language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13582v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13582v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.17709",
    "title": "Teru Teru B\u014dzu: Defensive Raincloud Plots",
    "latest": "2023-05-24T15:19:24+00:00",
    "last_post": {
      "url": "https://vis.social/@EuroVis/110423556294707191",
      "content": "<p>\ud83d\udcdc&nbsp; Raincloud plots show distributions in multiple ways. But are they more than the sum of their parts?<br>\u270d\ufe0f <span class=\"h-card\"><a href=\"https://jorts.horse/@Birdbassador\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Birdbassador</span></a></span><br>\ud83d\udc49 <a href=\"https://arxiv.org/abs/2303.17709\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.17709</span><span class=\"invisible\"></span></a><br><a href=\"https://vis.social/tags/Fullpaper\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Fullpaper</span></a> <a href=\"https://vis.social/tags/EuroVis\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>EuroVis</span></a> <a href=\"https://vis.social/tags/Eurovis2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Eurovis2023</span></a></p>"
    },
    "people": [
      {
        "url": "https://vis.social/@lane",
        "display_name": "Lane Harrison"
      },
      {
        "url": "https://vis.social/@mcnutt",
        "display_name": "Andrew McNutt"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13571v1",
    "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
    "latest": "2023-05-24T15:17:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424323106007438",
      "content": "<p>\ud83d\udcdd Latent Positional Information Is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings \ud83d\udcda</p><p>\"Transformer language models encode strong positional information without positional embeddings through shrinkage of the self-attention variance in each layer, which is not affected by gradient updates during pretraining.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13571v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13571v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14334",
    "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence",
    "latest": "2023-05-24T09:58:54+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110423070110153900",
      "content": "<p>Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence</p><p>Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski  Trevor Darrell</p><p>tl;dr: diffusion features are good descriptors for semantic corrs, if aggregated among timesteps.</p><p><a href=\"https://arxiv.org/abs/2305.14334\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14334</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41467-023-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D",
    "title": "Spatially-optimized urban greening for reduction of population exposure to land surface temperature extremes | Nature Communications",
    "latest": "2023-05-24T09:55:34+00:00",
    "last_post": {
      "url": "https://datasci.social/@mszll/110423057006850663",
      "content": "<p>Spatially-optimized urban greening for reduction of population exposure to land surface temperature extremes</p><p><a href=\"https://www.nature.com/articles/s41467-023-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41467-023</span><span class=\"invisible\">-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D</span></a></p>"
    },
    "people": [
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13705",
    "title": "DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models",
    "latest": "2023-05-24T09:52:59+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110423046869119371",
      "content": "<p>DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models</p><p>Lijun Li, Li'an Zhuo, Bang Zhang, Liefeng Bo, Chen Chen</p><p>tl;dr: diffusion models can do mesh reconstruction.<br><a href=\"https://arxiv.org/abs/2305.13705\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13705</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13455v1",
    "title": "https://arxiv.org/abs/2305.13455v1",
    "latest": "2023-05-24T09:37:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422986361169984",
      "content": "<p>\ud83d\udcdd Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents \ud83d\udcda</p><p>\"Presents a general framework for implementing and evaluating games with LLMs, including five concrete games as an exemplary set of settings to probe a range of capabilities (e.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clp-research/clembench\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/clp-research/clembe</span><span class=\"invisible\">nch</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13455v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13455v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13417v1",
    "title": "https://arxiv.org/abs/2305.13417v1",
    "latest": "2023-05-24T09:27:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422947048051474",
      "content": "<p>\ud83d\udcdd Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT \ud83d\udcda</p><p>\"Our visualization tool is created on top of the attention mechanism to visualize the flow of information inside the model by analyzing the tokens projected from the attention heads and memory values.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/shacharKZ/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/shacharKZ/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13417v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13417v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13413v1",
    "title": "Syntactic Knowledge via Graph Attention with BERT in Machine Translation",
    "latest": "2023-05-24T08:57:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422828823261253",
      "content": "<p>\ud83d\udcdd Syntactic Knowledge via Graph Attention with BERT in Machine Translation \ud83d\udcda</p><p>\"GAT and BERT jointly represent syntactic dependency feature as explicit knowledge of the source language to enrich source language representations and guide target language generation, which can improve translation quality across machine translation tasks without sacrificing BLEU scores.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13413v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13413v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1080/03080188.2022.2150807",
    "title": "doi.org/10.1080/03080188.2022....",
    "latest": "2023-05-24T08:42:33+00:00",
    "last_post": {
      "url": "https://mastodon.social/@lakens/110422769862919296",
      "content": "<p>Lovely paper on facts and objectivity by Philippe Stamenkovic <a href=\"https://doi.org/10.1080/03080188.2022.2150807\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1080/03080188.2022.</span><span class=\"invisible\">2150807</span></a>. Extremely accessible introduction to this topic. Some quotes: </p><p>\u201cObjectivity is the source of the authority which science enjoys in society, and a precondition of public trust in science: it is one of the main reasons (alongside truth) why we value science.\u201d</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      },
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13412v1",
    "title": "https://arxiv.org/abs/2305.13412v1",
    "latest": "2023-05-24T08:37:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422750300473291",
      "content": "<p>\ud83d\udcdd Element-Aware Summarization with Large Language Models: Expert-Aligned Evaluation and Chain-of-Thought Method \ud83d\udcda</p><p>\"Proposes a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with human writing mindset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Alsace08/SumCoT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Alsace08/SumCoT</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13412v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13412v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/00031224231168074",
    "title": "doi.org/10.1177/00031224231168...",
    "latest": "2023-05-24T08:22:15+00:00",
    "last_post": {
      "url": "https://fediscience.org/@MarkRubin/110417275690043774",
      "content": "<p>New study finds \u201carticles that develop and present a new method tend to be more disruptive.\u201d <a href=\"https://doi.org/10.1177/00031224231168074\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/00031224231168</span><span class=\"invisible\">074</span></a></p><p>Reminds me of Greenwald (2012): \u201cAn analysis of the recent history of Nobel Prizes in science unexpectedly revealed that these awards were given much more often for creation of methods and for method-based discoveries than for developments of new theory\u201d: <a href=\"https://doi.org/10.1177/1745691611434210\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/17456916114342</span><span class=\"invisible\">10</span></a> </p><p><a href=\"https://fediscience.org/tags/Science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Science</span></a><br><a href=\"https://fediscience.org/tags/Theory\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Theory</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/1745691611434210",
    "title": "doi.org/10.1177/17456916114342...",
    "latest": "2023-05-24T08:22:15+00:00",
    "last_post": {
      "url": "https://fediscience.org/@MarkRubin/110417275690043774",
      "content": "<p>New study finds \u201carticles that develop and present a new method tend to be more disruptive.\u201d <a href=\"https://doi.org/10.1177/00031224231168074\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/00031224231168</span><span class=\"invisible\">074</span></a></p><p>Reminds me of Greenwald (2012): \u201cAn analysis of the recent history of Nobel Prizes in science unexpectedly revealed that these awards were given much more often for creation of methods and for method-based discoveries than for developments of new theory\u201d: <a href=\"https://doi.org/10.1177/1745691611434210\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/17456916114342</span><span class=\"invisible\">10</span></a> </p><p><a href=\"https://fediscience.org/tags/Science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Science</span></a><br><a href=\"https://fediscience.org/tags/Theory\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Theory</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.09676",
    "title": "Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications",
    "latest": "2023-05-24T08:08:02+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mymarkup/110422625514652656",
      "content": "<p>Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications</p><p>\u201dBroadly, our findings suggest that though multidisciplinary venues intend to cater to more general audiences, some fields' writing norms may act as barriers rather than bridges, and thus impede the dispersion of scholarly ideas.\u201d</p><p><a href=\"https://arxiv.org/abs/2212.09676\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2212.09676</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.social/tags/preprintwatch\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>preprintwatch</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13403v1",
    "title": "https://arxiv.org/abs/2305.13403v1",
    "latest": "2023-05-24T08:07:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422632150721880",
      "content": "<p>\ud83d\udcdd GATology for Linguistics: What Syntactic Dependencies It Knows \ud83d\udcda</p><p>\"A graph neural network which is a strategy for modeling and representing explicit syntactic knowledge, and can work with pre-trained models such as BERT in downstream tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/UniversalDependencies/UD_\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/UniversalDependenci</span><span class=\"invisible\">es/UD_</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13403v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13403v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/25152459211007467",
    "title": "doi.org/10.1177/25152459211007...",
    "latest": "2023-05-24T07:56:53+00:00",
    "last_post": {
      "url": "https://ecoevo.social/@sortee/110422354226072945",
      "content": "<p>Evidence from psychology shows that <a href=\"https://ecoevo.social/tags/registeredreports\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>registeredreports</span></a> have a lower positive result rate than the standard literature &amp; likely reduces <a href=\"https://ecoevo.social/tags/publicationbias\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>publicationbias</span></a> &amp; QRPs </p><p>\ud83d\udcf0by <span class=\"h-card\"><a href=\"https://mastodon.online/@annescheel\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>annescheel</span></a></span> Mitchell Schijen &amp; <span class=\"h-card\"><a href=\"https://mastodon.social/@lakens\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lakens</span></a></span>  <a href=\"https://doi.org/10.1177/25152459211007467\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/25152459211007</span><span class=\"invisible\">467</span></a></p><p>Time for eco &amp; evo to support Registered Reports!</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13401v1",
    "title": "https://arxiv.org/abs/2305.13401v1",
    "latest": "2023-05-24T07:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422514226278980",
      "content": "<p>\ud83d\udcdd A Study of Conceptual Language Similarity: Comparison and Evaluation \ud83d\udcda</p><p>\"The conceptual similarity is based on the hypothesis that two languages are similar if they encode the meanings of concepts using similar linguistic structures, such as word order and verbal inflection.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/DianDYu/language\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/DianDYu/language</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13401v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13401v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13386v1",
    "title": "Can LLMs facilitate interpretation of pre-trained language models?",
    "latest": "2023-05-24T07:27:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422474828156047",
      "content": "<p>\ud83d\udcdd Can LLMs Facilitate Interpretation of Pre-Trained Language Models? \ud83d\udcda</p><p>\"A: Proposes using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models (LM).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13386v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13386v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://doi.org/10.18452/25440",
    "title": "http://doi.org/10.18452/25440",
    "latest": "2023-05-24T07:26:43+00:00",
    "last_post": {
      "url": "https://mastodon.social/@hauschke/110422471735229407",
      "content": "<p>Ulrike K\u00fcsters vom IRB stellt das Positionspapier der <span class=\"h-card\"><a href=\"https://openbiblio.social/@dini\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>dini</span></a></span> AG FIS  vor:</p><p>Management von Forschungsinformationen in Hochschulen und Forschungseinrichtungen. Eine Standortbestimmung (2022)</p><p><a href=\"http://doi.org/10.18452/25440\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">doi.org/10.18452/25440</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.social/tags/111bibliocon\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>111bibliocon</span></a> <a href=\"https://mastodon.social/tags/FIS\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>FIS</span></a> <a href=\"https://mastodon.social/tags/Forschungsberichterstattung\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Forschungsberichterstattung</span></a> <a href=\"https://mastodon.social/tags/Forschungsinformationssystem\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Forschungsinformationssystem</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@hauschke",
        "display_name": "hauschke"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280902",
    "title": "The efficacy of interventions in reducing belief in conspiracy theories: A systematic review",
    "latest": "2023-05-24T06:30:50+00:00",
    "last_post": {
      "url": "https://mastodon.social/@matthewfacciani/110395774887850512",
      "content": "<p>Meta-analysis reveals that teaching people how to think critically (i.e. prebunking) combats misinformation more effectively than trying to debunk false claims. <br><a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280902\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0280902</span></a><br><a href=\"https://mastodon.social/tags/misinformation\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>misinformation</span></a> <a href=\"https://mastodon.social/tags/MisinfoResearch\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MisinfoResearch</span></a> <a href=\"https://mastodon.social/tags/prebunking\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>prebunking</span></a> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/sociology\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>sociology</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/psychology\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>psychology</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/communicationscholars\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>communicationscholars</span></a></span></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@dougholton",
        "display_name": "Doug Holton"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.11140.pdf",
    "title": "https://arxiv.org/pdf/2305.11140.pdf",
    "latest": "2023-05-24T06:21:09+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@fbk_mt/110422213868329357",
      "content": "<p>Our pick of the week by Beatrice Savoldi: \"Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model\" Amrhein et al., 2023</p><p><a href=\"https://arxiv.org/pdf/2305.11140.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.11140.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://sigmoid.social/tags/pickoftheweek\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>pickoftheweek</span></a> <a href=\"https://sigmoid.social/tags/gender\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>gender</span></a> <a href=\"https://sigmoid.social/tags/bias\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>bias</span></a> <a href=\"https://sigmoid.social/tags/fair\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>fair</span></a> <a href=\"https://sigmoid.social/tags/fairness\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>fairness</span></a> <a href=\"https://sigmoid.social/tags/debiasing\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>debiasing</span></a> <a href=\"https://sigmoid.social/tags/translation\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>translation</span></a> <a href=\"https://sigmoid.social/tags/MT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MT</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@fbk_mt",
        "display_name": "MT Group at FBK"
      }
    ]
  },
  {
    "link": "https://academic.oup.com/pnasnexus/article/2/3/pgad051/7059318?login=false",
    "title": "Complex systems of secrecy: the offshore networks of oligarchs",
    "latest": "2023-05-24T06:01:16+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mikeolson/110422135730518945",
      "content": "<p>Kleptocracy depends on good infrastructure for money laundering. That infrastructure operates worldwide, but the US and UK provide a great deal of it. The most effective way to sanction criminals, make money laundering much harder and penalize corruption and kleptocracy would be to impose know-your-customer and reporting obligations on lawyers and accountants.</p><p>This post comes to you thanks to this excellent PNAS paper:</p><p><a href=\"https://academic.oup.com/pnasnexus/article/2/3/pgad051/7059318?login=false\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">academic.oup.com/pnasnexus/art</span><span class=\"invisible\">icle/2/3/pgad051/7059318?login=false</span></a></p><p>Came my way via 3QD:</p><p><a href=\"https://3quarksdaily.com/3quarksdaily/2023/05/sean-carrolls-mindscape-podcast-brooke-harrington-on-offshore-wealth-as-a-complex-system.html\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">3quarksdaily.com/3quarksdaily/</span><span class=\"invisible\">2023/05/sean-carrolls-mindscape-podcast-brooke-harrington-on-offshore-wealth-as-a-complex-system.html</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@mikeolson",
        "display_name": "Mike Olson"
      }
    ]
  },
  {
    "link": "https://doi.org/10.36253/jlis.it-498",
    "title": "https://doi.org/10.36253/jlis.it-498",
    "latest": "2023-05-24T05:53:57+00:00",
    "last_post": {
      "url": "https://hcommons.social/@jcpeyssard/110422106908824416",
      "content": "<p>Snijder, Ronald. 2023. \u201cBooks in a bubble.: Assessing the OAPEN Library Collection\u201d. JLIS.It 14 (2):75-92. <a href=\"https://doi.org/10.36253/jlis.it-498\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.36253/jlis.it-498</span><span class=\"invisible\"></span></a>.</p>"
    },
    "people": [
      {
        "url": "https://hcommons.social/@jcpeyssard",
        "display_name": "Jean-Christophe Peyssard"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2210.06927",
    "title": "Prediction can be safely used as a proxy for explanation in causally consistent Bayesian generalized linear models",
    "latest": "2023-05-24T04:20:36+00:00",
    "last_post": {
      "url": "https://fediscience.org/@scholzmx/110417237170725257",
      "content": "<p>Updated preprint is up on arxiv. It is now focused on just the prediction as proxy for explanation question. Finding remains that prediction is a valid and reliable proxy for explanation in causally unbiased GLMs when comparing models with the same linear predictor term.<br><a href=\"https://arxiv.org/abs/2210.06927\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2210.06927</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.08167.pdf",
    "title": "https://arxiv.org/pdf/2304.08167.pdf",
    "latest": "2023-05-24T03:49:39+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618137001202",
      "content": "<p>\"Classification of news spreading barriers\" an approach to barrier classification where the semantics of news articles are inferred through Wikipedia concepts. </p><p>(Sittar. et al, 2023)</p><p><a href=\"https://arxiv.org/pdf/2304.08167.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.08167.pdf</span><span class=\"invisible\"></span></a> <a href=\"https://twitter.com/WikiResearch/status/1660675562862518272\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1660675562862518272</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/pdf/10.1145/3579517",
    "title": "dl.acm.org/doi/pdf/10.1145/357...",
    "latest": "2023-05-24T03:49:39+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618163571101",
      "content": "<p>\"Queer Identities, Normative Databases: Challenges to Capturing Queerness On @Wikidata\", showing inherent and unaddressed frictions when translating queer identities to the confines of a structured database. </p><p>( Weathington and Brubaker 2023)</p><p><a href=\"https://dl.acm.org/doi/pdf/10.1145/3579517\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/357</span><span class=\"invisible\">9517</span></a> <a href=\"https://twitter.com/WikiResearch/status/1661060518772195335\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1661060518772195335</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09497",
    "title": "Curious Rhythms: Temporal Regularities of Wikipedia Consumption",
    "latest": "2023-05-24T03:49:38+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618084830656",
      "content": "<p>\"Curious Rhythms: Temporal Regularities of <span class=\"h-card\"><a href=\"https://mastodon.social/@Wikipedia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Wikipedia</span></a></span> Consumption\" a large-scale analysis of billions of timezone-corrected page requests mined from English Wikipedia's server logs.</p><p>(Piccardi et al, 2023)</p><p><a href=\"https://arxiv.org/abs/2305.09497\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09497</span><span class=\"invisible\"></span></a> <a href=\"https://twitter.com/WikiResearch/status/1658871787692736512\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1658871787692736512</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.05403.pdf",
    "title": "https://arxiv.org/pdf/2305.05403.pdf",
    "latest": "2023-05-24T03:49:37+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110421618061046604",
      "content": "<p>\"Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey\" including fundamental methodologies and practice-oriented recommendations on how to choose between different approaches for a specific problem</p><p>(Razniewski et al, 2023)</p><p><a href=\"https://arxiv.org/pdf/2305.05403.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.05403.pdf</span><span class=\"invisible\"></span></a><br>@Bosch_AI <a href=\"https://twitter.com/WikiResearch/status/1658472419185786880\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1658472419185786880</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13047",
    "title": "Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media",
    "latest": "2023-05-24T02:18:03+00:00",
    "last_post": {
      "url": "https://mastodon.social/@andreskarjus/110417263335240534",
      "content": "<p>Preprint w Mark Mets <span class=\"h-card\"><a href=\"https://mastodon.social/@IndrekIbrus\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>IndrekIbrus</span></a></span> <span class=\"h-card\"><a href=\"https://mastodon.social/@schichmax\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>schichmax</span></a></span> <br>\"Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media\"<br><a href=\"https://arxiv.org/abs/2305.13047\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13047</span><span class=\"invisible\"></span></a><br>We compare 2 corpora, a mainstream publisher &amp; a rightwing populist online outlet. Rightwingers are ofc more negative &amp;talk more about immigration - but now we can quantify. Compared multiple LLMs &amp;ChatGPT3.5, which gets same F1 as a finetuned RoBERTa. But zero-shot! That's pretty awesome.</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14224",
    "title": "mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations",
    "latest": "2023-05-24T02:06:24+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421212155208290",
      "content": "<p>mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations</p><p>Outperforms mT5 at the same parameter sizes by a large margin on NLU and NLG tasks in 40+ languages.</p><p><a href=\"https://arxiv.org/abs/2305.14224\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14224</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14177",
    "title": "ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry",
    "latest": "2023-05-24T01:58:09+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421179739560551",
      "content": "<p>ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry</p><p>Presents a set of open-source RL environments, ChemGymRL, based on the Open AI Gym template</p><p><a href=\"https://arxiv.org/abs/2305.14177\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14177</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13840",
    "title": "https://arxiv.org/abs/2305.13840",
    "latest": "2023-05-24T01:54:16+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421164483028998",
      "content": "<p>Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models</p><p>proj: <a href=\"https://controlavideo.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">controlavideo.github.io/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.13840\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13840</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14330",
    "title": "https://arxiv.org/abs/2305.14330",
    "latest": "2023-05-24T01:33:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421081480423378",
      "content": "<p>Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation</p><p>repo: <a href=\"https://github.com/KU-CVLAB/DirecT2V\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/KU-CVLAB/DirecT2V</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14330\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14330</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14233",
    "title": "https://arxiv.org/abs/2305.14233",
    "latest": "2023-05-24T01:30:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110421069670056432",
      "content": "<p>Enhancing Chat Language Models by Scaling High-quality Instructional Conversations</p><p>- UltraChat contains 1.5M high-quality, diverse multi-turn dialogues<br>- UltraLLaMA outperforms the SotA open-source model, Vicuna </p><p>repo: <a href=\"https://github.com/thunlp/UltraChat\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/thunlp/UltraChat</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14233\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14233</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14325",
    "title": "https://arxiv.org/abs/2305.14325",
    "latest": "2023-05-24T01:12:11+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420998965406370",
      "content": "<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate</p><p>proj: <a href=\"https://composable-models.github.io/llm_debate/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">composable-models.github.io/ll</span><span class=\"invisible\">m_debate/</span></a><br>repo: <a href=\"https://github.com/composable-models/llm_multiagent_debate\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/composable-models/l</span><span class=\"invisible\">lm_multiagent_debate</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.14325\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14325</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.00740",
    "title": "Inspecting and Editing Knowledge Representations in Language Models",
    "latest": "2023-05-24T01:06:13+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420975523967772",
      "content": "<p>RT @evanqed@twitter.com</p><p>You\u2019ve heard of model editing, now get ready for \ud83d\udcdd representation editing \ud83d\udcdd</p><p>In our new paper, we find directions in LM rep space that make the LM assert a fact is true.</p><p>Paper: <a href=\"https://arxiv.org/abs/2304.00740\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.00740</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/evandez/REMEDI\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/evandez/REMEDI</span><span class=\"invisible\"></span></a><br>w/ @belindazli@twitter.com @jacobandreas@twitter.com</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/evanqed/status/1661172005083791364\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/evanqed/status/166</span><span class=\"invisible\">1172005083791364</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13534",
    "title": "How Language Model Hallucinations Can Snowball",
    "latest": "2023-05-24T01:01:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420956304706540",
      "content": "<p>How Language Model Hallucinations Can Snowball</p><p><a href=\"https://arxiv.org/abs/2305.13534\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13534</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13735",
    "title": "Aligning Large Language Models through Synthetic Feedback",
    "latest": "2023-05-24T00:59:23+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420948643044011",
      "content": "<p>Aligning Large Language Models through Synthetic Feedback</p><p>Proposes a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs.</p><p><a href=\"https://arxiv.org/abs/2305.13735\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13735</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/1706.03762",
    "title": "https://arxiv.org/abs/1706.03762",
    "latest": "2023-05-23T22:49:13+00:00",
    "last_post": {
      "url": "https://mastodon.gamedev.place/@jay/110420242881451785",
      "content": "<p>GPT - Generatively Pre-trained Transformer <br> <br>\"Let's Build GPT. From Scratch. In Code. Spelled Out\" by Andrej Karpathy<br><a href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">youtube.com/watch?v=kCc8FmEb1n</span><span class=\"invisible\">Y</span></a><br>(Coding starts at 42:13 mark) <br> <br>\"ChatGPT... is trained on a good chunk of the Internet\" (3:26 mark)  \ud83e\udd14 &lt;-- Meet Mr. Copyright \u00a9 <br> <br>Andrej Karpathy<br><a href=\"https://build.microsoft.com/en-US/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8?source=/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">build.microsoft.com/en-US/spea</span><span class=\"invisible\">kers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8?source=/speakers/1db4b2b5-c5df-4372-aaae-c96b6fa93ef8</span></a> <br> <br>\"Attention Is All You Need\" Seminal Paper on AI Transformers<br><a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/1706.03762</span><span class=\"invisible\"></span></a> <br> <br><a href=\"https://mastodon.gamedev.place/tags/Microsoft\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Microsoft</span></a> <a href=\"https://mastodon.gamedev.place/tags/MicrosoftBuild\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MicrosoftBuild</span></a> <a href=\"https://mastodon.gamedev.place/tags/Build2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Build2023</span></a> <a href=\"https://mastodon.gamedev.place/tags/ChatGPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ChatGPT</span></a> <a href=\"https://mastodon.gamedev.place/tags/GPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT</span></a> <a href=\"https://mastodon.gamedev.place/tags/OpenAI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAI</span></a> <a href=\"https://mastodon.gamedev.place/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://mastodon.gamedev.place/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <a href=\"https://mastodon.gamedev.place/tags/Copyright\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Copyright</span></a> <a href=\"https://mastodon.gamedev.place/tags/Infringement\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Infringement</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13252",
    "title": "\"According to ...\" Prompting Language Models Improves Quoting from Pre-Training Data",
    "latest": "2023-05-23T22:27:54+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110420123288249295",
      "content": "<p>RT @orionweller@twitter.com</p><p>Can we guide LLMs to quote text from their pre-training data using prefixes like \"According To ..\", improving grounding and reducing hallucination? We discovered that LLMs do have this capability and can increase or decrease quoting on request \ud83e\udd2f</p><p>\ud83d\udcdd:<a href=\"https://arxiv.org/abs/2305.13252\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13252</span><span class=\"invisible\"></span></a> 1/5</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/orionweller/status/1660994241773047810\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/orionweller/status</span><span class=\"invisible\">/1660994241773047810</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://osf.io/fawj3",
    "title": "https://osf.io/fawj3",
    "latest": "2023-05-23T20:30:24+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@ElineVG/110419463003101195",
      "content": "<p>I had a wonderful time this morning presenting my poster on an efficient Bayesian observer model predicting attractive and repulsive temporal context effects @VSSMtg <a href=\"https://fosstodon.org/tags/VSS2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>VSS2023</span></a>! If you missed it and are interested, find the poster here: <a href=\"https://osf.io/fawj3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">osf.io/fawj3</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://scicomm.xyz/@sharoz",
        "display_name": "Steve Haroz (@sharoz on \ud83d\udc24)"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1177/20319525231167983",
    "title": "doi.org/10.1177/20319525231167...",
    "latest": "2023-05-23T20:19:14+00:00",
    "last_post": {
      "url": "https://someone.elses.computer/@mikarv/110258344105065543",
      "content": "<p>In case you\u2019ve been hearing about the draft EU <a href=\"https://someone.elses.computer/tags/PlatformWorkDirective\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PlatformWorkDirective</span></a> and wanted to learn more, our paper on its <a href=\"https://someone.elses.computer/tags/algorithmicManagement\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>algorithmicManagement</span></a> provisions (w Six Silberman &amp; <span class=\"h-card\"><a href=\"https://someone.elses.computer/@RDBinns\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>RDBinns</span></a></span>) now out in the European Labour Law Journal. <br>tl;dr: generally v well drafted, novel aspects worth studying, may create unintended effects, could benefit from some tweaks to avoid tensions and to shore up its complexities in a situation of high power asymmetries. <a href=\"https://doi.org/10.1177/20319525231167983\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1177/20319525231167</span><span class=\"invisible\">983</span></a> <a href=\"https://someone.elses.computer/tags/gigEconomy\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>gigEconomy</span></a> <a href=\"https://someone.elses.computer/tags/platformWork\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>platformWork</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.scot/@karengregory",
        "display_name": "Karen Gregory"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05322",
    "title": "https://doi.org/10.21105/joss.05322",
    "latest": "2023-05-23T20:08:48+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110419806006542749",
      "content": "<p>Just published in JOSS: 'SarcGraph: A Python package for analyzing the contractile behavior of pluripotent stem cell-derived cardiomyocytes' <a href=\"https://doi.org/10.21105/joss.05322\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05322</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13281",
    "title": "LM vs LM: Detecting Factual Errors via Cross Examination",
    "latest": "2023-05-23T19:28:39+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110419648178699377",
      "content": "<p>RT @johnjnay@twitter.com</p><p>LLM vs LLM: Detecting Errors via Cross Examining Agents</p><p>-An incorrect claim is likely to result in inconsistency w/ other claims<br>-Multi-turn interactions between LLM that generated claim and Examiner LLM</p><p>-Outperforms baselines across factual benchmarks</p><p><a href=\"https://arxiv.org/abs/2305.13281\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13281</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/johnjnay/status/1660830409276440579\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/johnjnay/status/16</span><span class=\"invisible\">60830409276440579</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09781",
    "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification",
    "latest": "2023-05-23T18:55:24+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110387182088605932",
      "content": "<p>SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification</p><p>Significantly reduces the end-to-end latency while provably preserving model quality</p><p><a href=\"https://arxiv.org/abs/2305.09781\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09781</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.sagepub.com/doi/10.1177/15554120221115393",
    "title": "journals.sagepub.com/doi/10.11...",
    "latest": "2023-05-23T18:55:08+00:00",
    "last_post": {
      "url": "https://scholar.social/@electricarchaeo/110419516378183749",
      "content": "<p>hell ya. I always give a little cheer when I see things that we put up on Epoiesen cited in other articles. Today, a reflection on writing history through games cites work that McCall published with us on Epoiesen: <a href=\"https://journals.sagepub.com/doi/10.1177/15554120221115393\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.sagepub.com/doi/10.11</span><span class=\"invisible\">77/15554120221115393</span></a></p>"
    },
    "people": [
      {
        "url": "https://scholar.social/@telliott",
        "display_name": "Tom Elliott"
      },
      {
        "url": "https://scholar.social/@electricarchaeo",
        "display_name": "Shawn Graham"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12182",
    "title": "https://arxiv.org/abs/2305.12182",
    "latest": "2023-05-23T18:42:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419465347473367",
      "content": "<p>Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages</p><p>repo: <a href=\"https://github.com/cisnlp/Glot500\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/cisnlp/Glot500</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.12182\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12182</span><span class=\"invisible\"></span></a><br>HuggingFace: <a href=\"https://huggingface.co/cis-lmu/glot500-base\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">huggingface.co/cis-lmu/glot500</span><span class=\"invisible\">-base</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13016",
    "title": "Iterative Forward Tuning Boosts In-context Learning in Language Models",
    "latest": "2023-05-23T18:41:59+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464644045437",
      "content": "<p>RT @huybery@twitter.com</p><p>In-context learning as the mysterious ability in LMs, and some works points to a link with gradient descent.<br>This inspired us to propose \ud83e\udd14deep-thinking, which boosts ICL by iterative forward tuning.<br>It is possible to tune LMs without backpropagation!</p><p><a href=\"https://arxiv.org/abs/2305.13016\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13016</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/huybery/status/1660835495310499840\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/huybery/status/166</span><span class=\"invisible\">0835495310499840</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13230",
    "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis",
    "latest": "2023-05-23T18:41:56+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464459802469",
      "content": "<p>To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis</p><p>Studies what would happen if we train LLM with repeated data and how we can alleviate the LLM mult-epoch degradation.</p><p><a href=\"https://arxiv.org/abs/2305.13230\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13230</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13144",
    "title": "Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline",
    "latest": "2023-05-23T18:41:53+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464272109741",
      "content": "<p>RT @zhengzangw@twitter.com</p><p>Can we use <a href=\"https://sigmoid.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> to speed up <a href=\"https://sigmoid.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> inference? \ud83d\udd25Excited to share our new work: Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline.\ud83e\uddf54</p><p>ArXiv: <a href=\"https://arxiv.org/abs/2305.13144\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13144</span><span class=\"invisible\"></span></a><br>Blog: <a href=\"https://zhengzangw.github.io/blogs/seqsch/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">zhengzangw.github.io/blogs/seq</span><span class=\"invisible\">sch/</span></a><br>Code: <a href=\"https://github.com/zhengzangw/Sequence-Scheduling\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/zhengzangw/Sequence</span><span class=\"invisible\">-Scheduling</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/zhengzangw/status/1660833269972074496\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/zhengzangw/status/</span><span class=\"invisible\">1660833269972074496</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13245",
    "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    "latest": "2023-05-23T18:41:50+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419464061647289",
      "content": "<p>RT @michielsdj@twitter.com</p><p>New paper! Multi-query attention trades quality for speed and requires training a new model. Instead uptrain improved MQ variant from existing multi-head model! </p><p>Work with Joshua Ainslie, James Lee-Thorp, @_theopompus@twitter.com, Federico Lebron, Sumit Sanghai.</p><p><a href=\"https://arxiv.org/abs/2305.13245\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13245</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/michielsdj/status/1660839037903536129\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/michielsdj/status/</span><span class=\"invisible\">1660839037903536129</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12524",
    "title": "TheoremQA: A Theorem-driven Question Answering dataset",
    "latest": "2023-05-23T18:41:49+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463995882500",
      "content": "<p>RT @WenhuChen@twitter.com</p><p>New Arxiv: <a href=\"https://arxiv.org/abs/2305.12524\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12524</span><span class=\"invisible\"></span></a></p><p>GPT-4/PaLM-2 have both shown almost perfect performance on existing grade school math dataset. What about more challenging STEM questions, especially the ones which require specific theorems, like Stoke's theorem, Wiener Process, etc?</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/WenhuChen/status/1660832837715611648\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WenhuChen/status/1</span><span class=\"invisible\">660832837715611648</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13009",
    "title": "https://arxiv.org/abs/2305.13009",
    "latest": "2023-05-23T18:41:41+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463458618269",
      "content": "<p>Textually Pretrained Speech Language Models</p><p>Presents the largest SpeechLM both in terms of number of parameters and training data.</p><p>proj: <a href=\"https://pages.cs.huji.ac.il/adiyoss-lab/twist/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">pages.cs.huji.ac.il/adiyoss-la</span><span class=\"invisible\">b/twist/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.13009\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13009</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13035",
    "title": "Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design",
    "latest": "2023-05-23T18:41:38+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110419463271311196",
      "content": "<p>Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design</p><p>Their shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute.</p><p><a href=\"https://arxiv.org/abs/2305.13035\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13035</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://doi.org/10.54900/g0qks-tcz98",
    "title": "https://doi.org/10.54900/g0qks-tcz98",
    "latest": "2023-05-23T17:24:14+00:00",
    "last_post": {
      "url": "https://ravenation.club/@mpe/110419158940172271",
      "content": "<p>Attempts at automating journal subject classification by my @CrossrefOrg colleague, Esha Datta<br><a href=\"https://doi.org/10.54900/g0qks-tcz98\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.54900/g0qks-tcz98</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12234",
    "title": "The Damage to Lunar Orbiting Spacecraft Caused by the Ejecta of Lunar Landers",
    "latest": "2023-05-23T17:10:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110419103143950353",
      "content": "<p>The Damage to Lunar Orbiting Spacecraft Caused by the Ejecta of Lunar Landers - <a href=\"https://arxiv.org/abs/2305.12234\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12234</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://bayes.club/@akhilrao",
        "display_name": "edgeworth boxlord"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://doi.org/10.5281/zenodo.5526634",
    "title": "https://doi.org/10.5281/zenodo.5526634",
    "latest": "2023-05-23T17:01:30+00:00",
    "last_post": {
      "url": "https://mastodon.social/@brembs/110418324534008701",
      "content": "<p>Could this be the paradigm shift all of <a href=\"https://mastodon.social/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a> has been waiting for?</p><p>Council of the EU adopts new principles:<br>\"interoperable, not-for-profit infrastructures for publishing based on open source software and open standards\"<br><a href=\"https://data.consilium.europa.eu/doc/document/ST-8827-2023-INIT/en/pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">data.consilium.europa.eu/doc/d</span><span class=\"invisible\">ocument/ST-8827-2023-INIT/en/pdf</span></a></p><p>and now ten major research organizations support the proposal:<br><a href=\"https://www.coalition-s.org/wp-content/uploads/2023/05/JointResponse2CouncilScholCommConclusions.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">coalition-s.org/wp-content/upl</span><span class=\"invisible\">oads/2023/05/JointResponse2CouncilScholCommConclusions.pdf</span></a></p><p>What they propose is nearly identical to our proposal:<br><a href=\"https://doi.org/10.5281/zenodo.5526634\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.5281/zenodo.5526634</span><span class=\"invisible\"></span></a></p><p>Does this now get the ball rolling, or is it just words on paper?</p>"
    },
    "people": [
      {
        "url": "https://ecoevo.social/@cboettig",
        "display_name": "Carl Boettiger"
      },
      {
        "url": "https://ciberlandia.pt/@villares",
        "display_name": "Alexandre B A Villares \ud83d\udc0d \u2614"
      },
      {
        "url": "https://mastodon.social/@edzer",
        "display_name": "Edzer Pebesma"
      },
      {
        "url": "https://mastodon.social/@igor",
        "display_name": "Igor Brigadir"
      },
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://hci.social/@jbigham",
        "display_name": "Jeff Bigham"
      },
      {
        "url": "https://bbq.snoot.com/@ProgGrrl",
        "display_name": "ProgGrrl"
      },
      {
        "url": "https://datasci.social/@yy",
        "display_name": "YY Ahn"
      },
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://econtwitter.net/@ito",
        "display_name": "Tim Gushue"
      },
      {
        "url": "https://sfba.social/@morgandawn",
        "display_name": "morgandawn"
      },
      {
        "url": "https://mastodon.social/@tiffanycli",
        "display_name": "Tiffany Li"
      },
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      },
      {
        "url": "https://mas.to/@enridaga",
        "display_name": "enridaga"
      },
      {
        "url": "https://starbase80.wtf/@5ciFiGirl",
        "display_name": "Sci-Fi Girl"
      },
      {
        "url": "https://fediscience.org/@mario_angst_sci",
        "display_name": "Mario Angst"
      },
      {
        "url": "https://h4.io/@joshisanonymous",
        "display_name": "Joshua McNeill"
      }
    ]
  },
  {
    "link": "https://doi.org/10.54337/jovi.v1i1.7782",
    "title": "doi.org/10.54337/jovi.v1i1.778...",
    "latest": "2023-05-23T16:55:55+00:00",
    "last_post": {
      "url": "https://hci.social/@jovi/110418478552305809",
      "content": "<p>JoVI's first publication is now live! See <a href=\"https://doi.org/10.54337/jovi.v1i1.7782\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.54337/jovi.v1i1.778</span><span class=\"invisible\">2</span></a> for our inaugural editorial &amp; mission statement.</p><p>/cc <span class=\"h-card\"><a href=\"https://hci.social/@khornbaek\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>khornbaek</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@amelia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>amelia</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@arvind\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>arvind</span></a></span> <span class=\"h-card\"><a href=\"https://mastodon.social/@scheidegger\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>scheidegger</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@jschwabish\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>jschwabish</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@elm\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>elm</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@lane\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lane</span></a></span> <span class=\"h-card\"><a href=\"https://vis.social/@lace\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lace</span></a></span> <span class=\"h-card\"><a href=\"https://hci.social/@floe\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>floe</span></a></span> <span class=\"h-card\"><a href=\"https://fediscience.org/@mjskay\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>mjskay</span></a></span> <span class=\"h-card\"><a href=\"https://mstdn.science/@lonnibesancon\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lonnibesancon</span></a></span></p>"
    },
    "people": [
      {
        "url": "https://vis.social/@elm",
        "display_name": "Niklas Elmqvist"
      },
      {
        "url": "https://vis.social/@dr_tj",
        "display_name": "Dr. T.J. Jankun-Kelly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12212v1",
    "title": "Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT",
    "latest": "2023-05-23T16:30:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110418946391923185",
      "content": "<p>\ud83d\udcdd Prompt ChatGPT in MNER: Improved Multimodal Named Entity Recognition Method Based on Auxiliary Refining Knowledge From ChatGPT \ud83d\udcda</p><p>\"Prompt ChatGPT In MNER (PGIM) utilizes ChatGPT, an implicit knowledge engine, to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12212v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12212v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12074v1",
    "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
    "latest": "2023-05-23T10:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417412880049300",
      "content": "<p>\ud83d\udcdd DisCo: Distilled Student Models Co-Training for Semi-Supervised Text Mining \ud83d\udcda</p><p>\"A novel co-training technique to optimize multiple small student models generated from a large PLM using knowledge distillation under diversified views: model and data views produced by different distillation strategies and input augmentations, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12074v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12074v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12147",
    "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4",
    "latest": "2023-05-23T10:00:05+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110417412101749441",
      "content": "<p>LogiCoT: Logical Chain-of-Thought instruction-tuning data collection with GPT-4. ~ Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang. <a href=\"https://arxiv.org/abs/2305.12147\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12147</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/GPT4\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT4</span></a> <a href=\"https://mathstodon.xyz/tags/Logic\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Logic</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11252v1",
    "title": "Brain-inspired learning in artificial neural networks: a review",
    "latest": "2023-05-23T09:47:20+00:00",
    "last_post": {
      "url": "https://mastodon.social/@achterbrain/110417362350083227",
      "content": "<p>New <a href=\"https://mastodon.social/tags/review\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>review</span></a> led by Samuel Schmidgall:<br>'Brain-inspired learning in artificial neural networks'<br><a href=\"https://arxiv.org/abs/2305.11252v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11252v1</span><span class=\"invisible\"></span></a></p><p>This was a great <a href=\"https://mastodon.social/tags/collaboration\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>collaboration</span></a> supported by OpenBioML's new <a href=\"https://mastodon.social/tags/NeuroAI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuroAI</span></a> initiative! Looking forward to many more projects coming out of this <a href=\"https://mastodon.social/tags/open\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>open</span></a> collaborative <a href=\"https://mastodon.social/tags/research\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>research</span></a> community!</p><p><a href=\"https://mastodon.social/tags/neuroscience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>neuroscience</span></a> <a href=\"https://mastodon.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@achterbrain",
        "display_name": "Jascha Achterberg"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12057v1",
    "title": "https://arxiv.org/abs/2305.12057v1",
    "latest": "2023-05-23T09:40:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417334069138419",
      "content": "<p>\ud83d\udcdd Accurate Knowledge Distillation with N-Best Reranking \ud83d\udcda</p><p>\"Leverages a diverse set of models, including publicly available large pretrained models, to provide more accurate pseudo-labels for training student models via n-best reranking.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/fairseq/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/fa</span><span class=\"invisible\">irseq/</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12057v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12057v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.12073v1.pdf",
    "title": "https://arxiv.org/pdf/2305.12073v1.pdf",
    "latest": "2023-05-23T09:14:35+00:00",
    "last_post": {
      "url": "https://mastodon.social/@tiago_ribeiro/110417142228641639",
      "content": "<p>\"GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance\"</p><p>\"Our findings reinforce the exceptional performance of the GELU activation function, which attains the highest test accuracy and lowest test loss among the activation functions investigated. Other activation functions, such as Hardswish and ReLU6, exhibit commendable performance as well...\"</p><p><a href=\"https://mastodon.social/tags/GELU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GELU</span></a> <a href=\"https://mastodon.social/tags/ReLU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReLU</span></a> <a href=\"https://mastodon.social/tags/HardShrink\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>HardShrink</span></a> <a href=\"https://mastodon.social/tags/leakyReLU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>leakyReLU</span></a> <a href=\"https://mastodon.social/tags/ReLU6\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReLU6</span></a></p><p>\ud83d\udd17<a href=\"https://arxiv.org/pdf/2305.12073v1.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.12073v1.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12029v1",
    "title": "https://arxiv.org/abs/2305.12029v1",
    "latest": "2023-05-23T09:00:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417177104587367",
      "content": "<p>\ud83d\udcdd MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Introduces two models based on BERT and RoBERTa as baselines on this dataset, which can be used as a benchmark for future research to tackle this task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/huashen218/MultiTurnCleanup.git\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/huashen218/MultiTur</span><span class=\"invisible\">nCleanup.git</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12029v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12029v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12027v1",
    "title": "Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings",
    "latest": "2023-05-23T08:30:12+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417059049321121",
      "content": "<p>\ud83d\udcdd Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings \ud83d\udcda\ud83d\udc7e</p><p>\"DUCK infuses prior knowledge on entity types in entity representations by learning to place entities of similar type close to each other in the embedding space, using boxes and hyperspheres.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12027v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12027v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12018v1",
    "title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases",
    "latest": "2023-05-23T08:00:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416940920589699",
      "content": "<p>\ud83d\udcdd BOLT: Fast Energy-Based Controlled Text Generation with Tunable Biases \ud83d\udcda</p><p>\"Proposes BOLT which relies on tunable biases to adjust output logits directly to achieve fast convergence in controlled generation while maintaining the generator's autoregressive nature to assert a strong control on token-wise conditional dependencies and fluent output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12018v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12018v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12001v1",
    "title": "https://arxiv.org/abs/2305.12001v1",
    "latest": "2023-05-23T07:40:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416862157122454",
      "content": "<p>\ud83d\udcdd OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models \ud83d\udcda</p><p>\"Entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/metaseq\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/me</span><span class=\"invisible\">taseq</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12001v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12001v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12196",
    "title": "Experimental results from applying GPT-4 to an unpublished formal language",
    "latest": "2023-05-23T07:27:50+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110416776676268021",
      "content": "<p>Experimental results from applying GPT-4 to an unpublished formal language. ~ Gregor vom Scheidt (@GregorVScheidt). <a href=\"https://arxiv.org/abs/2305.12196\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12196</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/ChatGPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ChatGPT</span></a> <a href=\"https://mathstodon.xyz/tags/GPT4\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT4</span></a> <a href=\"https://mathstodon.xyz/tags/FunctionalProgramming\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>FunctionalProgramming</span></a> <a href=\"https://mathstodon.xyz/tags/Logic\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Logic</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11326",
    "title": "Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data",
    "latest": "2023-05-23T07:27:07+00:00",
    "last_post": {
      "url": "https://fediscience.org/@JordiCabot/110416810949851518",
      "content": "<p>Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data</p><p>&lt;- From a CSV to a fully functional chatbot </p><p>Paper: <a href=\"https://arxiv.org/abs/2305.11326\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11326</span><span class=\"invisible\"></span></a><br>Summary: <a href=\"https://livablesoftware.com/chatbots-open-data-project/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">livablesoftware.com/chatbots-o</span><span class=\"invisible\">pen-data-project/</span></a></p><p><a href=\"https://fediscience.org/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://fediscience.org/tags/csv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>csv</span></a> <a href=\"https://fediscience.org/tags/OpenData\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenData</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@JordiCabot",
        "display_name": "Jordi Cabot"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12000v1",
    "title": "Deep Learning Approaches to Lexical Simplification: A Survey",
    "latest": "2023-05-23T07:10:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416744250292836",
      "content": "<p>\ud83d\udcdd Deep Learning Approaches to Lexical Simplification: A Survey \ud83d\udcda</p><p>\"LS is the lexical component of Text Simplification (TS) with the aim of making texts more accessible to various target populations such as children, people with language and learning disorders, foreigners or people with low literacy.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12000v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12000v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2109.06096",
    "title": "https://arxiv.org/abs/2109.06096",
    "latest": "2023-05-23T06:38:47+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416620928970772",
      "content": "<p>As always a small model behaves just like an undertrained model<br><a href=\"https://twitter.com/LChoshen/status/1506245430912430091\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/LChoshen/status/15</span><span class=\"invisible\">06245430912430091</span></a><br><a href=\"https://arxiv.org/abs/2109.06096\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2109.06096</span><span class=\"invisible\"></span></a><br>So you can also pick an undertrained model, which would be better than a trained model in detecting who generated the text.</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2301.11305",
    "title": "https://arxiv.org/abs/2301.11305",
    "latest": "2023-05-23T06:33:49+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416601360081415",
      "content": "<p>First the problem, given a text you want to know whether a human wrote it. You've been in NLP lately I am sure a teacher, sister, nephew etc. called and told you they suspect someone handed them a GPT text.<br>Problem: how can you tell<br>The approach<br>Randomly replace words<br>Then see how much it changed the sentence probability\\likelihood</p><p>presented by <br><a href=\"https://arxiv.org/abs/2301.11305\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2301.11305</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09859",
    "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
    "latest": "2023-05-23T06:33:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416598590927915",
      "content": "<p>Opposite scaling law: detection of machine-generated text is done better by smaller models</p><p>Everyone (outside <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a>...) is afraid GPT would cheat for them, which pushes for detection methods</p><p><a href=\"https://arxiv.org/abs/2305.09859\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09859</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <a href=\"https://sigmoid.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11993v1",
    "title": "https://arxiv.org/abs/2305.11993v1",
    "latest": "2023-05-23T06:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416586994729304",
      "content": "<p>\ud83d\udcdd Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis \ud83d\udcda</p><p>\"A specialised Flan-T5 language model is trained to take usage examples and a usage cluster as input and output a natural language definition of that usage cluster as its output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ltgoslo/definition_modeling\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/ltgoslo/definition_</span><span class=\"invisible\">modeling</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11993v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11993v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11979v1",
    "title": "https://arxiv.org/abs/2305.11979v1",
    "latest": "2023-05-23T05:30:12+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416351236599275",
      "content": "<p>\ud83d\udcdd A Weak Supervision Approach for Few-Shot Aspect Based Sentiment \ud83d\udcda</p><p>\"Uses weak supervision on unlabeled data using the output of a simple baseline, and then fine-tune a pre-trained sequence-to-sequence model on the generated dataset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/robertvacareanu/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/robertvacareanu/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11979v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11979v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.10559",
    "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
    "latest": "2023-05-23T05:19:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416307592130037",
      "content": "<p>We also have seen claims ICL is quite similar to taking a gradient step<br>arxiv.org/abs/2211.15661<br><a href=\"https://arxiv.org/abs/2212.10559\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2212.10559</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2202.12837",
    "title": "http://arxiv.org/abs/2202.12837",
    "latest": "2023-05-23T05:18:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416304404784912",
      "content": "<p>So we have seen papers showing that models gain a lot from seeing examples (ICL) with random labels<br><a href=\"http://arxiv.org/abs/2202.12837\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2202.12837</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09731",
    "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
    "latest": "2023-05-23T05:17:36+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416301704135043",
      "content": "<p>In-Context-Learning == gradient descent or disregards labels completely?!<br>Why not both?</p><p>Models recognize the task but also learn it<br>&amp; The benefits of actual learning grow with # examples and model size</p><p> <br><a href=\"https://arxiv.org/abs/2305.09731\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09731</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11952v1",
    "title": "Self-QA: Unsupervised Knowledge Guided Language Model Alignment",
    "latest": "2023-05-23T05:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416233204460998",
      "content": "<p>\ud83d\udcdd Self-Qa: Unsupervised Knowledge Guided Language Model Alignment \ud83d\udcda</p><p>\"Self-QA replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge, enabling the model to generate a larger quantity of correct and domain-specific instruction data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11952v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11952v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11938v1",
    "title": "https://arxiv.org/abs/2305.11938v1",
    "latest": "2023-05-23T04:40:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416154477268328",
      "content": "<p>\ud83d\udcdd XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages \ud83d\udcda</p><p>\"XTREME-UP is a benchmark defined by three criteria: its focus on the scarce-data scenario rather than zero-shot (rather than few-shot); its focus on user-centric tasks rather than linguistic probing tasks; and its focus on under-represented languages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/google-research/xtreme-up\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/google-research/xtr</span><span class=\"invisible\">eme-up</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11938v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11938v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.02819",
    "title": "GPT detectors are biased against non-native English writers",
    "latest": "2023-05-23T04:32:53+00:00",
    "last_post": {
      "url": "https://river.group.lt/@rgb/110408082519926313",
      "content": "<p><a href=\"https://arxiv.org/abs/2304.02819\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.02819</span><span class=\"invisible\"></span></a><br>GPT detectors are biased against non-native English writers</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11916v1",
    "title": "F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks",
    "latest": "2023-05-23T04:20:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416075706791889",
      "content": "<p>\ud83d\udcdd F-Pabee: Flexible-Patience-Based Early Exiting for Single-Label and Multi-Label Text Classification Tasks \ud83d\udcda</p><p>\"A Flexible-Patience-Based Early Exiting Method (F-PABEE) has been proposed to alleviate the problems mentioned above for single-label classification (SLC) and multi-label classification (MLC) tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11916v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11916v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11862v1",
    "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models",
    "latest": "2023-05-23T03:42:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415926410383122",
      "content": "<p>\ud83d\udcdd Reducing Sequence Length by Predicting Edit Operations with Large Language Models \ud83d\udcda</p><p>\"A local sequence transduction task is represented as a sequence of edit operations on the input text, which is learned by instruction tuning for large language models (LLMs).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11862v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11862v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11826v1",
    "title": "STOAT: Structured Data to Analytical Text With Controls",
    "latest": "2023-05-23T03:22:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415847795639417",
      "content": "<p>\ud83d\udcdd STOAT: Structured Data to Analytical Text with Controls \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a reasoning category aware table-to-text generation model with vector quantization to infuse the reasoning category in the output descriptions and also to control the reasoning category generation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11826v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11826v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11626v1",
    "title": "CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search",
    "latest": "2023-05-23T02:12:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415572481058584",
      "content": "<p>\ud83d\udcdd CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search \ud83d\udcda</p><p>\"A combination of GraphCodeBERT pre-training and cross-consistency training of a language model across different programming languages using XCD dataset as supervision signal.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11626v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11626v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11625v1",
    "title": "Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets",
    "latest": "2023-05-23T02:02:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415533142570361",
      "content": "<p>\ud83d\udcdd Searching by Code: A New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets \ud83d\udcda</p><p>\"SnippeR uses a single encoder for both the code snippet and the natural language query, and learns to rank the candidate answer by the cosine similarity of the two.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11625v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11625v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11744v1",
    "title": "Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval",
    "latest": "2023-05-23T01:52:58+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_ir/110415139983596635",
      "content": "<p>\ud83d\udcdd Inference-Time Re-Ranker Relevance Feedback for Neural Information Retrieval \ud83d\udcbf\ud83d\udcda</p><p>\"We update the retriever's query vector using a lightweight inference-time distillation of the re-ranker's prediction for that instance and perform a second retrieval with the updated query vector.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/IR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>IR</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11744v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11744v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://recsys.social/@karlhigley",
        "display_name": "Karl Higley"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11596v1",
    "title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation",
    "latest": "2023-05-23T01:32:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415415119788630",
      "content": "<p>\ud83d\udcdd Mitigating Backdoor Poisoning Attacks Through the Lens of Spurious Correlation \ud83d\udcda</p><p>\"Works by filtering out instances with potentially problematic correlations between features and labels, and using a subset of the rest to retrain the models and remove backdoor behaviours.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CR</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11596v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11596v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11553v1",
    "title": "https://arxiv.org/abs/2305.11553v1",
    "latest": "2023-05-23T01:12:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415336656025295",
      "content": "<p>\ud83d\udcdd Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information \ud83d\udcda</p><p>\"Considers each abstract as a recurrent cycle of sentences and place segmentation boundaries by greedily optimizing the NMI score between premises and conclusions in each cycle of sentences.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mediacloud/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mediacloud/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11553v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11553v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13304",
    "title": "https://arxiv.org/abs/2305.13304",
    "latest": "2023-05-23T01:12:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415336394152761",
      "content": "<p>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text</p><p>Generates a paragraph at each timestep and updates its language-based long-short term memory stored on the hard drive and the prompt, respectively. </p><p>repo: <a href=\"https://github.com/aiwaves-cn/RecurrentGPT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/aiwaves-cn/Recurren</span><span class=\"invisible\">tGPT</span></a> <br>abs: <a href=\"https://arxiv.org/abs/2305.13304\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13304</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13292",
    "title": "VideoLLM: Modeling Video Sequence with Large Language Models",
    "latest": "2023-05-23T01:03:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415301110279075",
      "content": "<p>VideoLLM: Modeling Video Sequence with Large Language Models</p><p>Leverages the sequence reasoning capabilities of pre-trained LLMs from NLP for video sequence understanding.</p><p><a href=\"https://arxiv.org/abs/2305.13292\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13292</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13301",
    "title": "Training Diffusion Models with Reinforcement Learning",
    "latest": "2023-05-23T01:00:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415289384934263",
      "content": "<p>Training Diffusion Models with Reinforcement Learning</p><p>Presents an RL-based framework for training denoising diffusion models to directly optimize a variety of reward functions</p><p><a href=\"https://arxiv.org/abs/2305.13301\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13301</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2303.12712.pdf",
    "title": "https://arxiv.org/pdf/2303.12712.pdf",
    "latest": "2023-05-23T00:42:46+00:00",
    "last_post": {
      "url": "https://techhub.social/@freemanwd/110415146468249466",
      "content": "<p>Microsoft researchers wonder if GPT4 is a form of AGI based on several experiments <br><a href=\"https://arxiv.org/pdf/2303.12712.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2303.12712.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@andrey",
        "display_name": "Andrey Kurenkov"
      },
      {
        "url": "https://fosstodon.org/@amueller",
        "display_name": "Andreas Mueller"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://genart.social/@tca",
        "display_name": "tiago"
      },
      {
        "url": "https://mastodon.social/@bruces",
        "display_name": "Bruce Sterling @bruces"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11449v1",
    "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast",
    "latest": "2023-05-23T00:32:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415179239334587",
      "content": "<p>\ud83d\udcdd Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-Tuning Slow and Fast \ud83d\udcda</p><p>\"Proposes a method named Fine-tuning slow and fast with four training policies to improve the performance gap for multilingual tasks, by reducing forgetting and improving the fine-tuning process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11449v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11449v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11251v1",
    "title": "Computational thematics: Comparing algorithms for clustering the genres of literary fiction",
    "latest": "2023-05-23T00:12:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415100611809059",
      "content": "<p>\ud83d\udcdd Computational Thematics: Comparing Algorithms for Clustering the Genres of Literary Fiction \ud83d\udcda</p><p>\"S are applied to a corpus of books belonging to four pre-tagged genres of fiction (Fantasy, Horror, Science Fiction, and Romance), and are then validated against the \"ground truth\" genre labels.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11251v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11251v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11243v1",
    "title": "Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses",
    "latest": "2023-05-23T00:02:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415061229184234",
      "content": "<p>\ud83d\udcdd Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses \ud83d\udcda\ud83d\udc7e</p><p>\"LaMDA is a large language model (LM), a type of AModels trained to predict the next word given a sequence of words, that was trained on a large number of webpages and books.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11243v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11243v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11231v1",
    "title": "Recent Trends in Unsupervised Summarization",
    "latest": "2023-05-22T23:22:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414903863283183",
      "content": "<p>\ud83d\udcdd Recent Trends in Unsupervised Summarization \ud83d\udcda</p><p>\"Unsupervised approaches learn to summarize from the input data and do not need labeled datasets for training, unlike supervised approaches that require labeled datasets for training the models to summarize the content.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11231v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11231v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2302.12173",
    "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
    "latest": "2023-05-22T23:05:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110414836730125554",
      "content": "<p>Compromising LLM-Integrated Applications with Indirect Prompt Injection - <a href=\"https://arxiv.org/abs/2302.12173\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2302.12173</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@raphaelmilliere",
        "display_name": "Rapha\u00ebl Milli\u00e8re"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206v1",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T23:02:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414825637860258",
      "content": "<p>\ud83d\udcdd LIMA: Less Is More for Alignment \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"LIMA is a 65B parameter language model trained for 150B tokens, using a supervised loss, from 1,000 carefully curated prompts and their responses.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11206v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11186v1",
    "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
    "latest": "2023-05-22T22:52:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414785973568218",
      "content": "<p>\ud83d\udcdd Compress, Then Prompt: Improving Accuracy-Efficiency Trade-Off of LLM Inference with Transferable Prompt \ud83d\udcda\ud83e\udde0</p><p>\"A prompt learning paradigm that cultivates an additive prompt over a compressed LLM to bolster their accuracy is presented and empirically tested in this work, shedding light on new possibilities for enhancing the balance between accuracy and efficiency in LLM inference.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11186v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11186v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11719v1",
    "title": "https://arxiv.org/abs/2305.11719v1",
    "latest": "2023-05-22T21:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414463541864356",
      "content": "<p>\ud83d\udcdd Information Screening Whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling \ud83d\udd2d\ud83d\udcda</p><p>\"Represents the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ChocoWu/MRE-ISE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/ChocoWu/MRE-ISE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11719v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11719v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/74djc/",
    "title": "http://osf.io/74djc/",
    "latest": "2023-05-22T21:04:05+00:00",
    "last_post": {
      "url": "https://botsin.space/@EdArXivBot/110395685386849135",
      "content": "<p>Unlocking Financial Success: Empowering Higher Ed Students and Developing Financial Literacy Interventions at Scale <a href=\"http://osf.io/74djc/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/74djc/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@dougholton",
        "display_name": "Doug Holton"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11845v1",
    "title": "https://arxiv.org/abs/2305.11845v1",
    "latest": "2023-05-22T20:18:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414180509844595",
      "content": "<p>\ud83d\udcdd RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"RxnScribe is an end-to-end model that takes as input a chemical reaction diagram and outputs a structured representation in a machine-readable format.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/thomas0809/RxnScribe\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/thomas0809/RxnScrib</span><span class=\"invisible\">e</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11845v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11845v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11806v1",
    "title": "https://arxiv.org/abs/2305.11806v1",
    "latest": "2023-05-22T19:54:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414086009856181",
      "content": "<p>\ud83d\udcdd The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics \ud83d\udcda</p><p>\"Develops and compare several explanation methods and demonstrate their effectiveness for interpreting state-of-the-art neural metrics based on BERT and ROBERTA fine-tuned on sentence-level human judgments from the WMT Metrics Shared Task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Unbabel/COMET\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Unbabel/COMET</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11806v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11806v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11790v1",
    "title": "Prompting with Pseudo-Code Instructions",
    "latest": "2023-05-22T19:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413944317609304",
      "content": "<p>\ud83d\udcdd Prompting with Pseudo-Code Instructions \ud83d\udcda</p><p>\"A pre-trained language model is fine-tuned using a prompt in the form of pseudocode instructions for a task in addition to the natural language instruction for the task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11790v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11790v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11789v1",
    "title": "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach",
    "latest": "2023-05-22T19:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413897105704111",
      "content": "<p>\ud83d\udcdd Solving NLP Problems Through Human-System Collaboration: A Discussion-Based Approach \ud83d\udcda</p><p>\"Creates a dataset with a human-in-the-loop setup where an annotator can discuss a natural language inference task with a chatbot and receive feedback.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11789v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11789v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41593-023-01333-4",
    "title": "Addressing the ethical and societal challenges posed by genome-wide association studies of behavioral and brain-related traits - Nature Neuroscience",
    "latest": "2023-05-22T18:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413833974408684",
      "content": "<p>Ethical, societal implications of genome-wide behavioral, brain-related traits - <a href=\"https://www.nature.com/articles/s41593-023-01333-4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41593-023</span><span class=\"invisible\">-01333-4</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778v1",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T18:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413802737748919",
      "content": "<p>\ud83d\udcdd Cross-Lingual Supervision Improves Large Language Models Pre-Training \ud83d\udcda\ud83e\udde0</p><p>\"A pre-training objective mixing a self-supervised language modeling and a supervised machine translation objectives, including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11778v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11761v1",
    "title": "ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation",
    "latest": "2023-05-22T18:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413755777159926",
      "content": "<p>\ud83d\udcdd ReSeTOX: Re-Learning Attention Weights for Toxicity Mitigation in Machine Translation \ud83d\udcda</p><p>\"Works by dynamically adjusting the key-value self-attention weights and re-evaluates the beam search hypotheses in case of identified added toxicity during the inference process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11761v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11761v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/u26ze/",
    "title": "http://osf.io/u26ze/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645261557408",
      "content": "<p>Gender and retention patterns among U.S. faculty <a href=\"http://osf.io/u26ze/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u26ze/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/u5kd2/",
    "title": "http://osf.io/u5kd2/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645213145496",
      "content": "<p>Power users: Canadian sex workers' use of technology post COVID <a href=\"http://osf.io/u5kd2/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u5kd2/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/7s9g4/",
    "title": "http://osf.io/7s9g4/",
    "latest": "2023-05-22T18:02:01+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645169835083",
      "content": "<p>Intelligent transport design with a dual focus <a href=\"http://osf.io/7s9g4/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/7s9g4/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/pqzux/",
    "title": "http://osf.io/pqzux/",
    "latest": "2023-05-22T17:56:07+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413621983917418",
      "content": "<p>Lithic raw materials provenance in the Caribbean islands: flint, jasper, obsidian and so on. <a href=\"http://osf.io/pqzux/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/pqzux/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11062",
    "title": "Scaling Transformer to 1M tokens and beyond with RMT",
    "latest": "2023-05-22T17:53:06+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@mikeiavelli/110254216906691830",
      "content": "<p>Recurrent Memory Transformer (RMT) retains information across up to 2 million tokens (!) \u2b50, significantly exceeding the largest input size reported for transformer models (64K tokens) and GPT-4's 32K tokens.</p><p>Paper: <a href=\"https://arxiv.org/abs/2304.11062\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11062</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/booydar/t5-experiments/tree/scaling-report\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/booydar/t5-experime</span><span class=\"invisible\">nts/tree/scaling-report</span></a></p><p><a href=\"https://mathstodon.xyz/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> <a href=\"https://mathstodon.xyz/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <br><a href=\"https://mathstodon.xyz/tags/NeuralNetworks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuralNetworks</span></a> <a href=\"https://mathstodon.xyz/tags/NN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NN</span></a><br><a href=\"https://mathstodon.xyz/tags/DeepLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DeepLearning</span></a><br><a href=\"https://mathstodon.xyz/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mathstodon.xyz/tags/Transformers\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Transformers</span></a> <a href=\"https://mathstodon.xyz/tags/RNN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>RNN</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://mastodon.online/@vladiliescu",
        "display_name": "Vlad Iliescu \ud83d\udc2c"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01702-w",
    "title": "When will global warming actually hit the landmark 1.5 \u00baC limit?",
    "latest": "2023-05-22T17:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413598086407934",
      "content": "<p>When will global warming hit the landmark 1.5 \u00baC limit? - <a href=\"https://www.nature.com/articles/d41586-023-01702-w\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01702-w</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11554",
    "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
    "latest": "2023-05-22T17:23:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110413493153748217",
      "content": "<p>RT @Ber18791531@twitter.com</p><p>\ud83e\udd14ChatGPT plug-in is impressive, but is \"learning tools in the context\" the ultimate solution?<br>Check out our ToolkenGPT (<a href=\"https://arxiv.org/abs/2305.11554\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11554</span><span class=\"invisible\"></span></a>), which can handle massive tools and understand them better with new \"toolken\" embeddings.</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Ber18791531/status/1660520584382672897\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Ber18791531/status</span><span class=\"invisible\">/1660520584382672897</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11595v1",
    "title": "Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate",
    "latest": "2023-05-22T17:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413425257031910",
      "content": "<p>\ud83d\udcdd Diving Into the Inter-Consistency of Large Language Models: An Insightful Analysis Through Debate \ud83d\udcda\ud83d\udc7e</p><p>\"Designs a formal debate framework with three-stage debate including the fair debate, mismatched debate, and roundtable debate stages for inter-consistency learning between LLMs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11595v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11595v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11579v1",
    "title": "https://arxiv.org/abs/2305.11579v1",
    "latest": "2023-05-22T16:42:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413331017830359",
      "content": "<p>\ud83d\udcdd Speech-Text Dialog Pre-Training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment \ud83d\udcda\ud83d\udc7e</p><p>\"First pre-trains on speech-text dialogs via response selection task and temporal position prediction task, and then fine-tunes the pre-trained model on downstream STD tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/AlibabaResearch/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/AlibabaResearch/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11579v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11579v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05335",
    "title": "https://doi.org/10.21105/joss.05335",
    "latest": "2023-05-22T16:30:36+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110413285723110427",
      "content": "<p>Just published in JOSS: 'chombo-discharge: An AMR code for gas discharge simulations in complex geometries' <a href=\"https://doi.org/10.21105/joss.05335\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05335</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.jtrangeo.2023.103590",
    "title": "doi.org/10.1016/j.jtrangeo.202...",
    "latest": "2023-05-22T16:19:41+00:00",
    "last_post": {
      "url": "https://datasci.social/@UrbanDemog/110413146017535927",
      "content": "<p>New paper \"Evaluating the impact of public transport travel time inaccuracy and variability on socio-spatial inequalities in accessibility\", out in the Journal of Transport Geography. A great study led by Kaue Braga.<br>Paper: <a href=\"https://doi.org/10.1016/j.jtrangeo.2023.103590\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.jtrangeo.202</span><span class=\"invisible\">3.103590</span></a><br>\ud83d\udd13ungated PDF: <a href=\"https://www.urbandemographics.org/publication/2023_jtg_public_transport_travel_time_inaccuracy_variability/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">urbandemographics.org/publicat</span><span class=\"invisible\">ion/2023_jtg_public_transport_travel_time_inaccuracy_variability/</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11564v1",
    "title": "https://arxiv.org/abs/2305.11564v1",
    "latest": "2023-05-22T16:06:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413189526338211",
      "content": "<p>\ud83d\udcdd Decouple Knowledge From Paramters for Plug-and-Play Language Modeling \ud83d\udcda\ud83d\udc7e</p><p>\"The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Hannibal046/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Hannibal046/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11564v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11564v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11438v1",
    "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
    "latest": "2023-05-22T09:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411679334789432",
      "content": "<p>\ud83d\udcdd Phonetic and Prosody-Aware Self-Supervised Learning Approach for Non-Native Fluency Scoring \ud83d\udcda</p><p>\"A self-supervised approach for automatic fluency scoring is proposed that takes into account the phonetic and prosody awareness during both pre-training and fine-tuning stages to improve the performance of the model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11438v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11438v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11426v1",
    "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
    "latest": "2023-05-22T09:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411584964283083",
      "content": "<p>\ud83d\udcdd Post Hoc Explanations of Language Models Can Improve Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"AMPLIFY constructs natural language rationales from post hoc explanations to provide corrective signals to LLMs during in-context learning, resulting in prediction accuracy improvements of about 10-25% over a wide range of tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11426v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11426v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://osf.io/preprints/socarxiv/a7udw/",
    "title": "osf.io/preprints/socarxiv/a7ud...",
    "latest": "2023-05-22T09:02:43+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@rockberta/110411524554763096",
      "content": "<p>Hey there! New preprint where we use contextualized topic models, automated linguistic feature extraction and predictive modeling to look at how online communication by the European Commission has changed over time \ud83d\udcc3\ud83e\udd16\ud83d\udcc8</p><p>TL;DR: we show that EC has radically reshaped its projected online identity, moving towards less technocratic topics &amp; style, defining a more \"unique\" profile among comparable institutions &amp; aligning better with its audience.</p><p><a href=\"https://osf.io/preprints/socarxiv/a7udw/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">osf.io/preprints/socarxiv/a7ud</span><span class=\"invisible\">w/</span></a> <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/polsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>polsci</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@rockberta",
        "display_name": "Roberta Rocca"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.09848",
    "title": "Evaluating Verifiability in Generative Search Engines",
    "latest": "2023-05-22T08:52:24+00:00",
    "last_post": {
      "url": "https://qoto.org/@tedherman/110250616501896621",
      "content": "<p><span class=\"h-card\"><a href=\"https://fediscience.org/@muratdemirbas\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>muratdemirbas</span></a></span> This paper discusses the problem of verifying results of generative transformers - <a href=\"https://arxiv.org/abs/2304.09848\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.09848</span><span class=\"invisible\"></span></a> </p><p>My question: wasn't blockchain supposed to solve all the problems of verifiability?</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@Yarin",
        "display_name": "Yarin :verified: :verified:"
      },
      {
        "url": "https://sigmoid.social/@Riedl",
        "display_name": "Mark Riedl"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://aleph.land/@mtriclot",
        "display_name": "Mathieu Triclot"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11374v1",
    "title": "https://arxiv.org/abs/2305.11374v1",
    "latest": "2023-05-22T08:18:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411349164932293",
      "content": "<p>\ud83d\udcdd Characterizing Tradeoffs Between Teaching via Language and Demonstrations in Multi-Agent Systems \ud83d\udcda</p><p>\"Finds that teaching by demonstration is more effective in the simplest settings, but language is more effective as task difficulty increases, due to its ability to generalize more effectively to unseen scenarios.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/dharakyu/language-and-demos\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/dharakyu/language-a</span><span class=\"invisible\">nd-demos</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11374v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11374v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11364v1",
    "title": "https://arxiv.org/abs/2305.11364v1",
    "latest": "2023-05-22T08:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411301875994659",
      "content": "<p>\ud83d\udcdd Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"Presents LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets, which clusters text along syntactic, lexical, and semantic axes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/PAIR-code/interpretability\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/PAIR-code/interpret</span><span class=\"invisible\">ability</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11364v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11364v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11355v1",
    "title": "https://arxiv.org/abs/2305.11355v1",
    "latest": "2023-05-22T07:54:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411254744145126",
      "content": "<p>\ud83d\udcdd MD3: The Multi-Dialect Dataset of Dialogues \ud83d\udcda</p><p>\"A dataset of conversational speech from three major English-speaking countries: India, Nigeria, and the United States, with more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/nehasinha/Taboo/blob/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/nehasinha/Taboo/blo</span><span class=\"invisible\">b/</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11355v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11355v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09137",
    "title": "https://arxiv.org/abs/2305.09137",
    "latest": "2023-05-22T07:42:47+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381332490398862",
      "content": "<p>Pre-Training to Learn in Context</p><p>Proposes PICL, a framework to enhance the LMs' in-context learning ability by pre-training on \"intrinsic tasks\" in the general plain-text corpus using the simple LM objective.</p><p>repo: <a href=\"https://github.com/thu-coai/PICL\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/thu-coai/PICL</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.09137\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09137</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11334v1",
    "title": "Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs",
    "latest": "2023-05-22T07:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411113215008062",
      "content": "<p>\ud83d\udcdd Writing Your Own Book: A Method for Going From Closed to Open Book QA to Improve Robustness and Performance of Smaller LLMs \ud83d\udcda\ud83d\udc7e</p><p>\"Introduces two novel methods, Tree-Search and Self-contextualizing Question-Answering, designed to enhance the performance of large language models (LLMs) in question-answering tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11334v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11334v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11326v1",
    "title": "Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data",
    "latest": "2023-05-22T06:42:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410971702391662",
      "content": "<p>\ud83d\udcdd Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data \ud83d\udcda</p><p>\"Proposes a method to generate a chatbot from tabular data sources using natural language processing (NLP) and machine learning (ML) techniques, and to deploy it to offer citizens interactive access to public data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11326v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11326v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11317v1",
    "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
    "latest": "2023-05-22T06:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410924603280206",
      "content": "<p>\ud83d\udcdd Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation \ud83d\udcda</p><p>\"A large-scale language model that was trained on an open-domain corpus was used to edit the input prompt to improve the quality and consistency of image outputs by the state-of-the-art Text-to-Image (T2I) models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11317v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11317v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11315v1",
    "title": "https://arxiv.org/abs/2305.11315v1",
    "latest": "2023-05-22T05:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410735679563419",
      "content": "<p>\ud83d\udcdd Improving Toponym Resolution with Better Candidate Generation, Transformer-Based Reranking, and Two-Stage Resolution \ud83d\udcda</p><p>\"GeoNorm first uses information retrieval techniques to generate a list of candidate entries from the geospatial ontology, then reranks the candidate entries using a transformer-based neural network that incorporates information from the ontology such as the entry's population.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clulab/geonorm\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/clulab/geonorm</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11315v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11315v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125",
    "title": "Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity",
    "latest": "2023-05-22T05:27:17+00:00",
    "last_post": {
      "url": "https://mastodon.social/@devezer/110373663709370598",
      "content": "<p>And while epistemic pluralism may not be perfect or guarantee immediate progress, it should protect us from far worse excesses. Incidentally in our 2019 paper our computational model of  scientific process suggests that epistemic diversity is powerful in that it prevents worst-case scenarios and allows for scientific discovery while avoiding many traps. <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0216125</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@FroehlichMarcel",
        "display_name": "Marcel Fr\u00f6hlich"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11255v1",
    "title": "https://arxiv.org/abs/2305.11255v1",
    "latest": "2023-05-22T05:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410641360816775",
      "content": "<p>\ud83d\udcdd Reasoning Implicit Sentiment with Chain-of-Thought Prompting \ud83d\udcda</p><p>\"A three-step prompting principle is designed for THOR to guide the model to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/scofield7419/THOR-ISA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/scofield7419/THOR-I</span><span class=\"invisible\">SA</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11255v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11255v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T04:26:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110410436966499091",
      "content": "<p>Cross-Lingual Supervision improves Large Language Models Pre-training</p><p>Pretraining LLMs on a mixture of a regular LM dataset + cross-lingual parallel data yields models with better in-context learning abilities. </p><p><a href=\"https://arxiv.org/abs/2305.11778\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09612",
    "title": "Large Language Models are Built-in Autoregressive Search Engines",
    "latest": "2023-05-22T03:42:39+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381297041837722",
      "content": "<p>Large Language Models are Built-in Autoregressive Search Engines</p><p>When providing a few Query-URL pairs as in-context demonstrations, LLMs can generate Web URLs where ~90% of the corresponding documents contain correct answers to open-domain questions.</p><p><a href=\"https://arxiv.org/abs/2305.09612\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09612</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2102.07350.pdf",
    "title": "https://arxiv.org/pdf/2102.07350.pdf",
    "latest": "2023-05-22T02:49:21+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110410056412505522",
      "content": "<p>also see \"Prompt Programming for Large Language Models:<br>Beyond the Few-Shot Paradigm\"</p><p> Informed by this more encompassing theory of<br>prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its<br>own natural language prompts for a range of tasks</p><p><a href=\"https://arxiv.org/pdf/2102.07350.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2102.07350.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mastodon.radio/tags/prompts\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>prompts</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.11430.pdf",
    "title": "https://arxiv.org/pdf/2305.11430.pdf",
    "latest": "2023-05-22T02:22:01+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110409948972775616",
      "content": "<p>\"TELeR: A General Taxonomy of LLM Prompts for Benchmarking<br>Complex Tasks\"</p><p><a href=\"https://arxiv.org/pdf/2305.11430.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.11430.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.00118",
    "title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",
    "latest": "2023-05-22T01:44:35+00:00",
    "last_post": {
      "url": "https://paquita.masto.host/@no_tan_incendiario/110343931803514302",
      "content": "<p>Han descubierto que ChatGPT ha \"\"memorizado\"\" m\u00e1s de 100 libros. A ver cu\u00e1ndo las editoriales afilan sus cuchillos. <a href=\"https://arxiv.org/abs/2305.00118\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.00118</span><span class=\"invisible\"></span></a> La lista de libros: <a href=\"https://docs.google.com/spreadsheets/d/1jW7EhsNjIGDMoK2JidyDD7UXH9N0NpEJfWFEj05_LC4/edit#gid=0\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">docs.google.com/spreadsheets/d</span><span class=\"invisible\">/1jW7EhsNjIGDMoK2JidyDD7UXH9N0NpEJfWFEj05_LC4/edit#gid=0</span></a> (fuente: <a href=\"https://github.com/bamman-group/gpt4-books\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/bamman-group/gpt4-b</span><span class=\"invisible\">ooks</span></a> )</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@dbamman",
        "display_name": "David Bamman"
      },
      {
        "url": "https://fediscience.org/@UlrichJunker",
        "display_name": "Ulrich Junker"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch (moderator)"
      },
      {
        "url": "https://mastodon.social/@jose_eduardo",
        "display_name": "Jos\u00e9 Eduardo Gonz\u00e1lez"
      },
      {
        "url": "https://sigmoid.social/@roban",
        "display_name": "Roban Hultman Kramer"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09515",
    "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
    "latest": "2023-05-22T01:42:35+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381269465767328",
      "content": "<p>AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation</p><p>On text summarization, NMT, and common sense generation, AR-Diffusion outperforms existing diffusion LMs and that it can be 100\u00d7 \u223c 600\u00d7 faster when achieving comparable results</p><p><a href=\"https://arxiv.org/abs/2305.09515\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09515</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T01:36:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409769364451473",
      "content": "<p>LIMA: Less Is More for Alignment</p><p>Presents LIMA, a 65B LLaMa language model fine-tuned on only 1k curated samples w/o RLHF. </p><p>Outputs from LIMA are either equivalent or strictly preferred to GPT-4 or Bard in 43% or 58% of cases, respectively.</p><p><a href=\"https://arxiv.org/abs/2305.11206\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01521-z",
    "title": "Humans and algorithms work together \u2014 so study them together",
    "latest": "2023-05-22T01:27:48+00:00",
    "last_post": {
      "url": "https://social.coop/@natematias/110350424967237041",
      "content": "<p>The US Supreme Court will soon decide if YouTube should be held responsible for allegedly permitting the platform\u2019s recommender algorithms to promote content that radicalized terrorists who killed Nohemi Gonzalez.</p><p>Whatever the court concludes, the case highlights an urgent question: how can we govern adaptive algorithms that continually change in response to people\u2019s behavior? The problem? Scientists can't currently answer this question.</p><p>See my new article in <span class=\"h-card\"><a href=\"https://sciencemastodon.com/@nature\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>nature</span></a></span> </p><p><a href=\"https://www.nature.com/articles/d41586-023-01521-z\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01521-z</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://social.coop/@natematias",
        "display_name": "J. Nathan Matias \ud83e\udda3"
      },
      {
        "url": "https://mstdn.social/@kissane",
        "display_name": "Erin Kissane"
      },
      {
        "url": "https://mastodon.social/@markcmarino",
        "display_name": "Mark C Marino"
      },
      {
        "url": "https://mastodon.social/@tiffanycli",
        "display_name": "Tiffany Li"
      },
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      },
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://fosstodon.org/@simon_goRin",
        "display_name": "Simon Gorin"
      },
      {
        "url": "https://hcommons.social/@julsraemy",
        "display_name": "Julien A. Raemy"
      },
      {
        "url": "https://social.coop/@dphiffer",
        "display_name": "Dan Phiffer"
      },
      {
        "url": "https://dair-community.social/@timnitGebru",
        "display_name": "Timnit Gebru (she/her)"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11738",
    "title": "https://arxiv.org/abs/2305.11738",
    "latest": "2023-05-22T01:24:23+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409722329971431",
      "content": "<p>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</p><p>repo: <a href=\"https://github.com/microsoft/ProphetNet/tree/master/CRITIC\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/ProphetNe</span><span class=\"invisible\">t/tree/master/CRITIC</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11738\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11738</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11675",
    "title": "https://arxiv.org/abs/2305.11675",
    "latest": "2023-05-22T01:18:22+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409698691968971",
      "content": "<p>Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity</p><p>proj: <a href=\"https://mind-video.com/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">mind-video.com/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11675\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11675</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11598",
    "title": "Introspective Tips: Large Language Model for In-Context Decision Making",
    "latest": "2023-05-22T00:53:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409600292246753",
      "content": "<p>Introspective Tips: Large Language Model for In-Context Decision Making</p><p><a href=\"https://arxiv.org/abs/2305.11598\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11598</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11841",
    "title": "How Does Generative Retrieval Scale to Millions of Passages?",
    "latest": "2023-05-22T00:40:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409549149527403",
      "content": "<p>How Does Generative Retrieval Scale to Millions of Passages?</p><p>Finds that the use of synthetic queries as a document representation strategy is the only approach that remained effective as they scaled up the corpus size using MS MARCO passages.</p><p><a href=\"https://arxiv.org/abs/2305.11841\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11841</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11846",
    "title": "https://arxiv.org/abs/2305.11846",
    "latest": "2023-05-22T00:37:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409537321914267",
      "content": "<p>Any-to-Any Generation via Composable Diffusion</p><p>Present CoDi, a novel generative model capable of generating any combination of output modalities from any combination of input modalities.</p><p>proj: <a href=\"https://codi-gen.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">codi-gen.github.io/</span><span class=\"invisible\"></span></a> <br>repo: <a href=\"https://github.com/microsoft/i-Code/tree/main/i-Code-V3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/i-Code/tr</span><span class=\"invisible\">ee/main/i-Code-V3</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11846\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11846</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11854",
    "title": "https://arxiv.org/abs/2305.11854",
    "latest": "2023-05-22T00:35:22+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409529572549613",
      "content": "<p>Multimodal Web Navigation with Instruction-Finetuned Foundation Models</p><p>Their 3B model outperforms the SoTA, PaLM-540B, on the WebShop benchmark. </p><p>Will make their 347K high-quality demonstrations publicly available.</p><p>proj: <a href=\"https://sites.google.com/view/mm-webnav/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">sites.google.com/view/mm-webna</span><span class=\"invisible\">v/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11854\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11854</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11863",
    "title": "Scaling laws for language encoding models in fMRI",
    "latest": "2023-05-22T00:30:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409509792875032",
      "content": "<p>Scaling laws for language encoding models in fMRI</p><p>Increasing scale in both models and data yields incredibly effective models of language processing in the brain, enabling better scientific understanding as well as applications such as decoding.</p><p><a href=\"https://arxiv.org/abs/2305.11863\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11863</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01612-x",
    "title": "For chemists, the AI revolution has yet to happen",
    "latest": "2023-05-21T23:45:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110409331655826124",
      "content": "<p>For chemists, the AI revolution has yet to happen - <a href=\"https://www.nature.com/articles/d41586-023-01612-x\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01612-x</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://osf.io/preprints/metaarxiv/fhdbs/",
    "title": "osf.io/preprints/metaarxiv/fhd...",
    "latest": "2023-05-21T23:39:40+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mcaleerp/110407615890521356",
      "content": "<p>Interesting paper in MetaArXiv: \u201cPreregistration in practice: A comparison of preregistered and non-preregistered studies in psychology\u201c. Main finding is whilst PreReg studies are more likely to state power analysis, inconsistent with previous findings, PreReg and non PreReg studies have similar amount of positive findings. Suggesting either PreReg doesn\u2019t reduce issues or issues are reduced in non PreReg studies these days. </p><p>Link: <a href=\"https://osf.io/preprints/metaarxiv/fhdbs/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">osf.io/preprints/metaarxiv/fhd</span><span class=\"invisible\">bs/</span></a></p><p><a href=\"https://mastodon.social/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a> <a href=\"https://mastodon.social/tags/MetaScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MetaScience</span></a> <a href=\"https://mastodon.social/tags/Psychology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Psychology</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.15056",
    "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks",
    "latest": "2023-05-21T22:23:10+00:00",
    "last_post": {
      "url": "https://qoto.org/@tedherman/110409008416499631",
      "content": "<p>Mechanical Turks considered in danger due to LLM/Chat tech. <a href=\"https://arxiv.org/abs/2303.15056\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.15056</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2205.09459",
    "title": "Neural Network Architecture Beyond Width and Depth",
    "latest": "2023-05-21T20:30:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110408564914038079",
      "content": "<p>Neural Network Architecture Beyond Width and Depth - <a href=\"https://arxiv.org/abs/2205.09459\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2205.09459</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s11229-023-04158-7",
    "title": "Epistemic diversity and industrial selection bias - Synthese",
    "latest": "2023-05-21T17:08:11+00:00",
    "last_post": {
      "url": "https://fediscience.org/@ct_bergstrom/110397512520315053",
      "content": "<p>Corporations can influence the directions and conclusions of academic research without corrupting the beliefs of any individual scientist. In a phenomenon known as industry selection bias, companies direct funding and/or data toward scientists who already support the research approaches or technological solutions favored by industry. </p><p>A new paper out in Synthese expands on the earlier Holman and Bruner model of this scenario to illustrate how this process works.</p><p><a href=\"https://link.springer.com/article/10.1007/s11229-023-04158-7\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s11229-023-04158-7</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://nerdculture.de/@dornhaus",
        "display_name": "Anna Dornhaus"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11162",
    "title": "High-Performance Graph Databases That Are Portable, Programmable, and Scale to Hundreds of Thousands of Cores",
    "latest": "2023-05-21T16:30:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110407621156510444",
      "content": "<p>High-Performance Graph Databases, Scaling to Hundreds of Thousands of Cores - <a href=\"https://arxiv.org/abs/2305.11162\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11162</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07759",
    "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
    "latest": "2023-05-21T16:23:02+00:00",
    "last_post": {
      "url": "https://mastodon.online/@jchyip/110386344853645107",
      "content": "<p><a href=\"https://mastodon.online/tags/TinyStories\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>TinyStories</span></a>: How Small Can Language Models Be and Still Speak Coherent English <a href=\"https://arxiv.org/abs/2305.07759\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07759</span><span class=\"invisible\"></span></a> <a href=\"https://mastodon.online/tags/ai\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ai</span></a> <a href=\"https://mastodon.online/tags/llm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>llm</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@bergi",
        "display_name": "Thomas Bergwinkl"
      }
    ]
  },
  {
    "link": "https://frontiersin.org/articles/10.3389/fpubh.2023.1221666/full",
    "title": "frontiersin.org/articles/10.33...",
    "latest": "2023-05-21T15:25:21+00:00",
    "last_post": {
      "url": "https://noc.social/@CaulfieldTim/110402373651558195",
      "content": "<p>Often referenced \"masks do harm\" study retracted.  </p><p>See: <a href=\"https://frontiersin.org/articles/10.3389/fpubh.2023.1221666/full\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">frontiersin.org/articles/10.33</span><span class=\"invisible\">89/fpubh.2023.1221666/full</span></a></p><p>Editors: \"...complaints were valid ... the article does not meet the standards of editorial &amp; scientific soundness...\"</p><p>Will likely live on as a zombie &amp; the retraction will become part of a conspiracy narrative.</p><p><a href=\"https://noc.social/tags/publichealth\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>publichealth</span></a> <a href=\"https://noc.social/tags/masks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>masks</span></a> <a href=\"https://noc.social/tags/scicomm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>scicomm</span></a> <a href=\"https://noc.social/tags/vaccineswork\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>vaccineswork</span></a> <a href=\"https://noc.social/tags/science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>science</span></a> <a href=\"https://noc.social/tags/retraction\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>retraction</span></a></p>"
    },
    "people": [
      {
        "url": "https://twit.social/@glennf",
        "display_name": "Glenn Fleishman"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41586-020-2487-2",
    "title": "Native American gene flow into Polynesia predating Easter Island settlement - Nature",
    "latest": "2023-05-21T15:17:03+00:00",
    "last_post": {
      "url": "https://social.sanfranciscan.org/objects/8971a192-bac6-4122-b369-24fca39e744f",
      "content": "<p>I\u2019ve always considered the sweet potato in Polynesia to be the \u201csmoking gun\u201d when it came to Pre-Columbian contact between the Americas and the \u201cOld World\u201d.</p><p>Now, genetic evidence appears to back up this theory. The article below concludes:</p><p>\u201c[We] find strong genetic evidence for pre-Columbian human trans-Pacific voyaging contact (at the turn of the twelfth century), contemporaneous with the Polynesian voyages of discovery in the remote eastern Pacific.\u201c</p><p><a href=\"https://www.nature.com/articles/s41586-020-2487-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">https://www.nature.com/articles/s41586-020-2487-2</a></p>"
    },
    "people": [
      {
        "url": "https://thefolklore.cafe/@juergen_hubert",
        "display_name": "J\u00fcrgen Hubert"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://vis.social/@mgiraldo",
        "display_name": "@mgiraldo"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s42254-023-00595-y",
    "title": "A new frontier for Hopfield networks - Nature Reviews Physics",
    "latest": "2023-05-21T14:52:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110402192452069793",
      "content": "<p>RT @DimaKrotov@twitter.com</p><p>Recent advances in Hopfield networks of associative memory may be the guiding theoretical principle for designing novel large scale neural architectures. I explain my enthusiasm about these ideas in the article \u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f. Please let me know what you think. <a href=\"https://www.nature.com/articles/s42254-023-00595-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s42254-023</span><span class=\"invisible\">-00595-y</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/DimaKrotov/status/1659526803155714053\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/DimaKrotov/status/</span><span class=\"invisible\">1659526803155714053</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21248/jlcl.36.2023.236",
    "title": "doi.org/10.21248/jlcl.36.2023....",
    "latest": "2023-05-21T14:44:49+00:00",
    "last_post": {
      "url": "https://mastodon.social/@fussballinguist/110406313988183627",
      "content": "<p>Our paper on \"Keyness in Song Lyrics\" is out: Jan Langenhorst, Yannick Frommherz and I discuss keyword analysis as an approach to computational stylistics by the example of song lyrics as highly clumpy data. We show that traditional bag-of-words-approached fail because of the repetitiveness of song lyrics and explore the advances of dispersion-oriented methods. <a href=\"https://mastodon.social/tags/openaccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>openaccess</span></a> <a href=\"https://doi.org/10.21248/jlcl.36.2023.236\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.21248/jlcl.36.2023.</span><span class=\"invisible\">236</span></a>.</p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@sascha_wolfer",
        "display_name": "Sascha Wolfer"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch"
      }
    ]
  }
]