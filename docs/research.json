[
  {
    "link": "https://arxiv.org/abs/2305.11719v1",
    "title": "https://arxiv.org/abs/2305.11719v1",
    "latest": "2023-05-22T21:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414463541864356",
      "content": "<p>\ud83d\udcdd Information Screening Whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling \ud83d\udd2d\ud83d\udcda</p><p>\"Represents the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ChocoWu/MRE-ISE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/ChocoWu/MRE-ISE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11719v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11719v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/74djc/",
    "title": "http://osf.io/74djc/",
    "latest": "2023-05-22T21:04:05+00:00",
    "last_post": {
      "url": "https://botsin.space/@EdArXivBot/110395685386849135",
      "content": "<p>Unlocking Financial Success: Empowering Higher Ed Students and Developing Financial Literacy Interventions at Scale <a href=\"http://osf.io/74djc/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/74djc/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@dougholton",
        "display_name": "Doug Holton"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11845v1",
    "title": "https://arxiv.org/abs/2305.11845v1",
    "latest": "2023-05-22T20:18:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414180509844595",
      "content": "<p>\ud83d\udcdd RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"RxnScribe is an end-to-end model that takes as input a chemical reaction diagram and outputs a structured representation in a machine-readable format.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/thomas0809/RxnScribe\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/thomas0809/RxnScrib</span><span class=\"invisible\">e</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11845v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11845v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11806v1",
    "title": "https://arxiv.org/abs/2305.11806v1",
    "latest": "2023-05-22T19:54:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414086009856181",
      "content": "<p>\ud83d\udcdd The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics \ud83d\udcda</p><p>\"Develops and compare several explanation methods and demonstrate their effectiveness for interpreting state-of-the-art neural metrics based on BERT and ROBERTA fine-tuned on sentence-level human judgments from the WMT Metrics Shared Task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Unbabel/COMET\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Unbabel/COMET</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11806v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11806v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11790v1",
    "title": "Prompting with Pseudo-Code Instructions",
    "latest": "2023-05-22T19:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413944317609304",
      "content": "<p>\ud83d\udcdd Prompting with Pseudo-Code Instructions \ud83d\udcda</p><p>\"A pre-trained language model is fine-tuned using a prompt in the form of pseudocode instructions for a task in addition to the natural language instruction for the task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11790v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11790v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11789v1",
    "title": "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach",
    "latest": "2023-05-22T19:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413897105704111",
      "content": "<p>\ud83d\udcdd Solving NLP Problems Through Human-System Collaboration: A Discussion-Based Approach \ud83d\udcda</p><p>\"Creates a dataset with a human-in-the-loop setup where an annotator can discuss a natural language inference task with a chatbot and receive feedback.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11789v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11789v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41593-023-01333-4",
    "title": "Addressing the ethical and societal challenges posed by genome-wide association studies of behavioral and brain-related traits - Nature Neuroscience",
    "latest": "2023-05-22T18:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413833974408684",
      "content": "<p>Ethical, societal implications of genome-wide behavioral, brain-related traits - <a href=\"https://www.nature.com/articles/s41593-023-01333-4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41593-023</span><span class=\"invisible\">-01333-4</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778v1",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T18:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413802737748919",
      "content": "<p>\ud83d\udcdd Cross-Lingual Supervision Improves Large Language Models Pre-Training \ud83d\udcda\ud83e\udde0</p><p>\"A pre-training objective mixing a self-supervised language modeling and a supervised machine translation objectives, including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11778v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11761v1",
    "title": "ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation",
    "latest": "2023-05-22T18:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413755777159926",
      "content": "<p>\ud83d\udcdd ReSeTOX: Re-Learning Attention Weights for Toxicity Mitigation in Machine Translation \ud83d\udcda</p><p>\"Works by dynamically adjusting the key-value self-attention weights and re-evaluates the beam search hypotheses in case of identified added toxicity during the inference process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11761v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11761v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/u26ze/",
    "title": "http://osf.io/u26ze/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645261557408",
      "content": "<p>Gender and retention patterns among U.S. faculty <a href=\"http://osf.io/u26ze/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u26ze/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/u5kd2/",
    "title": "http://osf.io/u5kd2/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645213145496",
      "content": "<p>Power users: Canadian sex workers' use of technology post COVID <a href=\"http://osf.io/u5kd2/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u5kd2/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/7s9g4/",
    "title": "http://osf.io/7s9g4/",
    "latest": "2023-05-22T18:02:01+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645169835083",
      "content": "<p>Intelligent transport design with a dual focus <a href=\"http://osf.io/7s9g4/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/7s9g4/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/pqzux/",
    "title": "http://osf.io/pqzux/",
    "latest": "2023-05-22T17:56:07+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413621983917418",
      "content": "<p>Lithic raw materials provenance in the Caribbean islands: flint, jasper, obsidian and so on. <a href=\"http://osf.io/pqzux/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/pqzux/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11062",
    "title": "Scaling Transformer to 1M tokens and beyond with RMT",
    "latest": "2023-05-22T17:53:06+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@mikeiavelli/110254216906691830",
      "content": "<p>Recurrent Memory Transformer (RMT) retains information across up to 2 million tokens (!) \u2b50, significantly exceeding the largest input size reported for transformer models (64K tokens) and GPT-4's 32K tokens.</p><p>Paper: <a href=\"https://arxiv.org/abs/2304.11062\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11062</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/booydar/t5-experiments/tree/scaling-report\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/booydar/t5-experime</span><span class=\"invisible\">nts/tree/scaling-report</span></a></p><p><a href=\"https://mathstodon.xyz/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> <a href=\"https://mathstodon.xyz/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <br><a href=\"https://mathstodon.xyz/tags/NeuralNetworks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuralNetworks</span></a> <a href=\"https://mathstodon.xyz/tags/NN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NN</span></a><br><a href=\"https://mathstodon.xyz/tags/DeepLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DeepLearning</span></a><br><a href=\"https://mathstodon.xyz/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mathstodon.xyz/tags/Transformers\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Transformers</span></a> <a href=\"https://mathstodon.xyz/tags/RNN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>RNN</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://mastodon.online/@vladiliescu",
        "display_name": "Vlad Iliescu \ud83d\udc2c"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01702-w",
    "title": "When will global warming actually hit the landmark 1.5 \u00baC limit?",
    "latest": "2023-05-22T17:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413598086407934",
      "content": "<p>When will global warming hit the landmark 1.5 \u00baC limit? - <a href=\"https://www.nature.com/articles/d41586-023-01702-w\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01702-w</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11554",
    "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
    "latest": "2023-05-22T17:23:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110413493153748217",
      "content": "<p>RT @Ber18791531@twitter.com</p><p>\ud83e\udd14ChatGPT plug-in is impressive, but is \"learning tools in the context\" the ultimate solution?<br>Check out our ToolkenGPT (<a href=\"https://arxiv.org/abs/2305.11554\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11554</span><span class=\"invisible\"></span></a>), which can handle massive tools and understand them better with new \"toolken\" embeddings.</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Ber18791531/status/1660520584382672897\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Ber18791531/status</span><span class=\"invisible\">/1660520584382672897</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11595v1",
    "title": "Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate",
    "latest": "2023-05-22T17:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413425257031910",
      "content": "<p>\ud83d\udcdd Diving Into the Inter-Consistency of Large Language Models: An Insightful Analysis Through Debate \ud83d\udcda\ud83d\udc7e</p><p>\"Designs a formal debate framework with three-stage debate including the fair debate, mismatched debate, and roundtable debate stages for inter-consistency learning between LLMs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11595v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11595v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11579v1",
    "title": "https://arxiv.org/abs/2305.11579v1",
    "latest": "2023-05-22T16:42:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413331017830359",
      "content": "<p>\ud83d\udcdd Speech-Text Dialog Pre-Training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment \ud83d\udcda\ud83d\udc7e</p><p>\"First pre-trains on speech-text dialogs via response selection task and temporal position prediction task, and then fine-tunes the pre-trained model on downstream STD tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/AlibabaResearch/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/AlibabaResearch/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11579v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11579v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05335",
    "title": "https://doi.org/10.21105/joss.05335",
    "latest": "2023-05-22T16:30:36+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110413285723110427",
      "content": "<p>Just published in JOSS: 'chombo-discharge: An AMR code for gas discharge simulations in complex geometries' <a href=\"https://doi.org/10.21105/joss.05335\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05335</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.jtrangeo.2023.103590",
    "title": "doi.org/10.1016/j.jtrangeo.202...",
    "latest": "2023-05-22T16:19:41+00:00",
    "last_post": {
      "url": "https://datasci.social/@UrbanDemog/110413146017535927",
      "content": "<p>New paper \"Evaluating the impact of public transport travel time inaccuracy and variability on socio-spatial inequalities in accessibility\", out in the Journal of Transport Geography. A great study led by Kaue Braga.<br>Paper: <a href=\"https://doi.org/10.1016/j.jtrangeo.2023.103590\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.jtrangeo.202</span><span class=\"invisible\">3.103590</span></a><br>\ud83d\udd13ungated PDF: <a href=\"https://www.urbandemographics.org/publication/2023_jtg_public_transport_travel_time_inaccuracy_variability/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">urbandemographics.org/publicat</span><span class=\"invisible\">ion/2023_jtg_public_transport_travel_time_inaccuracy_variability/</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11564v1",
    "title": "https://arxiv.org/abs/2305.11564v1",
    "latest": "2023-05-22T16:06:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413189526338211",
      "content": "<p>\ud83d\udcdd Decouple Knowledge From Paramters for Plug-and-Play Language Modeling \ud83d\udcda\ud83d\udc7e</p><p>\"The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Hannibal046/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Hannibal046/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11564v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11564v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11438v1",
    "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
    "latest": "2023-05-22T09:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411679334789432",
      "content": "<p>\ud83d\udcdd Phonetic and Prosody-Aware Self-Supervised Learning Approach for Non-Native Fluency Scoring \ud83d\udcda</p><p>\"A self-supervised approach for automatic fluency scoring is proposed that takes into account the phonetic and prosody awareness during both pre-training and fine-tuning stages to improve the performance of the model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11438v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11438v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11426v1",
    "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
    "latest": "2023-05-22T09:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411584964283083",
      "content": "<p>\ud83d\udcdd Post Hoc Explanations of Language Models Can Improve Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"AMPLIFY constructs natural language rationales from post hoc explanations to provide corrective signals to LLMs during in-context learning, resulting in prediction accuracy improvements of about 10-25% over a wide range of tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11426v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11426v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://osf.io/preprints/socarxiv/a7udw/",
    "title": "osf.io/preprints/socarxiv/a7ud...",
    "latest": "2023-05-22T09:02:43+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@rockberta/110411524554763096",
      "content": "<p>Hey there! New preprint where we use contextualized topic models, automated linguistic feature extraction and predictive modeling to look at how online communication by the European Commission has changed over time \ud83d\udcc3\ud83e\udd16\ud83d\udcc8</p><p>TL;DR: we show that EC has radically reshaped its projected online identity, moving towards less technocratic topics &amp; style, defining a more \"unique\" profile among comparable institutions &amp; aligning better with its audience.</p><p><a href=\"https://osf.io/preprints/socarxiv/a7udw/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">osf.io/preprints/socarxiv/a7ud</span><span class=\"invisible\">w/</span></a> <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/polsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>polsci</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@rockberta",
        "display_name": "Roberta Rocca"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.09848",
    "title": "Evaluating Verifiability in Generative Search Engines",
    "latest": "2023-05-22T08:52:24+00:00",
    "last_post": {
      "url": "https://qoto.org/@tedherman/110250616501896621",
      "content": "<p><span class=\"h-card\"><a href=\"https://fediscience.org/@muratdemirbas\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>muratdemirbas</span></a></span> This paper discusses the problem of verifying results of generative transformers - <a href=\"https://arxiv.org/abs/2304.09848\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.09848</span><span class=\"invisible\"></span></a> </p><p>My question: wasn't blockchain supposed to solve all the problems of verifiability?</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@Yarin",
        "display_name": "Yarin :verified: :verified:"
      },
      {
        "url": "https://sigmoid.social/@Riedl",
        "display_name": "Mark Riedl"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://aleph.land/@mtriclot",
        "display_name": "Mathieu Triclot"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11374v1",
    "title": "https://arxiv.org/abs/2305.11374v1",
    "latest": "2023-05-22T08:18:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411349164932293",
      "content": "<p>\ud83d\udcdd Characterizing Tradeoffs Between Teaching via Language and Demonstrations in Multi-Agent Systems \ud83d\udcda</p><p>\"Finds that teaching by demonstration is more effective in the simplest settings, but language is more effective as task difficulty increases, due to its ability to generalize more effectively to unseen scenarios.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/dharakyu/language-and-demos\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/dharakyu/language-a</span><span class=\"invisible\">nd-demos</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11374v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11374v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11364v1",
    "title": "https://arxiv.org/abs/2305.11364v1",
    "latest": "2023-05-22T08:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411301875994659",
      "content": "<p>\ud83d\udcdd Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"Presents LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets, which clusters text along syntactic, lexical, and semantic axes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/PAIR-code/interpretability\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/PAIR-code/interpret</span><span class=\"invisible\">ability</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11364v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11364v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11355v1",
    "title": "https://arxiv.org/abs/2305.11355v1",
    "latest": "2023-05-22T07:54:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411254744145126",
      "content": "<p>\ud83d\udcdd MD3: The Multi-Dialect Dataset of Dialogues \ud83d\udcda</p><p>\"A dataset of conversational speech from three major English-speaking countries: India, Nigeria, and the United States, with more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/nehasinha/Taboo/blob/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/nehasinha/Taboo/blo</span><span class=\"invisible\">b/</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11355v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11355v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09137",
    "title": "https://arxiv.org/abs/2305.09137",
    "latest": "2023-05-22T07:42:47+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381332490398862",
      "content": "<p>Pre-Training to Learn in Context</p><p>Proposes PICL, a framework to enhance the LMs' in-context learning ability by pre-training on \"intrinsic tasks\" in the general plain-text corpus using the simple LM objective.</p><p>repo: <a href=\"https://github.com/thu-coai/PICL\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/thu-coai/PICL</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.09137\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09137</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11334v1",
    "title": "Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs",
    "latest": "2023-05-22T07:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411113215008062",
      "content": "<p>\ud83d\udcdd Writing Your Own Book: A Method for Going From Closed to Open Book QA to Improve Robustness and Performance of Smaller LLMs \ud83d\udcda\ud83d\udc7e</p><p>\"Introduces two novel methods, Tree-Search and Self-contextualizing Question-Answering, designed to enhance the performance of large language models (LLMs) in question-answering tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11334v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11334v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11326v1",
    "title": "Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data",
    "latest": "2023-05-22T06:42:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410971702391662",
      "content": "<p>\ud83d\udcdd Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data \ud83d\udcda</p><p>\"Proposes a method to generate a chatbot from tabular data sources using natural language processing (NLP) and machine learning (ML) techniques, and to deploy it to offer citizens interactive access to public data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11326v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11326v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11317v1",
    "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
    "latest": "2023-05-22T06:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410924603280206",
      "content": "<p>\ud83d\udcdd Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation \ud83d\udcda</p><p>\"A large-scale language model that was trained on an open-domain corpus was used to edit the input prompt to improve the quality and consistency of image outputs by the state-of-the-art Text-to-Image (T2I) models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11317v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11317v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11315v1",
    "title": "https://arxiv.org/abs/2305.11315v1",
    "latest": "2023-05-22T05:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410735679563419",
      "content": "<p>\ud83d\udcdd Improving Toponym Resolution with Better Candidate Generation, Transformer-Based Reranking, and Two-Stage Resolution \ud83d\udcda</p><p>\"GeoNorm first uses information retrieval techniques to generate a list of candidate entries from the geospatial ontology, then reranks the candidate entries using a transformer-based neural network that incorporates information from the ontology such as the entry's population.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clulab/geonorm\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/clulab/geonorm</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11315v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11315v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125",
    "title": "Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity",
    "latest": "2023-05-22T05:27:17+00:00",
    "last_post": {
      "url": "https://mastodon.social/@devezer/110373663709370598",
      "content": "<p>And while epistemic pluralism may not be perfect or guarantee immediate progress, it should protect us from far worse excesses. Incidentally in our 2019 paper our computational model of  scientific process suggests that epistemic diversity is powerful in that it prevents worst-case scenarios and allows for scientific discovery while avoiding many traps. <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0216125</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@FroehlichMarcel",
        "display_name": "Marcel Fr\u00f6hlich"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11255v1",
    "title": "https://arxiv.org/abs/2305.11255v1",
    "latest": "2023-05-22T05:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410641360816775",
      "content": "<p>\ud83d\udcdd Reasoning Implicit Sentiment with Chain-of-Thought Prompting \ud83d\udcda</p><p>\"A three-step prompting principle is designed for THOR to guide the model to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/scofield7419/THOR-ISA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/scofield7419/THOR-I</span><span class=\"invisible\">SA</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11255v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11255v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.02819",
    "title": "GPT detectors are biased against non-native English writers",
    "latest": "2023-05-22T05:02:26+00:00",
    "last_post": {
      "url": "https://hci.social/@samgoree/110407420750624172",
      "content": "<p>In case we needed any more evidence to not use AI text detectors, they seem to be substantially biased against non-native English speakers.</p><p><a href=\"https://arxiv.org/abs/2304.02819\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.02819</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T04:26:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110410436966499091",
      "content": "<p>Cross-Lingual Supervision improves Large Language Models Pre-training</p><p>Pretraining LLMs on a mixture of a regular LM dataset + cross-lingual parallel data yields models with better in-context learning abilities. </p><p><a href=\"https://arxiv.org/abs/2305.11778\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11251v1",
    "title": "Computational thematics: Comparing algorithms for clustering the genres of literary fiction",
    "latest": "2023-05-22T04:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410405426422554",
      "content": "<p>\ud83d\udcdd Computational Thematics: Comparing Algorithms for Clustering the Genres of Literary Fiction \ud83d\udcda</p><p>\"S are tested on 5000 random novels from the HathiTrust corpus, which have been tagged with genres according to the library catalog.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11251v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11251v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11243v1",
    "title": "Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses",
    "latest": "2023-05-22T03:54:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410310966693543",
      "content": "<p>\ud83d\udcdd Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses \ud83d\udcda\ud83d\udc7e</p><p>\"LaMDA uses 175 billion parameters, and is trained on 175 billion words of English-language text, including the entirety of English-language books ever published, the entirety of public speeches, and Wikipedia.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11243v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11243v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09612",
    "title": "Large Language Models are Built-in Autoregressive Search Engines",
    "latest": "2023-05-22T03:42:39+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381297041837722",
      "content": "<p>Large Language Models are Built-in Autoregressive Search Engines</p><p>When providing a few Query-URL pairs as in-context demonstrations, LLMs can generate Web URLs where ~90% of the corresponding documents contain correct answers to open-domain questions.</p><p><a href=\"https://arxiv.org/abs/2305.09612\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09612</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11231v1",
    "title": "Recent Trends in Unsupervised Summarization",
    "latest": "2023-05-22T03:06:03+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410122067333373",
      "content": "<p>\ud83d\udcdd Recent Trends in Unsupervised Summarization \ud83d\udcda</p><p>\"A large number of unsupervised models have been proposed for text summarization in literature, with varying degrees of complexity and performance, and can be broadly classified into extractive, abstractive and hybrid.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11231v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11231v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2102.07350.pdf",
    "title": "https://arxiv.org/pdf/2102.07350.pdf",
    "latest": "2023-05-22T02:49:21+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110410056412505522",
      "content": "<p>also see \"Prompt Programming for Large Language Models:<br>Beyond the Few-Shot Paradigm\"</p><p> Informed by this more encompassing theory of<br>prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its<br>own natural language prompts for a range of tasks</p><p><a href=\"https://arxiv.org/pdf/2102.07350.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2102.07350.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mastodon.radio/tags/prompts\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>prompts</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.11430.pdf",
    "title": "https://arxiv.org/pdf/2305.11430.pdf",
    "latest": "2023-05-22T02:22:01+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110409948972775616",
      "content": "<p>\"TELeR: A General Taxonomy of LLM Prompts for Benchmarking<br>Complex Tasks\"</p><p><a href=\"https://arxiv.org/pdf/2305.11430.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.11430.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206v1",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T02:18:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110409933740146680",
      "content": "<p>\ud83d\udcdd LIMA: Less Is More for Alignment \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Language models trained in two stages: unsupervised pretraining from raw text to learn general-purpose representations and large-scale instruction tuning with supervision and no human feedback to better align to the end task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11206v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.00118",
    "title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",
    "latest": "2023-05-22T01:44:35+00:00",
    "last_post": {
      "url": "https://paquita.masto.host/@no_tan_incendiario/110343931803514302",
      "content": "<p>Han descubierto que ChatGPT ha \"\"memorizado\"\" m\u00e1s de 100 libros. A ver cu\u00e1ndo las editoriales afilan sus cuchillos. <a href=\"https://arxiv.org/abs/2305.00118\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.00118</span><span class=\"invisible\"></span></a> La lista de libros: <a href=\"https://docs.google.com/spreadsheets/d/1jW7EhsNjIGDMoK2JidyDD7UXH9N0NpEJfWFEj05_LC4/edit#gid=0\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">docs.google.com/spreadsheets/d</span><span class=\"invisible\">/1jW7EhsNjIGDMoK2JidyDD7UXH9N0NpEJfWFEj05_LC4/edit#gid=0</span></a> (fuente: <a href=\"https://github.com/bamman-group/gpt4-books\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/bamman-group/gpt4-b</span><span class=\"invisible\">ooks</span></a> )</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@dbamman",
        "display_name": "David Bamman"
      },
      {
        "url": "https://fediscience.org/@UlrichJunker",
        "display_name": "Ulrich Junker"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch (moderator)"
      },
      {
        "url": "https://mastodon.social/@jose_eduardo",
        "display_name": "Jos\u00e9 Eduardo Gonz\u00e1lez"
      },
      {
        "url": "https://sigmoid.social/@roban",
        "display_name": "Roban Hultman Kramer"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09515",
    "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
    "latest": "2023-05-22T01:42:35+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381269465767328",
      "content": "<p>AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation</p><p>On text summarization, NMT, and common sense generation, AR-Diffusion outperforms existing diffusion LMs and that it can be 100\u00d7 \u223c 600\u00d7 faster when achieving comparable results</p><p><a href=\"https://arxiv.org/abs/2305.09515\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09515</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11186v1",
    "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
    "latest": "2023-05-22T01:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110409791899497595",
      "content": "<p>\ud83d\udcdd Compress, Then Prompt: Improving Accuracy-Efficiency Trade-Off of LLM Inference with Transferable Prompt \ud83d\udcda\ud83e\udde0</p><p>\"By introducing a prompt learning paradigm that cultivates an additive prompt over a compressed Language Model (LM) to bolster their accuracy and match the performance of uncompressed LMs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11186v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11186v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T01:36:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409769364451473",
      "content": "<p>LIMA: Less Is More for Alignment</p><p>Presents LIMA, a 65B LLaMa language model fine-tuned on only 1k curated samples w/o RLHF. </p><p>Outputs from LIMA are either equivalent or strictly preferred to GPT-4 or Bard in 43% or 58% of cases, respectively.</p><p><a href=\"https://arxiv.org/abs/2305.11206\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01521-z",
    "title": "Humans and algorithms work together \u2014 so study them together",
    "latest": "2023-05-22T01:27:48+00:00",
    "last_post": {
      "url": "https://social.coop/@natematias/110350424967237041",
      "content": "<p>The US Supreme Court will soon decide if YouTube should be held responsible for allegedly permitting the platform\u2019s recommender algorithms to promote content that radicalized terrorists who killed Nohemi Gonzalez.</p><p>Whatever the court concludes, the case highlights an urgent question: how can we govern adaptive algorithms that continually change in response to people\u2019s behavior? The problem? Scientists can't currently answer this question.</p><p>See my new article in <span class=\"h-card\"><a href=\"https://sciencemastodon.com/@nature\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>nature</span></a></span> </p><p><a href=\"https://www.nature.com/articles/d41586-023-01521-z\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01521-z</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://social.coop/@natematias",
        "display_name": "J. Nathan Matias \ud83e\udda3"
      },
      {
        "url": "https://mstdn.social/@kissane",
        "display_name": "Erin Kissane"
      },
      {
        "url": "https://mastodon.social/@markcmarino",
        "display_name": "Mark C Marino"
      },
      {
        "url": "https://mastodon.social/@tiffanycli",
        "display_name": "Tiffany Li"
      },
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      },
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://fosstodon.org/@simon_goRin",
        "display_name": "Simon Gorin"
      },
      {
        "url": "https://hcommons.social/@julsraemy",
        "display_name": "Julien A. Raemy"
      },
      {
        "url": "https://social.coop/@dphiffer",
        "display_name": "Dan Phiffer"
      },
      {
        "url": "https://dair-community.social/@timnitGebru",
        "display_name": "Timnit Gebru (she/her)"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11738",
    "title": "https://arxiv.org/abs/2305.11738",
    "latest": "2023-05-22T01:24:23+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409722329971431",
      "content": "<p>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</p><p>repo: <a href=\"https://github.com/microsoft/ProphetNet/tree/master/CRITIC\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/ProphetNe</span><span class=\"invisible\">t/tree/master/CRITIC</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11738\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11738</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11675",
    "title": "https://arxiv.org/abs/2305.11675",
    "latest": "2023-05-22T01:18:22+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409698691968971",
      "content": "<p>Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity</p><p>proj: <a href=\"https://mind-video.com/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">mind-video.com/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11675\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11675</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11598",
    "title": "Introspective Tips: Large Language Model for In-Context Decision Making",
    "latest": "2023-05-22T00:53:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409600292246753",
      "content": "<p>Introspective Tips: Large Language Model for In-Context Decision Making</p><p><a href=\"https://arxiv.org/abs/2305.11598\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11598</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11841",
    "title": "How Does Generative Retrieval Scale to Millions of Passages?",
    "latest": "2023-05-22T00:40:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409549149527403",
      "content": "<p>How Does Generative Retrieval Scale to Millions of Passages?</p><p>Finds that the use of synthetic queries as a document representation strategy is the only approach that remained effective as they scaled up the corpus size using MS MARCO passages.</p><p><a href=\"https://arxiv.org/abs/2305.11841\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11841</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11846",
    "title": "https://arxiv.org/abs/2305.11846",
    "latest": "2023-05-22T00:37:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409537321914267",
      "content": "<p>Any-to-Any Generation via Composable Diffusion</p><p>Present CoDi, a novel generative model capable of generating any combination of output modalities from any combination of input modalities.</p><p>proj: <a href=\"https://codi-gen.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">codi-gen.github.io/</span><span class=\"invisible\"></span></a> <br>repo: <a href=\"https://github.com/microsoft/i-Code/tree/main/i-Code-V3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/i-Code/tr</span><span class=\"invisible\">ee/main/i-Code-V3</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11846\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11846</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11854",
    "title": "https://arxiv.org/abs/2305.11854",
    "latest": "2023-05-22T00:35:22+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409529572549613",
      "content": "<p>Multimodal Web Navigation with Instruction-Finetuned Foundation Models</p><p>Their 3B model outperforms the SoTA, PaLM-540B, on the WebShop benchmark. </p><p>Will make their 347K high-quality demonstrations publicly available.</p><p>proj: <a href=\"https://sites.google.com/view/mm-webnav/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">sites.google.com/view/mm-webna</span><span class=\"invisible\">v/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11854\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11854</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11863",
    "title": "Scaling laws for language encoding models in fMRI",
    "latest": "2023-05-22T00:30:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409509792875032",
      "content": "<p>Scaling laws for language encoding models in fMRI</p><p>Increasing scale in both models and data yields incredibly effective models of language processing in the brain, enabling better scientific understanding as well as applications such as decoding.</p><p><a href=\"https://arxiv.org/abs/2305.11863\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11863</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01612-x",
    "title": "For chemists, the AI revolution has yet to happen",
    "latest": "2023-05-21T23:45:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110409331655826124",
      "content": "<p>For chemists, the AI revolution has yet to happen - <a href=\"https://www.nature.com/articles/d41586-023-01612-x\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01612-x</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://osf.io/preprints/metaarxiv/fhdbs/",
    "title": "osf.io/preprints/metaarxiv/fhd...",
    "latest": "2023-05-21T23:39:40+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mcaleerp/110407615890521356",
      "content": "<p>Interesting paper in MetaArXiv: \u201cPreregistration in practice: A comparison of preregistered and non-preregistered studies in psychology\u201c. Main finding is whilst PreReg studies are more likely to state power analysis, inconsistent with previous findings, PreReg and non PreReg studies have similar amount of positive findings. Suggesting either PreReg doesn\u2019t reduce issues or issues are reduced in non PreReg studies these days. </p><p>Link: <a href=\"https://osf.io/preprints/metaarxiv/fhdbs/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">osf.io/preprints/metaarxiv/fhd</span><span class=\"invisible\">bs/</span></a></p><p><a href=\"https://mastodon.social/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a> <a href=\"https://mastodon.social/tags/MetaScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MetaScience</span></a> <a href=\"https://mastodon.social/tags/Psychology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Psychology</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.15056",
    "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks",
    "latest": "2023-05-21T22:23:10+00:00",
    "last_post": {
      "url": "https://qoto.org/@tedherman/110409008416499631",
      "content": "<p>Mechanical Turks considered in danger due to LLM/Chat tech. <a href=\"https://arxiv.org/abs/2303.15056\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.15056</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2205.09459",
    "title": "Neural Network Architecture Beyond Width and Depth",
    "latest": "2023-05-21T20:30:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110408564914038079",
      "content": "<p>Neural Network Architecture Beyond Width and Depth - <a href=\"https://arxiv.org/abs/2205.09459\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2205.09459</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s11229-023-04158-7",
    "title": "Epistemic diversity and industrial selection bias - Synthese",
    "latest": "2023-05-21T17:08:11+00:00",
    "last_post": {
      "url": "https://fediscience.org/@ct_bergstrom/110397512520315053",
      "content": "<p>Corporations can influence the directions and conclusions of academic research without corrupting the beliefs of any individual scientist. In a phenomenon known as industry selection bias, companies direct funding and/or data toward scientists who already support the research approaches or technological solutions favored by industry. </p><p>A new paper out in Synthese expands on the earlier Holman and Bruner model of this scenario to illustrate how this process works.</p><p><a href=\"https://link.springer.com/article/10.1007/s11229-023-04158-7\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s11229-023-04158-7</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://nerdculture.de/@dornhaus",
        "display_name": "Anna Dornhaus"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11162",
    "title": "High-Performance Graph Databases That Are Portable, Programmable, and Scale to Hundreds of Thousands of Cores",
    "latest": "2023-05-21T16:30:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110407621156510444",
      "content": "<p>High-Performance Graph Databases, Scaling to Hundreds of Thousands of Cores - <a href=\"https://arxiv.org/abs/2305.11162\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11162</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07759",
    "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
    "latest": "2023-05-21T16:23:02+00:00",
    "last_post": {
      "url": "https://mastodon.online/@jchyip/110386344853645107",
      "content": "<p><a href=\"https://mastodon.online/tags/TinyStories\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>TinyStories</span></a>: How Small Can Language Models Be and Still Speak Coherent English <a href=\"https://arxiv.org/abs/2305.07759\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07759</span><span class=\"invisible\"></span></a> <a href=\"https://mastodon.online/tags/ai\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ai</span></a> <a href=\"https://mastodon.online/tags/llm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>llm</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@bergi",
        "display_name": "Thomas Bergwinkl"
      }
    ]
  },
  {
    "link": "https://frontiersin.org/articles/10.3389/fpubh.2023.1221666/full",
    "title": "frontiersin.org/articles/10.33...",
    "latest": "2023-05-21T15:25:21+00:00",
    "last_post": {
      "url": "https://noc.social/@CaulfieldTim/110402373651558195",
      "content": "<p>Often referenced \"masks do harm\" study retracted.  </p><p>See: <a href=\"https://frontiersin.org/articles/10.3389/fpubh.2023.1221666/full\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">frontiersin.org/articles/10.33</span><span class=\"invisible\">89/fpubh.2023.1221666/full</span></a></p><p>Editors: \"...complaints were valid ... the article does not meet the standards of editorial &amp; scientific soundness...\"</p><p>Will likely live on as a zombie &amp; the retraction will become part of a conspiracy narrative.</p><p><a href=\"https://noc.social/tags/publichealth\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>publichealth</span></a> <a href=\"https://noc.social/tags/masks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>masks</span></a> <a href=\"https://noc.social/tags/scicomm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>scicomm</span></a> <a href=\"https://noc.social/tags/vaccineswork\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>vaccineswork</span></a> <a href=\"https://noc.social/tags/science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>science</span></a> <a href=\"https://noc.social/tags/retraction\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>retraction</span></a></p>"
    },
    "people": [
      {
        "url": "https://twit.social/@glennf",
        "display_name": "Glenn Fleishman"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41586-020-2487-2",
    "title": "Native American gene flow into Polynesia predating Easter Island settlement - Nature",
    "latest": "2023-05-21T15:17:03+00:00",
    "last_post": {
      "url": "https://social.sanfranciscan.org/objects/8971a192-bac6-4122-b369-24fca39e744f",
      "content": "<p>I\u2019ve always considered the sweet potato in Polynesia to be the \u201csmoking gun\u201d when it came to Pre-Columbian contact between the Americas and the \u201cOld World\u201d.</p><p>Now, genetic evidence appears to back up this theory. The article below concludes:</p><p>\u201c[We] find strong genetic evidence for pre-Columbian human trans-Pacific voyaging contact (at the turn of the twelfth century), contemporaneous with the Polynesian voyages of discovery in the remote eastern Pacific.\u201c</p><p><a href=\"https://www.nature.com/articles/s41586-020-2487-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">https://www.nature.com/articles/s41586-020-2487-2</a></p>"
    },
    "people": [
      {
        "url": "https://thefolklore.cafe/@juergen_hubert",
        "display_name": "J\u00fcrgen Hubert"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://vis.social/@mgiraldo",
        "display_name": "@mgiraldo"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s42254-023-00595-y",
    "title": "A new frontier for Hopfield networks - Nature Reviews Physics",
    "latest": "2023-05-21T14:52:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110402192452069793",
      "content": "<p>RT @DimaKrotov@twitter.com</p><p>Recent advances in Hopfield networks of associative memory may be the guiding theoretical principle for designing novel large scale neural architectures. I explain my enthusiasm about these ideas in the article \u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f. Please let me know what you think. <a href=\"https://www.nature.com/articles/s42254-023-00595-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s42254-023</span><span class=\"invisible\">-00595-y</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/DimaKrotov/status/1659526803155714053\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/DimaKrotov/status/</span><span class=\"invisible\">1659526803155714053</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21248/jlcl.36.2023.236",
    "title": "doi.org/10.21248/jlcl.36.2023....",
    "latest": "2023-05-21T14:44:49+00:00",
    "last_post": {
      "url": "https://mastodon.social/@fussballinguist/110406313988183627",
      "content": "<p>Our paper on \"Keyness in Song Lyrics\" is out: Jan Langenhorst, Yannick Frommherz and I discuss keyword analysis as an approach to computational stylistics by the example of song lyrics as highly clumpy data. We show that traditional bag-of-words-approached fail because of the repetitiveness of song lyrics and explore the advances of dispersion-oriented methods. <a href=\"https://mastodon.social/tags/openaccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>openaccess</span></a> <a href=\"https://doi.org/10.21248/jlcl.36.2023.236\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.21248/jlcl.36.2023.</span><span class=\"invisible\">236</span></a>.</p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@sascha_wolfer",
        "display_name": "Sascha Wolfer"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1004226",
    "title": "SARS-CoV-2 transmission with and without mask wearing or air cleaners in schools in Switzerland: A modeling study of epidemiological, environmental, and molecular data",
    "latest": "2023-05-21T14:37:30+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mackayim2022/110406167335561539",
      "content": "<p>SARS-CoV-2 transmission with and without mask wearing or air cleaners in schools in Switzerland: A modeling study of epidemiological, environmental, and molecular data. <br><a href=\"https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1004226\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosmedicine</span><span class=\"invisible\">/article?id=10.1371/journal.pmed.1004226</span></a></p>"
    },
    "people": [
      {
        "url": "https://zirk.us/@mrxmrt",
        "display_name": "Marcus"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11072v1",
    "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
    "latest": "2023-05-21T09:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405994038173144",
      "content": "<p>\ud83d\udcdd Self-Supervised Fine-Tuning for Improved Content Representations by Speaker-Invariant Clustering \ud83d\udcda</p><p>\"Proposes speaker-invariant clustering (Spin) that disentangles speaker information and preserves content representations for pre-trained networks with just 45 minutes of fine-tuning on a single GPU.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11072v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11072v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11068v1",
    "title": "ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph",
    "latest": "2023-05-21T09:12:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405899711320680",
      "content": "<p>\ud83d\udcdd ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph \ud83d\udcda\ud83d\udc7e</p><p>\"Orkg-Leaderboards uses a combination of machine-learning and rule-based approaches to extract Task-Dataset-Metric tuples from scholarly papers in AI domain.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11068v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11068v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11038v1",
    "title": "https://arxiv.org/abs/2305.11038v1",
    "latest": "2023-05-21T08:12:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405663906727945",
      "content": "<p>\ud83d\udcdd Learning in-Context Learning for Named Entity Recognition \ud83d\udcda</p><p>\"A new extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, and a meta-function pre-training algorithm is proposed to pre-train PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/chen700564/metaner-icl\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/chen700564/metaner-</span><span class=\"invisible\">icl</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11038v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11038v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08596",
    "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
    "latest": "2023-05-21T07:55:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110405596165938435",
      "content": "<p>DarkBERT: A Language Model for the Dark Side of the Internet - <a href=\"https://arxiv.org/abs/2305.08596\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08596</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11082",
    "title": "Fundamental Limitations of Alignment in Large Language Models",
    "latest": "2023-05-21T07:01:50+00:00",
    "last_post": {
      "url": "https://scholar.social/@joakinen/110266044734622865",
      "content": "<p>\"We propose a theoretical approach called Behavior Expectation Bounds (BEB) to investigate several inherent characteristics and limitations of <a href=\"https://scholar.social/tags/alignment\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>alignment</span></a> in <a href=\"https://scholar.social/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a>. We prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt.\"</p><p>Fundamental Limitations of Alignment in Large Language Models<br><a href=\"https://arxiv.org/abs/2304.11082\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11082</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11029v1",
    "title": "https://arxiv.org/abs/2305.11029v1",
    "latest": "2023-05-21T07:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405380817866757",
      "content": "<p>\ud83d\udcdd Uncertainty Guided Label Denoising for Document-Level Distant Relation Extraction \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a document-level distant relation extraction framework, UGDRE, with uncertainty guided label denoising (UGLD) for reducing noise and retaining high-quality labels.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/QiSun123/UGDRE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/QiSun123/UGDRE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11029v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11029v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/full/10.1080/0950236X.2012.638759",
    "title": "tandfonline.com/doi/full/10.10...",
    "latest": "2023-05-21T06:37:28+00:00",
    "last_post": {
      "url": "https://ravenation.club/@mpe/110405291140999411",
      "content": "<p>It's not open access, but the Textual Practice issue on Amis's _Money_ from a few years ago, edited by my colleague Joe Brooker, is worth a read. <br><a href=\"https://www.tandfonline.com/doi/full/10.1080/0950236X.2012.638759\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/full/10.10</span><span class=\"invisible\">80/0950236X.2012.638759</span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/epdf/10.1080/17513472.2023.2191572",
    "title": "tandfonline.com/doi/epdf/10.10...",
    "latest": "2023-05-21T06:36:16.343000+00:00",
    "last_post": {
      "url": "https://ciberlandia.pt/@villares/110123876632324473",
      "content": "<p>Research Article<br>Knitted origami<br>Elizabeth L. Wilmer</p><p>\"Techniques are presented for embedding horizontal, vertical, and45\u25e6diagonal crease lines into garter stitch knitted fabric. While theseare mostly based on standard lace knitting stitches, the horizon-tal creases use double-cable-crossed elongated stitches in a non-standard way. This crease library suffices to knit a model of a squaretwist, a foundational origami tessellation unit.\"</p><p><a href=\"https://www.tandfonline.com/doi/epdf/10.1080/17513472.2023.2191572\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/epdf/10.10</span><span class=\"invisible\">80/17513472.2023.2191572</span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@AlgoCompSynth",
        "display_name": "AlgoCompSynth by znmeb #MaskUp"
      },
      {
        "url": "https://vis.social/@Justin_Lind",
        "display_name": "Justin Lind"
      },
      {
        "url": "https://dair-community.social/@trochee",
        "display_name": "Jeremy Kahn"
      },
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      },
      {
        "url": "https://mstdn.social/@felwert",
        "display_name": "Frederik Elwert"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11023v1",
    "title": "https://arxiv.org/abs/2305.11023v1",
    "latest": "2023-05-21T06:24:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405239283755794",
      "content": "<p>\ud83d\udcdd Generalized Multiple Intent Conditioned Slot Filling \ud83d\udcda</p><p>\"Casts slot filling as a JSON generation task and approach it using a language model trained on pre-training datasets and an in-domain dataset generated with GPT-3.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/reinfer/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/reinfer/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11023v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11023v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11016v1",
    "title": "https://arxiv.org/abs/2305.11016v1",
    "latest": "2023-05-21T05:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405097640223081",
      "content": "<p>\ud83d\udcdd Silver Syntax Pre-Training for Cross-Domain Relation Extraction \ud83d\udcda</p><p>\"Exploits the affinity between syntactic structure and semantic RE, and identify the syntactic relations which are closely related to RE by being on the shortest dependency path between two entities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mainlp/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mainlp/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11016v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11016v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11000v1",
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "latest": "2023-05-21T05:12:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404955920701559",
      "content": "<p>\ud83d\udcdd SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities \ud83d\udcda</p><p>\"A three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11000v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11000v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10998v1",
    "title": "The Web Can Be Your Oyster for Improving Large Language Models",
    "latest": "2023-05-21T04:36:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404814488183160",
      "content": "<p>\ud83d\udcdd The Web Can Be Your Oyster for Improving Large Language Models \ud83d\udcda</p><p>\"Proposes a web-augmented LLM UNIWEB which learns over 16 knowledge-intensive tasks in a unified text-to-text format.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10998v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10998v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10992v1",
    "title": "How does the task complexity of masked pretraining objectives affect downstream performance?",
    "latest": "2023-05-21T04:00:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404672752845374",
      "content": "<p>\ud83d\udcdd How Does the Task Complexity of Masked Pretraining Objectives Affect Downstream Performance? \ud83d\udcda\ud83d\udc7e</p><p>\"Pretraining a model with a masked language modeling task and fine-tuning it on downstream tasks such as sentiment analysis or natural language understanding tasks shows the state-of-the-art result.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10992v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10992v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10991v1",
    "title": "Less is More! A slim architecture for optimal language translation",
    "latest": "2023-05-21T03:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404436705280754",
      "content": "<p>\ud83d\udcdd Less Is More! A Slim Architecture for Optimal Language Translation \ud83d\udcda\ud83d\udc7e</p><p>\"A gating mechanism to remove excess parameters in softmax attention, and a hierarchical word embedding scheme to boost performance of the embedding layer while also reducing parameter counts by a factor of 3.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10991v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10991v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10985v1",
    "title": "https://arxiv.org/abs/2305.10985v1",
    "latest": "2023-05-21T02:48:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404389675586755",
      "content": "<p>\ud83d\udcdd Multi-CrossRE a Multi-Lingual Multi-Domain Dataset for Relation Extraction \ud83d\udcda</p><p>\"Multi-CrossRE is a multi-lingual RE dataset created by translating CrossRE (Bassignana and Plank, 2022) into different languages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mainlp/CrossRE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mainlp/CrossRE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10985v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10985v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10951v1",
    "title": "https://arxiv.org/abs/2305.10951v1",
    "latest": "2023-05-21T01:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404153958070478",
      "content": "<p>\ud83d\udcdd Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation \ud83d\udcda</p><p>\"Self-training uses an existing ASR system to generate transcriptions of untranscribed speech, which are then combined with existing human-transcribed speech to improve performance.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Bartelds/asr-augmentation\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/Bartelds/asr-augmen</span><span class=\"invisible\">tation</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10951v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10951v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s00239-019-09896-2",
    "title": "Ageing Throughout History: The Evolution of Human Lifespan - Journal of Molecular Evolution",
    "latest": "2023-05-21T01:25:49+00:00",
    "last_post": {
      "url": "https://mastodon.social/@freakonometrics/110404065675005494",
      "content": "<p>\"Ageing Throughout History: The Evolution of Human Lifespan\" <a href=\"https://link.springer.com/article/10.1007/s00239-019-09896-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s00239-019-09896-2</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@freakonometrics",
        "display_name": "Arthur Charpentier"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10930v1",
    "title": "https://arxiv.org/abs/2305.10930v1",
    "latest": "2023-05-21T01:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403965242101491",
      "content": "<p>\ud83d\udcdd On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation \ud83d\udcda\ud83d\udc7e</p><p>\"Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/chenllliang/Off-Target-MNMT}{https://github.com/chenllliang/Off-Target-MNMT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/chenllliang/Off-Tar</span><span class=\"invisible\">get-MNMT}{https://github.com/chenllliang/Off-Target-MNMT</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10930v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10930v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10927v1",
    "title": "Causal Document-Grounded Dialogue Pre-training",
    "latest": "2023-05-21T00:36:18+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403870937916053",
      "content": "<p>\ud83d\udcdd Causal Document-Grounded Dialogue Pre-Training \ud83d\udcda</p><p>\"A causally-complete dataset construction strategy and a causally-perturbed pre-training strategy to capture the causal relationships between variables and optimize the overall casual effect, achieving considerable and consistent improvements in the fully-supervised, low-resource, few-shot, and zero-shot settings.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10927v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10927v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281010",
    "title": "The king\u2019s spice cabinet\u2013Plant remains from Gribshunden, a 15th century royal shipwreck in the Baltic Sea",
    "latest": "2023-05-20T23:27:00+00:00",
    "last_post": {
      "url": "https://xoxo.zone/@lauraehall/110397333505819975",
      "content": "<p>1) The shipwreck of the 1495 medieval Danish warship Gribshunden turned out to have incredibly well-preserved plant remains, including expensive spices like saffron, peppercorns, ginger, and almond.</p><p>It's a \u201csubstantially complete royal medieval pantry\u201d and is \"[one of] the most fabulous discoveries of spices in any archaeological context, on land or sea\"</p><p>View the paper here: <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281010\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0281010</span></a></p><p><a href=\"https://xoxo.zone/tags/Medieval\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Medieval</span></a> <a href=\"https://xoxo.zone/tags/Gribshunden\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Gribshunden</span></a> <a href=\"https://xoxo.zone/tags/Shipwreck\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Shipwreck</span></a> <a href=\"https://xoxo.zone/tags/Archaeology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Archaeology</span></a> <a href=\"https://xoxo.zone/tags/MaritimeArchaeology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MaritimeArchaeology</span></a> <a href=\"https://xoxo.zone/tags/Viking\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Viking</span></a> <a href=\"https://xoxo.zone/tags/Vikings\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Vikings</span></a> <a href=\"https://xoxo.zone/tags/Histodons\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Histodons</span></a></p>"
    },
    "people": [
      {
        "url": "https://xoxo.zone/@lauraehall",
        "display_name": "Laura E. Hall"
      },
      {
        "url": "https://kolektiva.social/@aredridel",
        "display_name": "Mx. Aria Stewart"
      },
      {
        "url": "https://everything.happens.horse/@vruba",
        "display_name": "Charlie Loyd"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01705-7",
    "title": "China overtakes United States on contribution to research in Nature Index",
    "latest": "2023-05-20T23:25:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110403590743895835",
      "content": "<p>China overtakes United States on contribution to research in Nature Index - <a href=\"https://www.nature.com/articles/d41586-023-01705-7\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01705-7</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10920v1",
    "title": "https://arxiv.org/abs/2305.10920v1",
    "latest": "2023-05-20T23:12:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403540459568637",
      "content": "<p>\ud83d\udcdd Emergent Communication with Attention \ud83d\udcda\ud83d\udc7e</p><p>\"Attention allows the agents to learn to focus on particular concepts in the environment and align with their partner on the concepts they focus on, which leads to a more compositional and interpretable emergent language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/zalandoresearch/fashion-mnist\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/zalandoresearch/fas</span><span class=\"invisible\">hion-mnist</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10920v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10920v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07922",
    "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
    "latest": "2023-05-20T23:09:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110375897971452835",
      "content": "<p>CodeT5+: Open Code Large Language Models for Code Understanding and Generation</p><p>The instruction-tuned CodeT5+ 16B achieves new SoTA of 35.0% pass@1 on the HumanEval code generation task against other open code LLMs.</p><p><a href=\"https://arxiv.org/abs/2305.07922\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07922</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10907v1",
    "title": "Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation",
    "latest": "2023-05-20T22:24:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403351800700620",
      "content": "<p>\ud83d\udcdd Take a Break in the Middle: Investigating Subgoals Towards Hierarchical Script Generation \ud83d\udcda</p><p>\"The goal is first decomposed into a few steps and then each step is further decomposed into subgoals, until the leaf nodes (i,e, primitive actions) are reached.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10907v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10907v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10866v1",
    "title": "TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition",
    "latest": "2023-05-20T21:24:18+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403115970890544",
      "content": "<p>\ud83d\udcdd TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition \ud83d\udcda</p><p>\"Proposed to design auxiliary tasks with enlightened prompt learning for Implicit Discourse Relation Recognition task based on the joint training of three prompt learning tasks with shared argument representation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10866v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10866v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10847v1",
    "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
    "latest": "2023-05-20T20:00:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402785326190004",
      "content": "<p>\ud83d\udcdd Large Language Models Can Be Guided to Evade AI-Generated Text Detection \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts, which enables LLMs to evade existing plagiarism detectors.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10847v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10847v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10468",
    "title": "Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence",
    "latest": "2023-05-20T19:46:43+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110402732285772796",
      "content": "<p>[R] Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence</p><p><a href=\"https://arxiv.org/abs/2305.10468\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10468</span><span class=\"invisible\"></span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://arxiv.org/abs/2305.10468\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://arxiv.org/a</span><span class=\"invisible\">bs/2305.10468</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08596?utm_source=tldrai",
    "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
    "latest": "2023-05-20T19:30:02+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110402666666468743",
      "content": "<p>RT @neuroecology@twitter.com</p><p>\"We need to solve the alignment problem before a MegaCorp creates an AI that turns us all into paperclips\"</p><p>Meanwhile, academics:</p><p><a href=\"https://arxiv.org/abs/2305.08596?utm_source=tldrai\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">arxiv.org/abs/2305.08596?utm_s</span><span class=\"invisible\">ource=tldrai</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/neuroecology/status/1659998392817266690\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/neuroecology/statu</span><span class=\"invisible\">s/1659998392817266690</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10845v1",
    "title": "https://arxiv.org/abs/2305.10845v1",
    "latest": "2023-05-20T19:24:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402644030334523",
      "content": "<p>\ud83d\udcdd TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model \ud83d\udcda</p><p>\"Consists of two passes: the first pass uses a non-incremental model, and the second pass uses an incremental policy to revise its prediction based on the first pass.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/pkhdipraja/tapir\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/pkhdipraja/tapir</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10845v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10845v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10833v1",
    "title": "Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants",
    "latest": "2023-05-20T18:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402455092024406",
      "content": "<p>\ud83d\udcdd Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants \ud83d\udcda</p><p>\"Works on a dataset of flowers and plants with the help of different transformer-based models to distinguish between metaphoric and literal flower and plant names using transfer learning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10833v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10833v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10819v1",
    "title": "https://arxiv.org/abs/2305.10819v1",
    "latest": "2023-05-20T18:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402313723964201",
      "content": "<p>\ud83d\udcdd CLEME: Debiasing Multi-Reference Evaluation for Grammatical Error Correction \ud83d\udcda</p><p>\"CLEME first constructs consistent chunk sequences for each sentence, then computes F$_{0,5}$ scores based on the chunks, where the boundaries of grammatical errors are determined automatically.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/yejh123/CLEME\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/yejh123/CLEME</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10819v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10819v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/zpgdf/",
    "title": "http://osf.io/zpgdf/",
    "latest": "2023-05-20T17:56:43+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110402299702954366",
      "content": "<p>Measuring Backsliding with Observables: Observable-to-Subjective Score Mapping (OSM) <a href=\"http://osf.io/zpgdf/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/zpgdf/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2203.02155.pdf",
    "title": "https://arxiv.org/pdf/2203.02155.pdf",
    "latest": "2023-05-20T17:28:51+00:00",
    "last_post": {
      "url": "https://mastodon.online/@slashdottir/110402189991015772",
      "content": "<p>nvm, I think I found it</p><p><a href=\"https://arxiv.org/pdf/2203.02155.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2203.02155.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv ML / AI"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10786v1",
    "title": "https://arxiv.org/abs/2305.10786v1",
    "latest": "2023-05-20T17:12:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402124888334936",
      "content": "<p>\ud83d\udcdd Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings \ud83d\udcda</p><p>\"Diagonal Attention Pooling (Ditto) alleviates the anisotropy problem and improve pre-trained models on STS tasks with model-based importance estimations and weighted average of word representations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/princeton-nlp/SimCSE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/princeton-nlp/SimCS</span><span class=\"invisible\">E</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10786v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10786v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08298",
    "title": "Symbol tuning improves in-context learning in language models",
    "latest": "2023-05-20T16:51:30+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110375823308376698",
      "content": "<p>Symbol tuning improves in-context learning in language models</p><p>Presents symbol tuning\u2014finetuning LMs on in-context input\u2013label pairs where natural language labels (e.g., \u201cpositive sentiment\u201d) are replaced with arbitrary symbols (e.g., \u201cfoo/bar\u201d).</p><p><a href=\"https://arxiv.org/abs/2305.08298\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08298</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10736v1",
    "title": "Counterfactual Debiasing for Generating Factually Consistent Text Summaries",
    "latest": "2023-05-20T16:48:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402030532597530",
      "content": "<p>\ud83d\udcdd Counterfactual Debiasing for Generating Factually Consistent Text Summaries \ud83d\udcda\ud83d\udc7e</p><p>\"Constructs causal graphs for abstractive text summarization and identify the intrinsic causes of factual inconsistency, i,e, language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10736v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10736v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.15004.pdf",
    "title": "https://arxiv.org/pdf/2304.15004.pdf",
    "latest": "2023-05-20T16:20:16+00:00",
    "last_post": {
      "url": "https://dair-community.social/@timnitGebru/110398594256377809",
      "content": "<p>\"we present an al-<br>ternative explanation for emergent abilities: that for a particular task and model<br>family, when analyzing fixed model outputs, one can choose a metric which leads<br>to the inference of an emergent ability or another...which does not..our alternative suggests that..claims of emergent abilities are creations of the researcher\u2019s analyses, not fundamental changes in model behavior on specific tasks with scale\"</p><p>Rylan Schaeffer, Brando Miranda, &amp; Sanmi Koyejo</p><p><a href=\"https://arxiv.org/pdf/2304.15004.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.15004.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@eliocamp",
        "display_name": "Elio Campitelli"
      },
      {
        "url": "https://masto.ai/@cheng",
        "display_name": "Cheng Soon Ong"
      },
      {
        "url": "https://dair-community.social/@timnitGebru",
        "display_name": "Timnit Gebru (she/her)"
      },
      {
        "url": "https://vis.social/@futuraprime",
        "display_name": "Evan Hensleigh"
      },
      {
        "url": "https://wandering.shop/@janellecshane",
        "display_name": "Janelle Shane"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10824v1",
    "title": "https://arxiv.org/abs/2305.10824v1",
    "latest": "2023-05-20T16:16:26+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_ir/110401841834778788",
      "content": "<p>\ud83d\udcdd Integrating Item Relevance in Training Loss for Sequential Recommender Systems \ud83d\udcbf\ud83d\udc7e</p><p>\"Proposes a relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise in user interactions by leveraging the relevance among them.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/IR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>IR</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/pmixer/SASRec.pytorch\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/pmixer/SASRec.pytor</span><span class=\"invisible\">ch</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10824v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10824v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://recsys.social/@karlhigley",
        "display_name": "Karl Higley"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41598-023-32559-8",
    "title": "Efficient shallow learning as an alternative to deep learning - Scientific Reports",
    "latest": "2023-05-20T15:46:21+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110401787124130864",
      "content": "<p>[D] Efficient shallow learning as an alternative to deep learning</p><p><a href=\"https://www.nature.com/articles/s41598-023-32559-8\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41598-023</span><span class=\"invisible\">-32559-8</span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://www.nature.com/articles/s41598-023-32559-8\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://www.nature.</span><span class=\"invisible\">com/articles/s41598-023-32559-8</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10731v1",
    "title": "https://arxiv.org/abs/2305.10731v1",
    "latest": "2023-05-20T15:36:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110401747455686015",
      "content": "<p>\ud83d\udcdd Analyzing Norm Violations in Live-Stream Chat \ud83d\udcda</p><p>\"Collects a dataset of over 4,500 moderated Twitch chat comments and annotate each comment according to a taxonomy of norm violations specific to live-streams.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/TwitchIO/TwitchIO\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/TwitchIO/TwitchIO</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10731v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10731v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10713v1",
    "title": "https://arxiv.org/abs/2305.10713v1",
    "latest": "2023-05-20T15:24:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110401700309778533",
      "content": "<p>\ud83d\udcdd Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency \ud83d\udcda\ud83e\udde0</p><p>\"Quantifies the robustness of the model towards its parameter perturbations and is based on the flatness metric from statistical learning theory, which has been shown to be effective in quantifying generalization and robustness properties of models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/shadowkiller33/flatness\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/shadowkiller33/flat</span><span class=\"invisible\">ness</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10713v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10713v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10160",
    "title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
    "latest": "2023-05-20T15:08:59+00:00",
    "last_post": {
      "url": "https://techhub.social/@arthurh/110401640169356571",
      "content": "<p>While I understand the problem of data contamination, I thought it would be easy to detect using the difference in perplexity between the train and the test split <a href=\"https://techhub.social/tags/nlp\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>nlp</span></a> <a href=\"https://techhub.social/tags/ai\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ai</span></a> <a href=\"https://techhub.social/tags/llm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>llm</span></a></p><p>[2305.10160] Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks <a href=\"https://arxiv.org/abs/2305.10160\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10160</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://techhub.social/@arthurh",
        "display_name": "Arthur"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10709v1",
    "title": "NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing",
    "latest": "2023-05-20T14:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110401558676983973",
      "content": "<p>\ud83d\udcdd NoisywikiHow: A Benchmark for Learning with Real-World Noisy Labels in Natural Language Processing \ud83d\udcda\ud83d\udc7e</p><p>\"We explicitly construct multiple sources of label noise to imitate human errors throughout the annotation, replicating real-world noise, whose corruption is affected by both ground-truth labels and instances.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10709v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10709v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2101.04719",
    "title": "https://arxiv.org/abs/2101.04719",
    "latest": "2023-05-20T14:47:31+00:00",
    "last_post": {
      "url": "https://hci.social/@upol/110397497407073819",
      "content": "<p>\ud83c\udf81Starter pack to avoid Explainability Washing:</p><p>1) Don\u2019t just assert it\u2019s an explanation, justify how it is so: focus on the why-question [B,C]</p><p>2) Identify *who* the users are + *how* it helps them to *do* something [A, B]</p><p>3) Focus on *who* is opening the black-box, not just on opening it [A]</p><p>[A] The Who in XAI: <a href=\"https://twitter.com/UpolEhsan/status/1421142478229540867?t=0QmE6_gigEIAsdVYWMUYCQ&amp;s=19\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/UpolEhsan/status/1</span><span class=\"invisible\">421142478229540867?t=0QmE6_gigEIAsdVYWMUYCQ&amp;s=19</span></a></p><p>[B] Expanding Explainability: <a href=\"https://arxiv.org/abs/2101.04719\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2101.04719</span><span class=\"invisible\"></span></a></p><p>[C] XAI + Social Science: <a href=\"https://www.sciencedirect.com/science/article/pii/S0004370218305988\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">sciencedirect.com/science/arti</span><span class=\"invisible\">cle/pii/S0004370218305988</span></a></p><p>5/5 (n=5)</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://hci.social/@andresmh",
        "display_name": "Andr\u00e9s Monroy-Hern\u00e1ndez"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10601v1",
    "title": "https://arxiv.org/abs/2305.10601v1",
    "latest": "2023-05-20T09:48:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110400379093908849",
      "content": "<p>\ud83d\udcdd Tree of Thoughts: Deliberate Problem Solving with Large Language Models \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Language models are being increasingly used for a wide range of problem solving tasks, but are confined to making token-level left-to-right decisions during inference, and can fall short when initial decisions play a pivotal role or when the problem is not well defined.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ysymyth/tree-of-thought-llm\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/ysymyth/tree-of-tho</span><span class=\"invisible\">ught-llm</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10601v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10601v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10588v1",
    "title": "A Better Way to Do Masked Language Model Scoring",
    "latest": "2023-05-20T08:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110400143124504188",
      "content": "<p>\ud83d\udcdd A Better Way to Do Masked Language Model Scoring \ud83d\udcda</p><p>\"Estimating the log-likelihood of a given sentence under a masked language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10588v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10588v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10568v1",
    "title": "From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?",
    "latest": "2023-05-20T08:12:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110400001467812777",
      "content": "<p>\ud83d\udcdd From Chocolate Bunny to Chocolate Crocodile: Do Language Models Understand Noun Compounds? \ud83d\udcda</p><p>\"The outputs from GPT-3 often have significant overlap with a large web corpus, but the parroting strategy is less beneficial for novel noun compounds and for noun-noun compounds than more compositional compounds like adjective-noun and verb-object compounds.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10568v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10568v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10561v1",
    "title": "Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search",
    "latest": "2023-05-20T07:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399859886252548",
      "content": "<p>\ud83d\udcdd Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search \ud83d\udcda</p><p>\"ISI-Clear uses an existing state-of-the-art event extraction system (TriggerWild) to detect and classify event triggers in documents.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10561v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10561v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.09144",
    "title": "http://arxiv.org/abs/2305.09144",
    "latest": "2023-05-20T07:05:27+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@BenjaminHan/110399738810535290",
      "content": "<p>8/</p><p>[2] Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Jiawei Chen, Tianshu Wang, and Le Sun. 2023. Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. <a href=\"http://arxiv.org/abs/2305.09144\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.09144</span><span class=\"invisible\"></span></a> </p><p>[3] Hermann Ebbinghaus. 2013. Memory: a contribution to experimental psychology. Annals of neurosciences, 20(4):155\u2013156. <a href=\"http://dx.doi.org/10.5214/ans.0972.7531.200408\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"ellipsis\">dx.doi.org/10.5214/ans.0972.75</span><span class=\"invisible\">31.200408</span></a> </p><p><a href=\"https://sigmoid.social/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://sigmoid.social/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> <a href=\"https://sigmoid.social/tags/Paper\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Paper</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10557v1",
    "title": "Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations",
    "latest": "2023-05-20T07:00:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399718341880996",
      "content": "<p>\ud83d\udcdd Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations \ud83d\udcda</p><p>\"Regularizes self-attention to be more symmetric, which is expected to enhance APE models' understanding of the target language by encouraging the model to pay attention to more words within the sentence than just the words in the source.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10557v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10557v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10519v1",
    "title": "Statistical Knowledge Assessment for Generative Language Models",
    "latest": "2023-05-20T06:00:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399482432950391",
      "content": "<p>\ud83d\udcdd Statistical Knowledge Assessment for Generative Language Models \ud83d\udcda\ud83e\udde0</p><p>\"Our proposed knowledge assessment framework, which includes latent variables and the KaRR metric, quantifies a model's knowledge by computing the probability of generating factually correct text from diverse forms.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10519v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10519v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10496v1",
    "title": "https://arxiv.org/abs/2305.10496v1",
    "latest": "2023-05-20T05:36:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399387940976125",
      "content": "<p>\ud83d\udcdd Incorporating Attribution Importance for Improving Faithfulness Metrics \ud83d\udcda\ud83d\udc7e</p><p>\"Soft-faithfulness is a soft erasure criterion that replaces hard erasure criterion in computing faithfulness metrics, which better reflects how faithful an attribution is to the model prediction.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/casszhao/SoftFaith\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/casszhao/SoftFaith</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10496v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10496v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10448v1",
    "title": "Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding",
    "latest": "2023-05-20T04:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399152083963645",
      "content": "<p>\ud83d\udcdd Sequence-to-Sequence Pre-Training with Unified Modality Masking for Visual Document Understanding \ud83d\udcda\ud83d\udc7e</p><p>\"Adopts a transformer encoder-decoder architecture and pre-trains the model with unified masking across three modalities: text, image, and layout, and adopts both modality-specific instruction and the mixture of modality experts strategy to better capture each modality.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10448v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10448v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/vn5tr/",
    "title": "http://osf.io/vn5tr/",
    "latest": "2023-05-20T03:41:07+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110398935367821350",
      "content": "<p>Inoculating against Persuasion by Male Supremacy Messages: The Moderating Roles of Propaganda Form and Subtlety <a href=\"http://osf.io/vn5tr/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/vn5tr/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/epn2c/",
    "title": "http://osf.io/epn2c/",
    "latest": "2023-05-20T03:41:07+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110398935398590929",
      "content": "<p>Six Months in Exile: A New Life of Russian Emigrants <a href=\"http://osf.io/epn2c/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/epn2c/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10447v1",
    "title": "The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring",
    "latest": "2023-05-20T03:36:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398916256178738",
      "content": "<p>\ud83d\udcdd The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring \ud83d\udcda\ud83d\udc7e</p><p>\"Uses a dynamic loss function that creates an incentive for the model to predict with the correct distribution, as well as predicting the correct values using a modified mean square error loss.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10447v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10447v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/8n2gj/",
    "title": "http://osf.io/8n2gj/",
    "latest": "2023-05-20T03:32:21+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110398900870962571",
      "content": "<p>Visual autoethnography of daily sounds <a href=\"http://osf.io/8n2gj/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/8n2gj/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/c8qr4/",
    "title": "http://osf.io/c8qr4/",
    "latest": "2023-05-20T03:32:20+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110398900849602124",
      "content": "<p>The Echo of Neighborhood Disadvantage: Multigenerational Contextual Hardship and Adult Income for Whites, Blacks, and Latinos <a href=\"http://osf.io/c8qr4/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/c8qr4/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10438v1",
    "title": "https://arxiv.org/abs/2305.10438v1",
    "latest": "2023-05-20T03:12:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398822015564378",
      "content": "<p>\ud83d\udcdd IMAGINATOR: Pre-Trained Image+Text Joint Embeddings Using Word-Level Grounding of Images \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"Joint Embedding IMAGINATOR is jointly trained on three individual representations: (i) object-object co-location, (ii) word-object co-location, and (iii) word-object correlation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/varunakk/IMAGINATOR\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/varunakk/IMAGINATOR</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10438v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10438v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10436v1",
    "title": "SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues",
    "latest": "2023-05-20T02:12:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398586015737803",
      "content": "<p>\ud83d\udcdd SmartPhone: Exploring Keyword Mnemonic with Auto-Generated Verbal and Visual Cues \ud83d\udcda</p><p>\"Leverages large language models to generate cues based on a combination of three factors: (i) the new word, (ii) the keyword, and (iii) the cue-type.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10436v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10436v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/pdf/10.1145/3479860",
    "title": "dl.acm.org/doi/pdf/10.1145/347...",
    "latest": "2023-05-20T02:05:23+00:00",
    "last_post": {
      "url": "https://hci.social/@sarahgilbert/110395673665571480",
      "content": "<p><span class=\"h-card\"><a href=\"https://social.coop/@natematias\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>natematias</span></a></span> Dipto Das, a doc candidate at UC Boulder, has been researching colonialism on Quora. Perhaps his work will be helpful? <br> <a href=\"https://dl.acm.org/doi/pdf/10.1145/3479860\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/347</span><span class=\"invisible\">9860</span></a><br><a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517600\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/fullHtml/10.114</span><span class=\"invisible\">5/3491102.3517600</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517600",
    "title": "dl.acm.org/doi/fullHtml/10.114...",
    "latest": "2023-05-20T02:05:23+00:00",
    "last_post": {
      "url": "https://hci.social/@sarahgilbert/110395673665571480",
      "content": "<p><span class=\"h-card\"><a href=\"https://social.coop/@natematias\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>natematias</span></a></span> Dipto Das, a doc candidate at UC Boulder, has been researching colonialism on Quora. Perhaps his work will be helpful? <br> <a href=\"https://dl.acm.org/doi/pdf/10.1145/3479860\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/347</span><span class=\"invisible\">9860</span></a><br><a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517600\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/fullHtml/10.114</span><span class=\"invisible\">5/3491102.3517600</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10435v1",
    "title": "Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",
    "latest": "2023-05-20T01:36:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398444492867803",
      "content": "<p>\ud83d\udcdd Generative Pre-Trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions \ud83d\udcda\ud83d\udc7e</p><p>\"Provides a detailed overview of the Generative Pre-trained Transformer, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10435v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10435v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10434v1",
    "title": "https://arxiv.org/abs/2305.10434v1",
    "latest": "2023-05-20T01:24:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398397256873838",
      "content": "<p>\ud83d\udcdd Learning the Visualness of Text Using Large Vision-Language Models \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Proposes a fine-tuning strategy that adapts large vision-language models like CLIP to the task of scoring text visualness from text input alone by modifying contrastive learning objective to map text identified as non-visual and visual to a common NULL image and its corresponding image.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/pymupdf/PyMuPDF\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/pymupdf/PyMuPDF</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10434v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10434v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10433v1",
    "title": "https://arxiv.org/abs/2305.10433v1",
    "latest": "2023-05-20T00:36:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398208130523515",
      "content": "<p>\ud83d\udcdd Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback \ud83d\udcda</p><p>\"Provides a mechanism to improve the reliability of toxic language datasets by centering the evaluator's values in the dataset creation process through a human-in-the-loop process that involves the evaluator in dataset curation via an iterative feedback cycle.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/HKUST-KnowComp/MLMA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/HKUST-KnowComp/MLMA</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10433v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10433v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10355v1",
    "title": "https://arxiv.org/abs/2305.10355v1",
    "latest": "2023-05-19T23:30:25+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110397949604896968",
      "content": "<p>\ud83d\udcdd Evaluating Object Hallucination in Large Vision-Language Models \ud83d\udd2d\ud83d\udcda</p><p>\"POPE is a polling-based query mechanism for evaluation, which can better reflect the object hallucination problem of large vision-language models by filtering out the impact of different input instructions and generation styles.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/RUCAIBox/POPE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/RUCAIBox/POPE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10355v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10355v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09807v1",
    "title": "https://arxiv.org/abs/2305.09807v1",
    "latest": "2023-05-19T21:30:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110397478087456156",
      "content": "<p>\ud83d\udcdd On Dataset Transferability in Active Learning for Transformers \ud83e\udde0\ud83d\udcda</p><p>\"Studies the transferability of the actively acquired datasets from the perspective of the similarity between queries from different models used for active learning on the same task and find that models with more similar active sequences result in more transferable datasets.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/fjelenic/al-transfer\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/fjelenic/al-transfe</span><span class=\"invisible\">r</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09807v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09807v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10429v1",
    "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
    "latest": "2023-05-19T20:42:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110397289254856860",
      "content": "<p>\ud83d\udcdd DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining \ud83d\udcda\ud83e\udde0</p><p>\"DoReMi first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10429v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10429v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10427v1",
    "title": "Accelerating Transformer Inference for Translation via Parallel Decoding",
    "latest": "2023-05-19T20:06:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110397147711324485",
      "content": "<p>\ud83d\udcdd Accelerating Transformer Inference for Translation via Parallel Decoding \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Jacobi and Gauss-Seidel parallel decoding algorithms reframe the standard greedy decoding in MT leveraging fixed-point iteration methods to speed up inference without training or modifications while retaining translation quality.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10427v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10427v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09620",
    "title": "AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys",
    "latest": "2023-05-19T19:31:19+00:00",
    "last_post": {
      "url": "https://mastodon.social/@cbail/110396994037297923",
      "content": "<p>Can Generative AI help us deal with missing data? Interesting new paper: <a href=\"https://arxiv.org/abs/2305.09620\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09620</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10425v1",
    "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback",
    "latest": "2023-05-19T19:18:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110396958808094878",
      "content": "<p>\ud83d\udcdd SLiC-HF: Sequence Likelihood Calibration with Human Feedback \ud83d\udcda\ud83d\udc7e</p><p>\"Sequence Likelihood Calibration uses a simple modification to supervised learning that allows models to better learn from human feedback without the need for RL algorithms and without the need for human preference data for the model being optimized.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10425v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10425v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10973",
    "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold",
    "latest": "2023-05-19T18:45:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110396827400173185",
      "content": "<p>Drag Your GAN: Interactive Point-Based Manipulation of Images - <a href=\"https://arxiv.org/abs/2305.10973\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10973</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07558",
    "title": "Measuring Progress in Fine-grained Vision-and-Language Understanding",
    "latest": "2023-05-19T18:21:25+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110369937023196434",
      "content": "<p>Measuring Progress in Fine-grained Vision-and-Language Understanding</p><p>Finds that X-VLM consistently outperforms other <br>VL baselines, and that modelling innovations can impact performance more than scaling Web data.</p><p><a href=\"https://arxiv.org/abs/2305.07558\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07558</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10403v1",
    "title": "PaLM 2 Technical Report",
    "latest": "2023-05-19T18:18:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110396723061735952",
      "content": "<p>\ud83d\udcdd PaLM 2 Technical Report \ud83d\udcda\ud83d\udc7e</p><p>\"The PaLM 2 model is a Transformer-based model trained using a mixture of objectives: autoregressive language modeling, masked language modeling, and sentence order prediction (SOP).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10403v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10403v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11030",
    "title": "Is Our Organization Actually Measuring Productivity? How Contrasting Organizational and Individual Measures of Engineering Success is an Opportunity to Drive Engineering Transformation",
    "latest": "2023-05-19T18:05:46+00:00",
    "last_post": {
      "url": "https://mastodon.social/@grimalkina/110396673029522948",
      "content": "<p>We've got a new preprint out!! We give an example of using our Three Layer approach to \"productivity metrics\" in an exercise with leaders, managers, and IC developers, and how it reveals important insights about where engineers disagree with their organizations and want to suggest better. </p><p><a href=\"https://arxiv.org/abs/2305.11030\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11030</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@grimalkina",
        "display_name": "Cat Hicks"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10400v1",
    "title": "What You See is What You Read? Improving Text-Image Alignment Evaluation",
    "latest": "2023-05-19T17:42:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110396581396918372",
      "content": "<p>\ud83d\udcdd What You See Is What You Read? Improving Text-Image Alignment Evaluation \ud83d\udcda\ud83d\udd2d</p><p>\"Generates a set of questions about the image, and then evaluate how well these questions are answered by a given text-image pair using a visual question answering model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10400v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10400v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10387v1",
    "title": "https://arxiv.org/abs/2305.10387v1",
    "latest": "2023-05-19T17:06:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110396439763513750",
      "content": "<p>\ud83d\udcdd Elaborative Simplification as Implicit Questions Under Discussion \ud83d\udcda</p><p>\"Elaborative simplification is viewed through the lens of the Question Under Discussion (QUD) framework, providing a robust way to investigate what writers elaborate upon, how they elaborate, and how elaborations fit into the discourse context.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/sheffwb/elabQUD\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/sheffwb/elabQUD</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10387v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10387v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.10429",
    "title": "http://arxiv.org/abs/2305.10429",
    "latest": "2023-05-19T17:03:59+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110396430066556118",
      "content": "<p>RT @sangmichaelxie@twitter.com</p><p>Should LMs train on more books, news, or web data?</p><p>Introducing DoReMi\ud83c\udfb6, which optimizes the data mixture with a small 280M model.</p><p>Our data mixture makes 8B Pile models train 2.6x faster, get +6.5% few-shot acc, and get lower pplx on *all* domains!</p><p>\ud83e\uddf5\u2b07\ufe0f <a href=\"http://arxiv.org/abs/2305.10429\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.10429</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/sangmichaelxie/status/1659230705165975552\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/sangmichaelxie/sta</span><span class=\"invisible\">tus/1659230705165975552</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.00924",
    "title": "HasChor: Functional Choreographic Programming for All (Functional Pearl)",
    "latest": "2023-05-19T16:21:44+00:00",
    "last_post": {
      "url": "https://recurse.social/@lindsey/110396194594060604",
      "content": "<p>Very happy to share that \"HasChor: Functional Choreographic Programming for All\", a functional pearl paper led by my students Gan Shen and Shun Kashiwa, has been accepted to ICFP 2023! \ud83c\udf89 <a href=\"https://arxiv.org/abs/2303.00924\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.00924</span><span class=\"invisible\"></span></a></p><p>A short thread about HasChor: \ud83d\udc47</p>"
    },
    "people": [
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10154v1",
    "title": "Iterated learning and communication jointly explain efficient color naming systems",
    "latest": "2023-05-19T09:54:26+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394741008903010",
      "content": "<p>\ud83d\udcdd Iterated Learning and Communication Jointly Explain Efficient Color Naming Systems \ud83d\udcda</p><p>\"A combination of iterated learning and communication yields systems efficient with respect to the Information Bottleneck principle, and similar to human color naming systems, while iterated learning or communication on their own do not.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10154v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10154v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10149v1",
    "title": "https://arxiv.org/abs/2305.10149v1",
    "latest": "2023-05-19T09:18:25+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394599390464470",
      "content": "<p>\ud83d\udcdd Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog \ud83d\udcda</p><p>\"Proposes to decouple knowledge retrieval from response generation and introduce a multi-grained knowledge retriever (MAKER) that includes an entity selector to search for relevant entities and an attribute selector to filter out irrelevant attributes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/18907305772/MAKER\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/18907305772/MAKER</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10149v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10149v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2211.01301",
    "title": "Computational Desire Line Analysis of Cyclists on the Dybb\u00f8lsbro Intersection in Copenhagen",
    "latest": "2023-05-19T08:31:12+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@underdarkGIS/110390854326449081",
      "content": "<p>Did you know that <span class=\"h-card\"><a href=\"https://fosstodon.org/@movingpandas\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>movingpandas</span></a></span> also supports local image coordinates? </p><p>Currently working on a demo using <a href=\"https://fosstodon.org/tags/bicycle\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>bicycle</span></a> tracks extracted from <a href=\"https://fosstodon.org/tags/video\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>video</span></a> footage \ud83d\udeb4\ud83c\udfa5   published by <span class=\"h-card\"><a href=\"https://datasci.social/@mszll\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>mszll</span></a></span> </p><p><a href=\"https://arxiv.org/abs/2211.01301\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2211.01301</span><span class=\"invisible\"></span></a></p><p><a href=\"https://fosstodon.org/tags/openscience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>openscience</span></a> <a href=\"https://fosstodon.org/tags/MobilityAnalytics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MobilityAnalytics</span></a> <a href=\"https://fosstodon.org/tags/MobilityDataScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MobilityDataScience</span></a> <a href=\"https://fosstodon.org/tags/mobility\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>mobility</span></a> <a href=\"https://fosstodon.org/tags/MovingPandas\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MovingPandas</span></a> <a href=\"https://fosstodon.org/tags/TrafficFlow\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>TrafficFlow</span></a> <a href=\"https://fosstodon.org/tags/planning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>planning</span></a> <a href=\"https://fosstodon.org/tags/cycling\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>cycling</span></a> <a href=\"https://fosstodon.org/tags/TrafficBehavior\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>TrafficBehavior</span></a></p>"
    },
    "people": [
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      },
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10142v1",
    "title": "https://arxiv.org/abs/2305.10142v1",
    "latest": "2023-05-19T08:30:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394410744349025",
      "content": "<p>\ud83d\udcdd Improving Language Model Negotiation with Self-Play and in-Context Learning From AI Feedback \ud83d\udcda</p><p>\"Uses different LLMs as the buyer, the seller, and the critic, respectively, in a negotiation game that aims to reach an agreement on a deal price for a product.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/FranxYao/GPT-Bargaining\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/FranxYao/GPT-Bargai</span><span class=\"invisible\">ning</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10142v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10142v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10122v1",
    "title": "https://arxiv.org/abs/2305.10122v1",
    "latest": "2023-05-19T08:06:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394316501401879",
      "content": "<p>\ud83d\udcdd Empirical Analysis of Oral and Nasal Vowels of Konkani \ud83d\udcda</p><p>\"Compares the acoustic-phonetic properties of Konkani oral and nasal vowels, which can be helpful for linguistic research on vowels and speech synthesis systems specific to the Konkani language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/shashwatup9k/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/shashwatup9k/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10122v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10122v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10601",
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "latest": "2023-05-19T07:10:50+00:00",
    "last_post": {
      "url": "https://hachyderm.io/@markcarter/110394077530832644",
      "content": "<p>\ud83e\udd14 Tree of Thoughts: Deliberate Problem Solving with Large Language Models. we introduce a new framework for language model inference, which enables exploration over coherent units of text (thoughts) that serve as steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking <a href=\"https://arxiv.org/abs/2305.10601\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10601</span><span class=\"invisible\"></span></a> <a href=\"https://hachyderm.io/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10096v1",
    "title": "Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots",
    "latest": "2023-05-19T06:54:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394033326123274",
      "content": "<p>\ud83d\udcdd Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots \ud83d\udcda\ud83d\udc7e</p><p>\"Consists of two modules: 1) a response emotion/intent prediction module; and 2) a response generation module which conditions on these predicted emotion/intents.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10096v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10096v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2302.05669",
    "title": "Treat societally impactful scientific insights as open-source software artifacts",
    "latest": "2023-05-19T06:44:32+00:00",
    "last_post": {
      "url": "https://akademienl.social/@cynthiacsliem/110391932047344538",
      "content": "<p>For any <a href=\"https://akademienl.social/tags/icse2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>icse2023</span></a> attendees/followers and enthusiasts about open science and academic reform: looking forward to presenting 'Treat societally impactful scientific insights as open-source software artifacts', a Software Engineering in Society position paper co-authored with my PhD Andrew Demetriou, this afternoon 15:52 h @ room 110. Promising to make it provocative \ud83d\ude07 </p><p>Pre-print: <a href=\"https://arxiv.org/abs/2302.05669\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2302.05669</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://dair-community.social/@DrVeronikaCH",
        "display_name": "Veronika Cheplygina"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10046v1",
    "title": "https://arxiv.org/abs/2305.10046v1",
    "latest": "2023-05-19T05:54:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393797469724681",
      "content": "<p>\ud83d\udcdd Probing the Role of Positional Information in Vision-Language Models \ud83d\udcda\ud83d\udd2d</p><p>\"Introduces two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PUses Cross-Modality Matching.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/airsplay/lxmert/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/airsplay/lxmert/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10046v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10046v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10037v1",
    "title": "https://arxiv.org/abs/2305.10037v1",
    "latest": "2023-05-19T05:06:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393608680789679",
      "content": "<p>\ud83d\udcdd Can Language Models Solve Graph Problems in Natural Language? \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes NLGraph, a comprehensive benchmark of graph-based problem solving designed in natural language to evaluate the graph reasoning abilities of large language models, and provide instruction-based approaches to improve their performance.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Arthur-Heng/NLGraph\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Arthur-Heng/NLGraph</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10037v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10037v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10036v1",
    "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
    "latest": "2023-05-19T04:30:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393467089995099",
      "content": "<p>\ud83d\udcdd Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark \ud83d\udcda</p><p>\"The Embedding Watermark (EmbMarker) implants a watermark into the embeddings of the EaaS model's training set to verify copyright infringement.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CY\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CY</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10036v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10036v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10013v1",
    "title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario",
    "latest": "2023-05-19T03:42:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393278468069950",
      "content": "<p>\ud83d\udcdd When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario \ud83d\udcda\ud83d\udc7e</p><p>\"GDFO integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonized manner through knowledge distillation, which can achieve significant performance gains over previous state-of-the-art methods.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10013v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10013v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10010v1",
    "title": "https://arxiv.org/abs/2305.10010v1",
    "latest": "2023-05-19T02:42:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393042469890056",
      "content": "<p>\ud83d\udcdd AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression \ud83d\udcda</p><p>\"Attribution-driven Knowledge Distillation for pre-trained language models (eBERT) is a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/brucewsy/AD-KD\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/brucewsy/AD-KD</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10010v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10010v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10005v1",
    "title": "DinoSR: Self-Distillation and Online Clustering for Self-supervised Speech Representation Learning",
    "latest": "2023-05-19T02:30:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110392995234831762",
      "content": "<p>\ud83d\udcdd DinoSR: Self-Distillation and Online Clustering for Self-Supervised Speech Representation Learning \ud83d\udcda</p><p>\"A combination of masked language modeling, self-distillation and online clustering which complement each other and result in a strong representation learning model for speech, outperforming existing methods on several downstream tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10005v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10005v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09990v1",
    "title": "Dual Semantic Knowledge Composed Multimodal Dialog Systems",
    "latest": "2023-05-19T01:42:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110392806578530974",
      "content": "<p>\ud83d\udcdd Dual Semantic Knowledge Composed Multimodal Dialog Systems \ud83d\udcda</p><p>\"Devises a novel multimodal task-oriented dialog system (MDS-S2) to address the limitations of previous works by introducing a multi-level knowledge composition module and a representation-level semantic regularization.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09990v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09990v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09955v1",
    "title": "https://arxiv.org/abs/2305.09955v1",
    "latest": "2023-05-19T01:30:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110392759361571305",
      "content": "<p>\ud83d\udcdd CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge \ud83d\udcda</p><p>\"Introduces specialized language models, autoregressive models trained on corpora from a wide range of domains and sources, that serve as parametric knowledge repositories that are later prompted to generate background knowledge for general-purpose LMs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/sdtblck/Opensubtitles_dataset\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/sdtblck/Opensubtitl</span><span class=\"invisible\">es_dataset</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09955v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09955v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09975v1",
    "title": "https://arxiv.org/abs/2305.09975v1",
    "latest": "2023-05-19T01:18:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110392712151923162",
      "content": "<p>\ud83d\udcdd Smart Word Suggestions for Writing Assistance \ud83d\udcda</p><p>\"Includes a large distantly supervised dataset for training and a human labeled dataset for testing, and the framework for evaluation is provided to the participants to evaluate their models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/microsoft/SmartWordSuggestions\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/SmartWord</span><span class=\"invisible\">Suggestions</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09975v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09975v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09898v1",
    "title": "https://arxiv.org/abs/2305.09898v1",
    "latest": "2023-05-19T00:30:26+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110392523252416412",
      "content": "<p>\ud83d\udcdd Balancing Lexical and Semantic Quality in Abstractive Summarization \ud83d\udcda</p><p>\"Proposes a novel training method in which a re-ranker balances the lexical and semantic quality of candidate summaries to overcome exposure bias in the abstractive summarization task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/jeewoo1025/BalSum\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/jeewoo1025/BalSum</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09898v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09898v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2207.14382",
    "title": "Large Language Models and the Reverse Turing Test",
    "latest": "2023-05-19T00:18:59+00:00",
    "last_post": {
      "url": "https://federate.social/@Roundtrip/110392478058300764",
      "content": "<p>\ud83e\uddf5After watching Nova \u2018Your Brain: Perception Deceptions\u2019 </p><p>For a brilliant and enjoyable essay on Large Language Models, human cognition, and intelligence by Terrence Sejnowski, Francis Crick Professor at the Salk Institute for Biological Studies where he directs the Computational Neurobiology Laboratory.</p><p><a href=\"https://federate.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://federate.social/tags/cognition\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>cognition</span></a> <a href=\"https://federate.social/tags/PBS\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PBS</span></a></p><p><a href=\"https://arxiv.org/abs/2207.14382\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2207.14382</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2202.08005",
    "title": "Should You Mask 15% in Masked Language Modeling?",
    "latest": "2023-05-18T23:46:55+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110392352124576647",
      "content": "<p>[R] Should You Mask 15% In Masked Language Modeling?</p><p><a href=\"https://arxiv.org/abs/2202.08005\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2202.08005</span><span class=\"invisible\"></span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://arxiv.org/abs/2202.08005\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://arxiv.org/a</span><span class=\"invisible\">bs/2202.08005</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09892v1",
    "title": "Clustering-Aware Negative Sampling for Unsupervised Sentence Representation",
    "latest": "2023-05-18T23:30:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110392287459580348",
      "content": "<p>\ud83d\udcdd Clustering-Aware Negative Sampling for Unsupervised Sentence Representation \ud83d\udcda\ud83d\udc7e</p><p>\"Works by applying a modified K-means clustering algorithm to supply hard negatives and recognize in-batch false negatives during training, aiming to solve the two issues in one unified framework.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09892v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09892v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.10777",
    "title": "Reddit in the Time of COVID",
    "latest": "2023-05-18T21:59:24.384000+00:00",
    "last_post": {
      "url": "https://calckey.social/notes/9ex8nvmol4",
      "content": "<p><span>An academic paper says that COVID-19 doubled the horniness of </span><a href=\"https://calckey.social/tags/Reddit\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#Reddit</a><span>.<br><br>Prior to the pandemic, NSFW subreddits accounted for 6.7% of all activity on Reddit. By the end of the pandemic, it accounted for 13%. <br><br></span>\ud83e\udd75<span><br><br></span><a href=\"https://arxiv.org/abs/2304.10777\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">https://arxiv.org/abs/2304.10777</a><span><br><br></span><a href=\"https://venera.social/profile/socialmedianews\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@socialmedianews@venera.social</a></p>"
    },
    "people": [
      {
        "url": "https://calckey.social/@atomicpoet",
        "display_name": "Chris Trottier"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09864v1",
    "title": "The Jaseci Programming Paradigm and Runtime Stack: Building Scale-out Production Applications Easy and Fast",
    "latest": "2023-05-18T21:42:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110391862751321072",
      "content": "<p>\ud83d\udcdd The Jaseci Programming Paradigm and Runtime Stack: Building Scale-Out Production Applications Easy and Fast \ud83d\udcda</p><p>\"The programming language Jac is designed to express the logic of a scale-out application as a directed graph of components that communicate via remote procedure calls (RPCs).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/DC\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DC</span></a> <a href=\"https://creative.ai/tags/PL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09864v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09864v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09860v1",
    "title": "Epsilon Sampling Rocks: Investigating Sampling Strategies for \\\\Minimum Bayes Risk Decoding for Machine Translation",
    "latest": "2023-05-18T21:18:25+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110391768229272378",
      "content": "<p>\ud83d\udcdd Epsilon Sampling Rocks: Investigating Sampling Strategies for \\\\Minimum Bayes Risk Decoding for Machine Translation \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"MBR decoding with epsilon-sampling prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass and resulting in significantly better translations, according to human evaluations, across four language pairs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09860v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09860v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09859v1",
    "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
    "latest": "2023-05-18T20:30:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110391579768579885",
      "content": "<p>\ud83d\udcdd Smaller Language Models Are Better Black-Box Machine-Generated Text Detectors \ud83d\udcda\ud83e\udde0</p><p>\"Based on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not and is instead scattered across the probability space.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09859v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09859v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10425",
    "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback",
    "latest": "2023-05-18T20:17:39+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110391529243371223",
      "content": "<p>RT @peterjliu@twitter.com</p><p>Here is our \u201cslick\u201d RLHF-alternative without RL: <a href=\"https://arxiv.org/abs/2305.10425\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10425</span><span class=\"invisible\"></span></a> (SLiC-HF)</p><p>TL;DR: Works as well as RLHF, but a lot simpler.</p><p>About as easy and efficient as fine-tuning. Much better than simply fine-tuning on good examples.</p><p>From great collaborators: @yaozhaoai@twitter.com,\u2026 <a href=\"https://twitter.com/i/web/status/1659023597447565312\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/i/web/status/16590</span><span class=\"invisible\">23597447565312</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/peterjliu/status/1659023597447565312\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/peterjliu/status/1</span><span class=\"invisible\">659023597447565312</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09857v1",
    "title": "https://arxiv.org/abs/2305.09857v1",
    "latest": "2023-05-18T19:18:26+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110391296431859111",
      "content": "<p>\ud83d\udcdd CoEdIT: Text Editing by Task-Specific Instruction Tuning \ud83d\udcda\ud83d\udc7e</p><p>\"CoEdIT takes as input the original text along with an edit instruction, and it outputs the desired edit to the original text that satisfies the input instruction by fine-tuning a large language model on a diverse collection of edit instructions.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/vipulraheja/coedit\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/vipulraheja/coedit</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09857v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09857v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09785v1",
    "title": "Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models",
    "latest": "2023-05-18T18:30:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110391107720947228",
      "content": "<p>\ud83d\udcdd Distilling Semantic Concept Embeddings From Contrastively Fine-Tuned Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"Trains a language model on a general corpus, and use it for predicting properties of concepts, given their mentions in some corpus (e,g, scientific texts).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09785v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09785v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://onlinelibrary.wiley.com/doi/10.1002/ece3.10017",
    "title": "onlinelibrary.wiley.com/doi/10...",
    "latest": "2023-05-18T18:25:58+00:00",
    "last_post": {
      "url": "https://journa.host/@w7voa/110391090141382830",
      "content": "<p>Imagery published today indicates the ivory-billed woodpeckers might not be extinct.  <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/ece3.10017\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">onlinelibrary.wiley.com/doi/10</span><span class=\"invisible\">.1002/ece3.10017</span></a> <a href=\"https://journa.host/tags/Louisiana\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Louisiana</span></a></p>"
    },
    "people": [
      {
        "url": "https://journa.host/@w7voa",
        "display_name": "Steve Herman"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09781v1",
    "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification",
    "latest": "2023-05-18T17:54:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110390966189089720",
      "content": "<p>\ud83d\udcdd SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification \ud83d\udcda\ud83e\udde0</p><p>\"The token tree verifies the correctness of each candidate token sequence in parallel using a novel tree-based parallel decoding mechanism and returns the one with the highest confidence score as the final output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/DC\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DC</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09781v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09781v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09731v1",
    "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
    "latest": "2023-05-18T17:18:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110390824789282464",
      "content": "<p>\ud83d\udcdd What in-Context Learning \"Learns\" in-Context: Disentangling Task Recognition and Task Learning \ud83d\udcda\ud83e\udde0</p><p>\"Task recognition (TR) captures the extent to which models can recognize a task through demonstrations -- even without ground-truth labels -- and apply their pre-trained priors, Whereas, Task learning (TL) is the ability to capture new input-label mappings unseen in pre-training.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09731v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09731v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05415",
    "title": "https://doi.org/10.21105/joss.05415",
    "latest": "2023-05-18T17:07:06+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110390779995285435",
      "content": "<p>Just published in JOSS: 'xclim: xarray-based climate data analytics' <a href=\"https://doi.org/10.21105/joss.05415\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05415</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09656",
    "title": "Satisfiability-Aided Language Models Using Declarative Prompting",
    "latest": "2023-05-17T10:00:07+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110383438352403237",
      "content": "<p>Satisfiability-aided language models using declarative prompting. ~ Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett. <a href=\"https://arxiv.org/abs/2305.09656\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09656</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a> <a href=\"https://mathstodon.xyz/tags/SAT_Solver\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SAT_Solver</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.18120",
    "title": "UKP-SQuARE v3: A Platform for Multi-Agent QA Research",
    "latest": "2023-05-17T09:06:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@UKPLab/110383227279474106",
      "content": "<p>\"UKP-SQuARE v3: A Platform for Multi-Agent QA Research\" was accepted at <a href=\"https://sigmoid.social/tags/acl_2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>acl_2023</span></a> \ud83c\udde8\ud83c\udde6! Read more about UKP-SQuARE's amazing pre-print on Arxiv: <a href=\"https://arxiv.org/abs/2303.18120\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.18120</span><span class=\"invisible\"></span></a> or in this birdsite \ud83e\uddf5: <a href=\"https://twitter.com/UKP_SQuARE/status/1658433479011364864\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/UKP_SQuARE/status/</span><span class=\"invisible\">1658433479011364864</span></a> <a href=\"https://sigmoid.social/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@UKPLab",
        "display_name": "UKP Lab"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07895v1",
    "title": "https://arxiv.org/abs/2305.07895v1",
    "latest": "2023-05-17T08:32:52+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110383095638673629",
      "content": "<p>\ud83d\udcdd On the Hidden Mystery of OCR in Large Multimodal Models \ud83d\udd2d\ud83d\udcda</p><p>\"Conducts a comprehensive study of publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction from images with text.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Yuliang-Liu/MultimodalOCR\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/Yuliang-Liu/Multimo</span><span class=\"invisible\">dalOCR</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.07895v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07895v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08208v1",
    "title": "https://arxiv.org/abs/2305.08208v1",
    "latest": "2023-05-17T08:08:53+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110383001325760159",
      "content": "<p>\ud83d\udcdd Learning to Generalize for Cross-Domain QA \ud83d\udcda\ud83d\udc7e</p><p>\"First initialized with the output weights of a trained prompt-based classifier on source data and then fine-tuned on the target data, which is the linear probing then finetuning strategy.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/FreddieNIU/Prompt-QA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/FreddieNIU/Prompt-Q</span><span class=\"invisible\">A</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08208v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08208v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s00381-023-05969-2",
    "title": "Global trends, gaps, and future agenda in medulloblastoma research: a bibliometric analysis - Child's Nervous System",
    "latest": "2023-05-17T07:51:43+00:00",
    "last_post": {
      "url": "https://fediscience.org/@petersuber/110378942822124751",
      "content": "<p>Another article made it through peer review (<a href=\"https://fediscience.org/tags/SpringerNature\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SpringerNature</span></a>) with the false claim that all <a href=\"https://fediscience.org/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> journals charge <a href=\"https://fediscience.org/tags/APCs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>APCs</span></a>. <br><a href=\"https://link.springer.com/article/10.1007/s00381-023-05969-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s00381-023-05969-2</span></a></p><p>Reminder: Only a minority (\u2248 31%) of OA journals charge APCs, even if a majority of articles pub'd in OA journals are in the APC-based variety.<br><a href=\"https://fediscience.org/@petersuber/109344076065105780\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">fediscience.org/@petersuber/10</span><span class=\"invisible\">9344076065105780</span></a></p><p><a href=\"https://fediscience.org/tags/DiamondOA\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DiamondOA</span></a> <a href=\"https://fediscience.org/tags/GoldOA\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GoldOA</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@edzer",
        "display_name": "Edzer Pebesma"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08195v1",
    "title": "Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing",
    "latest": "2023-05-17T07:32:53+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110382859770814252",
      "content": "<p>\ud83d\udcdd Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing \ud83d\udcda</p><p>\"A novel framework for simulating natural language feedback for interactive semantic parsing, which can be used to improve the error correction ability of semantic parsers in low-data regimes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08195v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08195v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.3115/981732.981734",
    "title": "https://doi.org/10.3115/981732.981734",
    "latest": "2023-05-17T07:20:03+00:00",
    "last_post": {
      "url": "https://mastodon.social/@stacked_automation/110382809286637516",
      "content": "<p>Multi-paragraph segmentation of expository text<br>(1994) : Hearst, Marti A<br>DOI: <a href=\"https://doi.org/10.3115/981732.981734\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.3115/981732.981734</span><span class=\"invisible\"></span></a><br><a href=\"https://mastodon.social/tags/discourse\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>discourse</span></a> <a href=\"https://mastodon.social/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://mastodon.social/tags/text_tiling\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>text_tiling</span></a> <a href=\"https://mastodon.social/tags/algorithm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>algorithm</span></a> <a href=\"https://mastodon.social/tags/my_bibtex\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>my_bibtex</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@stacked_automation",
        "display_name": "nope"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08187v1",
    "title": "CroSentiNews 2.0: A Sentence-Level News Sentiment Corpus",
    "latest": "2023-05-17T06:44:50+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110382670822517349",
      "content": "<p>\ud83d\udcdd CroSentiNews 2.0: A Sentence-Level News Sentiment Corpus \ud83d\udcda</p><p>\"Annotators labeled sentence sentiment on a 5-point scale ranging from negative to extreme positive sentiment with a neutral point of zero in between and a half unit on either side (-2, -1, 0, 1, 2).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08187v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08187v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.01157",
    "title": "Promoting Bright Patterns",
    "latest": "2023-05-17T06:42:14+00:00",
    "last_post": {
      "url": "https://akademienl.social/@Frederik_Borgesius/110356434554554761",
      "content": "<p>'Promoting Bright Patterns', website &amp; paper by <span class=\"h-card\"><a href=\"https://mastodon.social/@sandhaus\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>sandhaus</span></a></span> <a href=\"https://arxiv.org/abs/2304.01157\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.01157</span><span class=\"invisible\"></span></a><br><a href=\"https://brightpatterns.org\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">brightpatterns.org</span><span class=\"invisible\"></span></a> <br><a href=\"https://akademienl.social/tags/darkpatterns\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>darkpatterns</span></a> <a href=\"https://akademienl.social/tags/adtech\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>adtech</span></a> <a href=\"https://akademienl.social/tags/advertising\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>advertising</span></a> <a href=\"https://akademienl.social/tags/law\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>law</span></a> <a href=\"https://akademienl.social/tags/tech\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>tech</span></a> <a href=\"https://akademienl.social/tags/ai\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ai</span></a> <a href=\"https://akademienl.social/tags/manipulation\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>manipulation</span></a></p>"
    },
    "people": [
      {
        "url": "https://idf.social/@arjen",
        "display_name": "Arjen P. de Vries Timmers \ud83d\udd4a\ufe0f"
      }
    ]
  },
  {
    "link": "http://osf.io/fdh5b/",
    "title": "http://osf.io/fdh5b/",
    "latest": "2023-05-17T06:31:46+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mzloteanu/110379360504187390",
      "content": "<p>\ud83d\udea8New preprint on <a href=\"https://mastodon.social/tags/deception\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deception</span></a> detection analysis \ud83d\udd0d We provide a tutorial on <a href=\"https://mastodon.social/tags/Bayesian\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Bayesian</span></a> Mixed Effects Models for veracity data; no more aggregating &amp; converting data to % \ud83d\ude24 (conflating acc w/ bias), just model the lie/truth answers directly! Bonus: they are SDT models \ud83e\uddd0 w/ <span class=\"h-card\"><a href=\"https://bayes.club/@matti\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>matti</span></a></span> </p><p>[link below] <br>Bayesian Generalized Linear Mixed Effects Models for Deception Detection Analyses <a href=\"http://osf.io/fdh5b/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/fdh5b/</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.social/tags/Bayesian\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Bayesian</span></a> <a href=\"https://mastodon.social/tags/glmm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>glmm</span></a> <a href=\"https://mastodon.social/tags/MixedEffects\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MixedEffects</span></a> <a href=\"https://mastodon.social/tags/statistics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>statistics</span></a> <a href=\"https://mastodon.social/tags/methods\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>methods</span></a> <a href=\"https://mastodon.social/tags/signaldetectiontheory\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>signaldetectiontheory</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@mjskay",
        "display_name": "Matthew Kay"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08173v1",
    "title": "Croatian Film Review Dataset (Cro-FiReDa): A Sentiment Annotated Dataset of Film Reviews",
    "latest": "2023-05-17T05:44:50+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110382434880198791",
      "content": "<p>\ud83d\udcdd Croatian Film Review Dataset (Cro-FiReDa): A Sentiment Annotated Dataset of Film Reviews \ud83d\udcda</p><p>\"Croatian movie reviews dataset for fine-grained sentiment analysis (CroFiReDa) is a sentiment- annotated dataset for Croatian in the domain of movie reviews.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08173v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08173v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08152v1",
    "title": "STORYWARS: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation",
    "latest": "2023-05-17T04:56:51+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110382246209794190",
      "content": "<p>\ud83d\udcdd STORYWARS: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation \ud83d\udcda</p><p>\"A multi-task benchmark for collaborative story understanding and generation consisting of 101 diverse tasks covering all fully-supervised, few-shot, and zero-shot scenarios.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08152v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08152v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://mstdn.social/@arnicas",
        "display_name": "arnicas"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.31235/osf.io/nug4p",
    "title": "https://doi.org/10.31235/osf.io/nug4p",
    "latest": "2023-05-17T04:44:51+00:00",
    "last_post": {
      "url": "https://social.cwts.nl/@LudoWaltman/110380029117249271",
      "content": "<p>I have written a review of a preprint that presents a review of the literature on the use of preprints!</p><p>The preprint by Amanda Blatch-Jones, Alejandra Recio Saucedo and Beth Giddins: <a href=\"https://doi.org/10.31235/osf.io/nug4p\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.31235/osf.io/nug4p</span><span class=\"invisible\"></span></a></p><p>My review of this preprint: <a href=\"https://ludowaltman.pubpub.org/pub/review-scoping-review-preprints/release/2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">ludowaltman.pubpub.org/pub/rev</span><span class=\"invisible\">iew-scoping-review-preprints/release/2</span></a></p><p><span class=\"h-card\"><a href=\"https://mas.to/@ASAPbio\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>ASAPbio</span></a></span> <a href=\"https://social.cwts.nl/tags/PublishYourReviews\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PublishYourReviews</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08146v1",
    "title": "ParaLS: Lexical Substitution via Pretrained Paraphraser",
    "latest": "2023-05-17T04:32:50+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110382151814420626",
      "content": "<p>\ud83d\udcdd ParaLS: Lexical Substitution via Pretrained Paraphraser \ud83d\udcda\ud83d\udc7e</p><p>\"Generates substitute candidates via two simple decoding strategies, which focus on the variations of the target word during decoding, using pre-trained models as a paraphraser.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08146v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08146v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08135v1",
    "title": "Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering",
    "latest": "2023-05-17T04:08:51+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110382057507243570",
      "content": "<p>\ud83d\udcdd Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering \ud83d\udcda\ud83d\udc7e</p><p>\"We retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module and generate corresponding contrastive explanations using acquired symbolic knowledge and explanation prompts as guidance for better modeling the knowledge distinguishment and interpretability.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08135v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08135v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/10.1145/1978942.1979452",
    "title": "Computers can't give credit | Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
    "latest": "2023-05-17T03:49:37+00:00",
    "last_post": {
      "url": "https://hci.social/@andresmh/110381981848239247",
      "content": "<p>also, reminded me of this paper <a href=\"https://dl.acm.org/doi/10.1145/1978942.1979452\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/10.1145/1978942</span><span class=\"invisible\">.1979452</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@andresmh",
        "display_name": "Andr\u00e9s Monroy-Hern\u00e1ndez"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08096v1",
    "title": "https://arxiv.org/abs/2305.08096v1",
    "latest": "2023-05-17T03:20:53+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110381868851165874",
      "content": "<p>\ud83d\udcdd Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a novel method named Top-1 Information Enhanced Knowledge Distillation (TIE-KD), which aims to address two limitations of the vanilla word-level KD method: (1) lacking the special treatment on the crucial top-1 information.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Unbabel/COMET\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Unbabel/COMET</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08096v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08096v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1353/bh.2023.0006",
    "title": "https://doi.org/10.1353/bh.2023.0006",
    "latest": "2023-05-17T02:38:03+00:00",
    "last_post": {
      "url": "https://mastodon.social/@agoldst/110380112615062651",
      "content": "<p>My article, \"Origins of the U.S. Genre-Fiction System, 1890\u20131956\" is now out in Book History.</p><p>Published version here: </p><p><a href=\"https://doi.org/10.1353/bh.2023.0006\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.1353/bh.2023.0006</span><span class=\"invisible\"></span></a></p><p>Open-access preprint:</p><p><a href=\"https://andrewgoldstone.com/research/bh2023ms.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">andrewgoldstone.com/research/b</span><span class=\"invisible\">h2023ms.pdf</span></a></p><p>If you want to know what's so special about 1956, you'll have to read the essay, or at least the blog post:</p><p><a href=\"https://andrewgoldstone.com/blog/origins-article/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">andrewgoldstone.com/blog/origi</span><span class=\"invisible\">ns-article/</span></a></p>"
    },
    "people": [
      {
        "url": "https://hcommons.social/@librarifran",
        "display_name": "franny gaede"
      },
      {
        "url": "https://mastodon.social/@jose_eduardo",
        "display_name": "Jos\u00e9 Eduardo Gonz\u00e1lez"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07759v1",
    "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
    "latest": "2023-05-17T02:33:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110378943510595394",
      "content": "<p>\ud83d\udcdd TinyStories: How Small Can Language Models Be and Still Speak Coherent English? \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"TinyStory is a synthetic dataset of stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3 and GPT-4.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.07759v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07759v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@nichg",
        "display_name": "Nicholas Guttenberg"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08088v1",
    "title": "https://arxiv.org/abs/2305.08088v1",
    "latest": "2023-05-17T02:20:50+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110381632774706853",
      "content": "<p>\ud83d\udcdd Make Prompt-Based Black-Box Tuning Colorful: Boosting Model Generalization From Three Orthogonal Perspectives \ud83d\udcda\ud83d\udc7e</p><p>\"Works by improving the initialization, searching for better prompts and using a novel verbalizer construction method to automatically generate labels from input texts for few shot settings for gradient free optimization of task specific language models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/QiushiSun/BBT-RGB\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/QiushiSun/BBT-RGB</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.08088v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08088v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07991v1",
    "title": "https://arxiv.org/abs/2305.07991v1",
    "latest": "2023-05-17T01:56:51+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110381538411792497",
      "content": "<p>\ud83d\udcdd Multilingual Previously Fact-Checked Claim Retrieval \ud83d\udcda</p><p>\"Collects 28k posts in 27 languages from social media, 206k fact-checks in 39 languages written by professional fact-checkers, as well as 31k connections between these two groups.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/google/cld3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/google/cld3</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.07991v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07991v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  }
]