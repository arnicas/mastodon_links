[
  {
    "link": "https://arxiv.org/abs/2207.13665",
    "title": "https://arxiv.org/abs/2207.13665",
    "latest": "2023-05-26T21:49:48+00:00",
    "last_post": {
      "url": "https://social.cwts.nl/@vtraag/110437190096995130",
      "content": "<p><span class=\"h-card\"><a href=\"https://colliderbias.net/@conjugateprior\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>conjugateprior</span></a></span> <span class=\"h-card\"><a href=\"https://social.cwts.nl/@LudoWaltman\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>LudoWaltman</span></a></span> </p><p>1. Completely agree! This is something that is more challenging to cover in a short blog post though. We do cover it in the paper <a href=\"https://arxiv.org/abs/2207.13665\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2207.13665</span><span class=\"invisible\"></span></a> .</p><p>2. Yes, the distinction is something that is clearly relevant. I would be interested in understanding better how this would be relevant to the legal side of things. Would you have a reference?</p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://onlinelibrary.wiley.com/doi/full/10.1002/leap.1545",
    "title": "onlinelibrary.wiley.com/doi/fu...",
    "latest": "2023-05-26T21:21:10+00:00",
    "last_post": {
      "url": "https://fediscience.org/@petersuber/110435466438023084",
      "content": "<p>New study on the state of academic <a href=\"https://fediscience.org/tags/publishing\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>publishing</span></a> in <a href=\"https://fediscience.org/tags/China\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>China</span></a>. <br><a href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/leap.1545\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">onlinelibrary.wiley.com/doi/fu</span><span class=\"invisible\">ll/10.1002/leap.1545</span></a></p><p>\"China now has more researchers than the US, outspends the US and EU in research and development and publishes more scientific papers each year than any other nation in the world\u2026 China continues to have problems with research integrity and has the highest number of retractions of any country due to plagiarism, invented data and fake peer review.\"</p>"
    },
    "people": [
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      }
    ]
  },
  {
    "link": "https://journals.sagepub.com/doi/10.1177/00380261221146403",
    "title": "journals.sagepub.com/doi/10.11...",
    "latest": "2023-05-26T21:08:19+00:00",
    "last_post": {
      "url": "https://aoir.social/@tanyakant1/110437014570793662",
      "content": "<p>It's Friday night UK time and I'm getting Twitter/ Mastodon notifications which can only mean <span class=\"h-card\"><a href=\"https://aoir.social/@sophiebishop\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>sophiebishop</span></a></span> has presented our paper on 'Algorithmic autobiographies and fictions' at <a href=\"https://aoir.social/tags/ica23\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ica23</span></a> - Major fomo at missing the conference but thanks to sophie for representing! check out the journal article the research here: <a href=\"https://journals.sagepub.com/doi/10.1177/00380261221146403\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.sagepub.com/doi/10.11</span><span class=\"invisible\">77/00380261221146403</span></a></p>"
    },
    "people": [
      {
        "url": "https://aoir.social/@nancybaym",
        "display_name": "Nancy Baym"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15507v1",
    "title": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",
    "latest": "2023-05-26T21:07:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110437023842436432",
      "content": "<p>\ud83d\udcdd The Larger They Are, the Harder They Fail: Language Models Do Not Recognize Identifier Swaps in Python \ud83d\udcda\ud83d\udc7e</p><p>\"Fails to properly generate correct Python code when default function names are swapped, and some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.15507v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15507v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15501v1",
    "title": "Deriving Language Models from Masked Language Models",
    "latest": "2023-05-26T20:07:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110436787875769741",
      "content": "<p>\ud83d\udcdd Deriving Language Models From Masked Language Models \ud83d\udcda</p><p>\"An approach based on identifying joints whose conditionals are closest to those of the MLM works well and outperforms existing Markov random field-based approaches, and the derived model's conditionals can even occasionally outperform the original MLM's conditionals.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.15501v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15501v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.16291",
    "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
    "latest": "2023-05-26T19:47:28+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110436709065007613",
      "content": "<p>Voyager: An LLM-powered learning agent in Minecraft</p><p><a href=\"https://arxiv.org/abs/2305.16291\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.16291</span><span class=\"invisible\"></span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://arxiv.org/abs/2305.16291\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://arxiv.org/a</span><span class=\"invisible\">bs/2305.16291</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15444v1",
    "title": "PromptNER: Prompting For Named Entity Recognition",
    "latest": "2023-05-26T19:07:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110436551853831539",
      "content": "<p>\ud83d\udcdd PromptNER: Prompting for Named Entity Recognition \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"In the case of few-shot NER, given a sentence and a set of few-shot examples, PromptNER prompts an LLM to fill in the blanks in the sentence, where each blank is associated with a different entity type.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.15444v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15444v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14497",
    "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
    "latest": "2023-05-26T19:01:25+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110435814692996266",
      "content": "<p>Self-Polish: Enhance reasoning in Large Language Models via problem refinement. ~ Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, Xuanjing Huang. <a href=\"https://arxiv.org/abs/2305.14497\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14497</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a> <a href=\"https://mathstodon.xyz/tags/Reasoning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Reasoning</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11089",
    "title": "Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces",
    "latest": "2023-05-26T18:39:36+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110436442230371852",
      "content": "<p>RT @dblueeye@twitter.com</p><p>Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces</p><p><a href=\"https://arxiv.org/abs/2305.11089\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11089</span><span class=\"invisible\"></span></a></p><p>Accepted by <a href=\"https://sigmoid.social/tags/ICML2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ICML2023</span></a> @javier_e_santos@twitter.com @zachrfox@twitter.com  @LosAlamosNatLab@twitter.com </p><p>\ud83e\uddf5</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/dblueeye/status/1661758526698700801\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/dblueeye/status/16</span><span class=\"invisible\">61758526698700801</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15525",
    "title": "Large Language Models are Few-Shot Health Learners",
    "latest": "2023-05-26T18:39:33+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110436441994201623",
      "content": "<p>Large Language Models are Few-Shot Health Learners</p><p>Shows that a LLM is capable of grounding various physiological and behavioral time-series data and making meaningful inferences on numerous health tasks for both clinical and wellness contexts.</p><p><a href=\"https://arxiv.org/abs/2305.15525\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15525</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15486",
    "title": "SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning",
    "latest": "2023-05-26T18:39:30+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110436441830387395",
      "content": "<p>SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning</p><p>Outperforms all SotA RL baselines, trained for 1M steps, without any training.</p><p><a href=\"https://arxiv.org/abs/2305.15486\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15486</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2301.07755",
    "title": "Optimal Transport for Counterfactual Estimation: A Method for Causal Inference",
    "latest": "2023-05-26T18:02:56+00:00",
    "last_post": {
      "url": "https://mastodon.social/@freakonometrics/110436298008424628",
      "content": "<p>\"Optimal Transport for Counterfactual Estimation: A Method for Causal Inference\" <a href=\"https://arxiv.org/abs/2301.07755\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2301.07755</span><span class=\"invisible\"></span></a> (codes and pictures <a href=\"http://egallic.fr/Recherche/Transport_Counterfactual/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"ellipsis\">egallic.fr/Recherche/Transport</span><span class=\"invisible\">_Counterfactual/</span></a>)</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@freakonometrics",
        "display_name": "Arthur Charpentier"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09620",
    "title": "AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys",
    "latest": "2023-05-26T17:41:14+00:00",
    "last_post": {
      "url": "https://sciences.social/@bklee/110430032087151669",
      "content": "<p>Our AI-Augmented survey shows a decline in moral opposition to abortion in the US by predicting trends from a question asked only once! Discover hidden insights from General Social Survey on our demo site. </p><p>\ud83e\udd16Demo: <a href=\"https://augmented-surveys-retrodict.hf.space\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">augmented-surveys-retrodict.hf</span><span class=\"invisible\">.space</span></a><br>\ud83d\udcc4Paper: <a href=\"https://arxiv.org/abs/2305.09620\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09620</span><span class=\"invisible\"></span></a></p><p>Currently, we\u2019re refining our model to incorporate the 2022 GSS survey data and additional GSS variables. Stay tuned for updates. </p><p>We would love to hear your feedback!</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.12544.pdf",
    "title": "https://arxiv.org/pdf/2305.12544.pdf",
    "latest": "2023-05-26T17:15:48+00:00",
    "last_post": {
      "url": "https://mastodon.social/@MattHodges/110436112705799042",
      "content": "<p>\"Generative models are designed to serve users end-to-end through natural language, and often the need for customizing these large models is not addressed due to high fine-tuning costs or proprietary technology. In the absence of expert or fine-tuned <a href=\"https://mastodon.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a>, the applications of such models in [Computational Social Science] remain limited to generic data labeling and processing such as stance detection or sentiment analysis.\"<br><a href=\"https://arxiv.org/pdf/2305.12544.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.12544.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@MattHodges",
        "display_name": "Matt Hodges"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14779v1",
    "title": "https://arxiv.org/abs/2305.14779v1",
    "latest": "2023-05-26T17:01:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110436056743666208",
      "content": "<p>\ud83d\udcdd Text Conditional Alt-Text Generation for Twitter Images \ud83d\udd2d\ud83d\udcda\ud83e\udde0</p><p>\"A CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'' which is concatenated with the text from the tweet itself.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/NikitaSrivatsan/AltText\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/NikitaSrivatsan/Alt</span><span class=\"invisible\">Text</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14779v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14779v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01732-4",
    "title": "Gravitational-wave detector LIGO is back \u2014 and can now spot more colliding black holes than ever",
    "latest": "2023-05-26T17:00:04+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110436050851441393",
      "content": "<p>Gravitational-wave detector LIGO is back - <a href=\"https://www.nature.com/articles/d41586-023-01732-4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01732-4</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14577v1",
    "title": "Difference-Masking: Choosing What to Mask in Continued Pretraining",
    "latest": "2023-05-26T16:41:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110435978232286519",
      "content": "<p>\ud83d\udcdd Difference-Masking: Choosing What to Mask in Continued Pretraining \ud83e\udde0\ud83d\udcda</p><p>\"Proposes to automatically choose what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain, using a learned representation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14577v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14577v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14521v1",
    "title": "https://arxiv.org/abs/2305.14521v1",
    "latest": "2023-05-26T16:21:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110435899629666375",
      "content": "<p>\ud83d\udcdd Eliminating Spurious Correlations From Pre-Trained Models via Data Mixing \ud83e\udde0\ud83d\udcda\ud83d\udd2d</p><p>\"Works by balancing the spurious attributes across all classes, which is achieved by mixing the examples with spurious attributes across all classes and then fine-tuning the pre-trained model on the mixed data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/singlasahil14/barlow\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/singlasahil14/barlo</span><span class=\"invisible\">w</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14521v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14521v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14795v1",
    "title": "https://arxiv.org/abs/2305.14795v1",
    "latest": "2023-05-26T16:01:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110435820871963824",
      "content": "<p>\ud83d\udcdd MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions \ud83d\udcda</p><p>\"MeLLo is a novel approach for Knowledge Editing that stores all edited facts externally while iteratively generating answers that are consistent with the edited facts and the language model's existing knowledge.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/princeton-nlp/MQuAKE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/princeton-nlp/MQuAK</span><span class=\"invisible\">E</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14795v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14795v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14380v1",
    "title": "Finding the Pillars of Strength for Multi-Head Attention",
    "latest": "2023-05-26T15:51:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110435781600032325",
      "content": "<p>\ud83d\udcdd Finding the Pillars of Strength for Multi-Head Attention \ud83e\udde0\ud83d\udcda</p><p>\"Grouped Head Attention uses a self-supervised group constraint to group attention heads, where each group focuses on an essential but distinctive feature subset, and thus mitigates redundancy and over-parameterization issues of Multi-Head Attention.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14380v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14380v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15717",
    "title": "The False Promise of Imitating Proprietary LLMs",
    "latest": "2023-05-26T15:47:11+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110435764260232881",
      "content": "<p>[R] The False Promise of Imitating Proprietary LLMs</p><p><a href=\"https://arxiv.org/abs/2305.15717\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15717</span><span class=\"invisible\"></span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://arxiv.org/abs/2305.15717\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://arxiv.org/a</span><span class=\"invisible\">bs/2305.15717</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14793v1",
    "title": "https://arxiv.org/abs/2305.14793v1",
    "latest": "2023-05-26T15:41:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110435742281020050",
      "content": "<p>\ud83d\udcdd Faithful Low-Resource Data-to-Text Generation Through Cycle Training \ud83d\udcda</p><p>\"A training strategy for the data-to-text generation task that involves training on two inverse tasks: (1) natural language text to structured data (NLG), and (2) structured data to natural language text (D2T).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Edillower/CycleNLG\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Edillower/CycleNLG</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14793v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14793v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14794v1",
    "title": "Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification",
    "latest": "2023-05-26T15:31:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110435702956857302",
      "content": "<p>\ud83d\udcdd Debiasing Made State-of-the-Art: Revisiting the Simple Seed-Based Weak Supervision for Text Classification \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Arguably the simplest way to generate pseudo labels and its power was greatly underestimated before; simply delete the seed words present in the matched input texts can mitigate the label bias and help learn better confidence; subsequently, its performance can be improved significantly.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14794v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14794v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.16183",
    "title": "https://arxiv.org/abs/2305.16183",
    "latest": "2023-05-26T10:02:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@lampinen/110434408282643873",
      "content": "<p>See paper for more results+discussion. Two quick points:<br>1) These results may have interesting implications for \u201cagency.\u201d<br>2) These results *certainly* do not imply that passive learning is better than active (cf. <a href=\"https://twitter.com/hausman_k/status/1661037780049494017?s=20\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/hausman_k/status/1</span><span class=\"invisible\">661037780049494017?s=20</span></a>), or that confounding is not a problem. </p><p>Thanks for making it through this thread! And thanks to my co-authors: <br><span class=\"h-card\"><a href=\"https://beta.mstdn.cf/users/scychan_brains\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>scychan_brains</span></a></span> , Ishita Dasgupta,  <br>Andrew Nam, and Jane Wang<br> (but any mistakes are no doubt mine). Paper link again: <a href=\"https://arxiv.org/abs/2305.16183\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.16183</span><span class=\"invisible\"></span></a> 7/7</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@lampinen",
        "display_name": "Andrew Lampinen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14735v1",
    "title": "Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection",
    "latest": "2023-05-26T09:51:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110434366045082821",
      "content": "<p>\ud83d\udcdd Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Develops a Group-based Performance Disparity Index (GPDI) that measures the extent to which a division of data into subgroups identifies those facing increased harms.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14735v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14735v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14734v1",
    "title": "Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation",
    "latest": "2023-05-26T09:31:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110434287274933881",
      "content": "<p>\ud83d\udcdd Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation \ud83d\udcda</p><p>\"Proposes the use of GED information as auxiliary input in GEC models to improve performance on Arabic GEC by 0,75 F1 points on average.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14734v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14734v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11111v1",
    "title": "Inducing anxiety in large language models increases exploration and bias",
    "latest": "2023-05-26T09:12:09+00:00",
    "last_post": {
      "url": "https://techhub.social/@nic221/110285935300519866",
      "content": "<p>Inducing anxiety in large language models increases exploration and bias <a href=\"https://arxiv.org/abs/2304.11111v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11111v1</span><span class=\"invisible\"></span></a> <a href=\"https://techhub.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://techhub.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://techhub.social/tags/bias\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>bias</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s10606-022-09459-y",
    "title": "Design Indirections - Computer Supported Cooperative Work (CSCW)",
    "latest": "2023-05-26T09:11:52+00:00",
    "last_post": {
      "url": "https://mastodon.design/@karl/110434164448468604",
      "content": "<p>Really happy to publish \u201eDesign Indirections: How Designers Find Their Ways in Shaping Algorithmic Systems\" in CSCW journal, written with my fantastic colleagues J\u00e9r\u00e9mie Poiroux, <span class=\"h-card\"><a href=\"https://mastodon.design/@nolwennm\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>nolwennm</span></a></span> <span class=\"h-card\"><a href=\"https://hci.social/@emeline\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>emeline</span></a></span> and <span class=\"h-card\"><a href=\"https://hci.social/@aurelien\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>aurelien</span></a></span>!</p><p><a href=\"https://link.springer.com/article/10.1007/s10606-022-09459-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s10606-022-09459-y</span></a> </p><p>We wanted to better understand how (UX, UI, service, product) designers, in different organizational contexts, participate in shaping algorithmic systems. \ud83d\udc47</p>"
    },
    "people": [
      {
        "url": "https://hci.social/@aurelien",
        "display_name": "Aur\u00e9lien"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14728v1",
    "title": "SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations",
    "latest": "2023-05-26T09:11:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110434208730960970",
      "content": "<p>\ud83d\udcdd SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations \ud83d\udcda</p><p>\"Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category for the passage.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14728v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14728v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14726v1",
    "title": "In-Context Demonstration Selection with Cross Entropy Difference",
    "latest": "2023-05-26T08:51:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110434129969386371",
      "content": "<p>\ud83d\udcdd In-Context Demonstration Selection with Cross Entropy Difference \ud83d\udcda\ud83d\udc7e</p><p>\"A cross-entropy difference method for selecting in-context demonstrations is proposed and evaluated on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14726v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14726v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14725v1",
    "title": "https://arxiv.org/abs/2305.14725v1",
    "latest": "2023-05-26T08:31:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110434051412330972",
      "content": "<p>\ud83d\udcdd AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes \ud83d\udcda</p><p>\"AMELI is a new dataset for attribute-aware multimodal entity linking, a challenging task that aims to link textual mention, visual image and product attribute values to an entity in a multimodal KB.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/JaidedAI/EasyOCR\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/JaidedAI/EasyOCR</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14725v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14725v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14718v1",
    "title": "https://arxiv.org/abs/2305.14718v1",
    "latest": "2023-05-26T08:11:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433972842781884",
      "content": "<p>\ud83d\udcdd Improving Language Models with Advantage-Based Offline Policy Gradients \ud83d\udcda</p><p>\"Presents Left-over Lunch RL (LoL-RL), a training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/abaheti95/LoL-RL\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/abaheti95/LoL-RL</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14718v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14718v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14716v1",
    "title": "https://arxiv.org/abs/2305.14716v1",
    "latest": "2023-05-26T07:51:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433894260924143",
      "content": "<p>\ud83d\udcdd GlobalBench: A Benchmark for Global Progress in Natural Language Processing \ud83d\udcda</p><p>\"Creates a leaderboard that tracks system performance across all datasets and languages, rewarding research that focuses on the most under-served languages and datasets, and provides a multi-faceted view of how language technology is serving people of the world.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ExpressAI/DataLab\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/ExpressAI/DataLab</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14716v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14716v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14707v1",
    "title": "The student becomes the master: Matching GPT3 on Scientific Factual Error Correction",
    "latest": "2023-05-26T07:41:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433854864563571",
      "content": "<p>\ud83d\udcdd The Student Becomes the Master: Matching GPT3 on Scientific Factual Error Correction \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization and uses a claim-aware decoding procedure to improve the quality of corrected claims.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14707v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14707v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.5281/zenodo.5526634",
    "title": "https://doi.org/10.5281/zenodo.5526634",
    "latest": "2023-05-26T07:25:42+00:00",
    "last_post": {
      "url": "https://mastodon.social/@brembs/110422458003613322",
      "content": "<p>ICYM the big <a href=\"https://mastodon.social/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a> news yesterday: the Council of the EU adopted Open Science recommendations:<br> <br><a href=\"https://data.consilium.europa.eu/doc/document/ST-8827-2023-INIT/en/pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">data.consilium.europa.eu/doc/d</span><span class=\"invisible\">ocument/ST-8827-2023-INIT/en/pdf</span></a></p><p>which were immediately supported by ten major research organizations:</p><p><a href=\"https://www.coalition-s.org/wp-content/uploads/2023/05/JointResponse2CouncilScholCommConclusions.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">coalition-s.org/wp-content/upl</span><span class=\"invisible\">oads/2023/05/JointResponse2CouncilScholCommConclusions.pdf</span></a></p><p>Why is this so significant? The adopted recommendations echo proposals from the research community, e.g.:</p><p><a href=\"https://doi.org/10.5281/zenodo.5526634\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.5281/zenodo.5526634</span><span class=\"invisible\"></span></a></p><p>asking for \"interoperable, not-for-profit infrastructures [...] based on open source software and open standards\"</p><p>No more journals!</p>"
    },
    "people": [
      {
        "url": "https://ecoevo.social/@cboettig",
        "display_name": "Carl Boettiger"
      },
      {
        "url": "https://ciberlandia.pt/@villares",
        "display_name": "Alexandre B A Villares \ud83d\udc0d \u2614"
      },
      {
        "url": "https://mastodon.social/@edzer",
        "display_name": "Edzer Pebesma"
      },
      {
        "url": "https://mastodon.social/@igor",
        "display_name": "Igor Brigadir"
      },
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://hci.social/@jbigham",
        "display_name": "Jeff Bigham"
      },
      {
        "url": "https://bbq.snoot.com/@ProgGrrl",
        "display_name": "ProgGrrl"
      },
      {
        "url": "https://datasci.social/@yy",
        "display_name": "YY Ahn"
      },
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://econtwitter.net/@ito",
        "display_name": "Tim Gushue"
      },
      {
        "url": "https://sfba.social/@morgandawn",
        "display_name": "morgandawn"
      },
      {
        "url": "https://mastodon.social/@tiffanycli",
        "display_name": "Tiffany Li"
      },
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      },
      {
        "url": "https://mas.to/@enridaga",
        "display_name": "enridaga"
      },
      {
        "url": "https://starbase80.wtf/@5ciFiGirl",
        "display_name": "Sci-Fi Girl"
      },
      {
        "url": "https://fediscience.org/@mario_angst_sci",
        "display_name": "Mario Angst"
      },
      {
        "url": "https://h4.io/@joshisanonymous",
        "display_name": "Joshua McNeill"
      },
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      },
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      },
      {
        "url": "https://scicomm.xyz/@leouieda",
        "display_name": "Dr Leonardo Uieda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14705v1",
    "title": "Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts",
    "latest": "2023-05-26T07:11:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433736907265230",
      "content": "<p>\ud83d\udcdd Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts \ud83d\udcda</p><p>\"Introduces Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models, and show that naively finetuning MoE models on a task-specific dataset often yield worse performance compared to dense models of the same computational complexity.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14705v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14705v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12544",
    "title": "A PhD Student's Perspective on Research in NLP in the Era of Very Large Language Models",
    "latest": "2023-05-26T07:10:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110433730777738063",
      "content": "<p>Research in NLP in the Era of Large Language Models - <a href=\"https://arxiv.org/abs/2305.12544\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12544</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14702v1",
    "title": "https://arxiv.org/abs/2305.14702v1",
    "latest": "2023-05-26T06:51:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433658315162463",
      "content": "<p>\ud83d\udcdd Analyzing Influential Factors in Human Preference Judgments via GPT-4 \ud83d\udcda</p><p>\"We examine a dataset of human judgements and analyze the influence of key factors using the BTL (Bradley-Terry-Luce) model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/openai/summarize-from-feedback\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/openai/summarize-fr</span><span class=\"invisible\">om-feedback</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14702v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14702v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.01315",
    "title": "Empirical Design in Reinforcement Learning",
    "latest": "2023-05-26T06:44:20+00:00",
    "last_post": {
      "url": "https://masto.ai/@cheng/110433629649974554",
      "content": "<p>How to do good experiments in <a href=\"https://masto.ai/tags/reinforcementlearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>reinforcementlearning</span></a> </p><p><a href=\"https://masto.ai/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> <a href=\"https://masto.ai/tags/ReproducibleResearch\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReproducibleResearch</span></a> <a href=\"https://masto.ai/tags/BestPractice\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>BestPractice</span></a> </p><p><a href=\"https://arxiv.org/abs/2304.01315\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.01315</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://masto.ai/@cheng",
        "display_name": "Cheng Soon Ong"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01700-y",
    "title": "Chronic stress can inflame the gut \u2014 now scientists know why",
    "latest": "2023-05-26T06:40:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110433612779709444",
      "content": "<p>Chronic stress can inflame the gut \u2013 now scientists know why - <a href=\"https://www.nature.com/articles/d41586-023-01700-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01700-y</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14701v1",
    "title": "https://arxiv.org/abs/2305.14701v1",
    "latest": "2023-05-26T06:31:38+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433579721010347",
      "content": "<p>\ud83d\udcdd Modeling Rapid Language Learning by Distilling Bayesian Priors Into Artificial Neural Networks \ud83d\udcda\ud83d\udc7e</p><p>\"Works by distilling a Bayesian model's biases into a neural network, which allows it to handle a broad range of learning scenarios including both formal and naturalistic data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/tommccoy1/ind\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/tommccoy1/ind</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14701v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14701v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.48550/arXiv.2207.13665",
    "title": "doi.org/10.48550/arXiv.2207.13...",
    "latest": "2023-05-26T06:26:31+00:00",
    "last_post": {
      "url": "https://social.cwts.nl/@LudoWaltman/110433549417820997",
      "content": "<p>There's a lot of discussion about bias in terms of gender, race, ethnicity etc., but what do we actually mean by bias?</p><p>The notion of bias is often discussed informally, also in academic literature, leading to imprecise discussions and potentially to misguided policy recommendations, which may even have adverse effects.</p><p><span class=\"h-card\"><a href=\"https://social.cwts.nl/@vtraag\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>vtraag</span></a></span> and I propose a more systematic way of studying biases <a href=\"https://blogs.lse.ac.uk/impactofsocialsciences/2023/05/25/the-bias-puzzle-understanding-gender-differences-in-academia/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">blogs.lse.ac.uk/impactofsocial</span><span class=\"invisible\">sciences/2023/05/25/the-bias-puzzle-understanding-gender-differences-in-academia/</span></a>.</p><p>Our <a href=\"https://social.cwts.nl/tags/LSE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LSE</span></a> blog post is based on a preprint published last year <a href=\"https://doi.org/10.48550/arXiv.2207.13665\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.48550/arXiv.2207.13</span><span class=\"invisible\">665</span></a>.</p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14696v1",
    "title": "https://arxiv.org/abs/2305.14696v1",
    "latest": "2023-05-26T06:11:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433500969307377",
      "content": "<p>\ud83d\udcdd SELFOOD: Self-Supervised Out-of-Distribution Detection via Learning to Rank \ud83d\udcda</p><p>\"Casts out-of-distribution detection as an inter-document intra-label ranking problem and train the classifier with pairwise ranking loss that achieves zero loss value if in-distribution documents belonging to the same label achieve higher softmax scores than documents from the other labels (IDIL ranking).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/dheeraj7596/SELFOOD\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/dheeraj7596/SELFOOD</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14696v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14696v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14695v1",
    "title": "A Causal View of Entity Bias in (Large) Language Models",
    "latest": "2023-05-26T05:51:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433422362216711",
      "content": "<p>\ud83d\udcdd A Causal View of Entity Bias in (Large) Language Models \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"A causal intervention is designed to perturb the original entity with neighboring entities and thus reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information between entities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14695v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14695v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14595",
    "title": "Operationalizing Counterfactual Metrics: Incentives, Ranking, and Information Asymmetry",
    "latest": "2023-05-26T05:31:29+00:00",
    "last_post": {
      "url": "https://masto.ai/@cheng/110433343194775671",
      "content": "<p>Theoretical analysis of <a href=\"https://masto.ai/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> decision making. <a href=\"https://masto.ai/tags/Fairness\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Fairness</span></a> viewed as <a href=\"https://masto.ai/tags/SocialWelfare\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SocialWelfare</span></a> should also take into account the alternative decision (called <a href=\"https://masto.ai/tags/counterfactual\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>counterfactual</span></a> outcomes). This paper proposes a framework which models two kinds of actors: (1) a \"principal\" who is the central organisation; (2) a set of \"agents\" who provides the service under consideration. The social welfare goal is to design reward functions with high utility for the principal.</p><p><a href=\"https://arxiv.org/abs/2305.14595\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14595</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://aus.social/@KathyReid",
        "display_name": "Kathy Reid"
      },
      {
        "url": "https://masto.ai/@cheng",
        "display_name": "Cheng Soon Ong"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.11062.pdf",
    "title": "https://arxiv.org/pdf/2304.11062.pdf",
    "latest": "2023-05-26T05:31:18+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wirepair/110257224044537977",
      "content": "<p>This seems like rather huge news <a href=\"https://arxiv.org/pdf/2304.11062.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.11062.pdf</span><span class=\"invisible\"></span></a>. The possibility of having 2 million token prompts, if true. Maybe then we could see useful applications of these llms</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14688v1",
    "title": "https://arxiv.org/abs/2305.14688v1",
    "latest": "2023-05-26T05:11:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433265116587696",
      "content": "<p>\ud83d\udcdd ExpertPrompting: Instructing Large Language Models to Be Distinguished Experts \ud83d\udcda\ud83d\udc7e</p><p>\"In-Context Learning is used to synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/OFA-Sys/ExpertLLaMA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/OFA-Sys/ExpertLLaMA</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14688v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14688v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14682v1",
    "title": "https://arxiv.org/abs/2305.14682v1",
    "latest": "2023-05-26T05:01:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433225679310790",
      "content": "<p>\ud83d\udcdd TACR: A Table-Alignment-Based Cell-Selection and Reasoning Model for Hybrid Question-Answering \ud83d\udcda</p><p>\"A novel Table-alignment-based Cell-selection and Reasoning model (TACR) for hybrid text and table QA, evaluated on the HybridQA and WikiTableQuestions datasets.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/WuJian1995/QAP\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/WuJian1995/QAP</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14682v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14682v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.13945.pdf",
    "title": "https://arxiv.org/pdf/2305.13945.pdf",
    "latest": "2023-05-26T04:54:03+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110430596906058923",
      "content": "<p>\"Wikipedia and open access\" open-access articles are extensively and increasingly more cited in <span class=\"h-card\"><a href=\"https://mastodon.social/@Wikipedia\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Wikipedia</span></a></span>. </p><p>(Yang et al, 2023)</p><p><a href=\"https://arxiv.org/pdf/2305.13945.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.13945.pdf</span><span class=\"invisible\"></span></a></p><p>@giovanni1085 <a href=\"https://twitter.com/WikiResearch/status/1661785718098735124\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1661785718098735124</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      },
      {
        "url": "https://mastodon.social/@RonaldSnijder",
        "display_name": "Ronald Snijder"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14681v1",
    "title": "Emergent inabilities? Inverse scaling over the course of pretraining",
    "latest": "2023-05-26T04:31:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433107587667009",
      "content": "<p>\ud83d\udcdd Emergent Inabilities? Inverse Scaling Over the Course of Pretraining \ud83d\udcda</p><p>\"Works by training on the language modeling task, but testing on a separate downstream task (quote-repetition or redefine-math), where performance on the downstream task decreases while overall performance increases.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14681v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14681v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14676v1",
    "title": "GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions",
    "latest": "2023-05-26T04:11:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110433029092667000",
      "content": "<p>\ud83d\udcdd GRILL: Grounded Vision-Language Pre-Training via Aligning Text and Image Regions \ud83d\udcda</p><p>\"GRound and vIsion Language AlIGNing (GRILL) exploits object-text alignments to train a novel VL model that can be generalized to diverse tasks including visual question answering, captioning, and grounding tasks with no or very few training instances.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14676v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14676v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2206.02336",
    "title": "Making Large Language Models Better Reasoners with Step-Aware Verifier",
    "latest": "2023-05-26T04:11:35+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110432983588358406",
      "content": "<p>Making Large Language Models better reasoners with step-aware verifier. ~ Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen. <a href=\"https://arxiv.org/abs/2206.02336\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2206.02336</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a> <a href=\"https://mathstodon.xyz/tags/Reasoning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Reasoning</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14671v1",
    "title": "Diffusion Models in NLP: A Survey",
    "latest": "2023-05-26T04:01:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432989670937363",
      "content": "<p>\ud83d\udcdd Diffusion Models in NLP: A Survey \ud83d\udcda</p><p>\"Diffusion models are a class of models that aim to capture the diffusion of information or signals across a network or manifold by modelling the process of random walks on a graph.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14671v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14671v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14992",
    "title": "Reasoning with Language Model is Planning with World Model",
    "latest": "2023-05-26T03:48:23+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110432937822025164",
      "content": "<p>[R] Reasoning with Language Model is Planning with World Model - Shibo Hao et al UC San Diego - RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting!</p><p><a href=\"https://arxiv.org/abs/2305.14992\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14992</span><span class=\"invisible\"></span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://arxiv.org/abs/2305.14992\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://arxiv.org/a</span><span class=\"invisible\">bs/2305.14992</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14660v1",
    "title": "Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction",
    "latest": "2023-05-26T03:41:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432911161323523",
      "content": "<p>\ud83d\udcdd Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction \ud83d\udcda</p><p>\"A new definition extraction method that masks mathematical symbols, creates a copy of each sentence for each symbol, specifies a target symbol, and predicts its corresponding definition spans using slot filling.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14660v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14660v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14663v1",
    "title": "You Are What You Annotate: Towards Better Models through Annotator Representations",
    "latest": "2023-05-26T03:31:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432871813437678",
      "content": "<p>\ud83d\udcdd You Are What You Annotate: Towards Better Models Through Annotator Representations \ud83d\udcda</p><p>\"Creates representations for the annotators (annotator embeddings) and their annotations (annotation embeddings) with learnable matrices associated with each annotator and annotation, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14663v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14663v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.13241.pdf",
    "title": "https://arxiv.org/pdf/2305.13241.pdf",
    "latest": "2023-05-26T03:26:23+00:00",
    "last_post": {
      "url": "https://mastodon.social/@regehr/110432791023889807",
      "content": "<p>lots of great compiler knowledge in this paper!</p><p>\"Whose baseline (compiler) is it anyway?\"</p><p><a href=\"https://arxiv.org/pdf/2305.13241.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.13241.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@gcampax",
        "display_name": "Giovanni Campagna"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14658v1",
    "title": "Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality",
    "latest": "2023-05-26T03:21:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432832494741681",
      "content": "<p>\ud83d\udcdd Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality \ud83d\udcda</p><p>\"KdConv-ADV and DSTC7-ADV are two adversarial meta-evaluation datasets for dialogue generation models, which are more challenging than the previous meta-evaluation dataset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14658v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14658v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14652v1",
    "title": "https://arxiv.org/abs/2305.14652v1",
    "latest": "2023-05-26T03:01:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432753645464222",
      "content": "<p>\ud83d\udcdd Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion \ud83d\udcda</p><p>\"Proposes a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion to filter out noise and redundancy with a restrained receptive field and a mutual information module to regulate the filter-out module to capture the key information.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/WSXRHFG/DBF\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/WSXRHFG/DBF</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14652v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14652v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14647v1",
    "title": "Meta-review Generation with Checklist-guided Iterative Introspection",
    "latest": "2023-05-26T02:41:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432675266260499",
      "content": "<p>\ud83d\udcdd Meta-Review Generation with Checklist-Guided Iterative Introspection \ud83d\udcda</p><p>\"Checklist-guided Iterative Introspection (CGI$^{2}$) approach, which decomposes the task into several stages and iteratively refines the summary under the guidance of questions from a checklist.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14647v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14647v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14630v1",
    "title": "Testing Causal Models of Word Meaning in GPT-3 and -4",
    "latest": "2023-05-26T02:31:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432635926511588",
      "content": "<p>\ud83d\udcdd Testing Causal Models of Word Meaning in GPT-3 and -4 \ud83d\udcda</p><p>\"Evaluates the lexical representations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of concept representations which focuses on representations of words describing artifacts (such as \"mop\", \"pencil\", and \"whistle\").\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14630v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14630v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.21688",
    "title": "onlinelibrary.wiley.com/doi/ab...",
    "latest": "2023-05-26T02:28:52+00:00",
    "last_post": {
      "url": "https://fediscience.org/@ct_bergstrom/110432625122746488",
      "content": "<p>But what about international collaborations? That's part of the detector as well.</p><p>Unsurprisingly, there's a ton of data on this. </p><p>Ganzi et al (2012), for example, have data from 2000-2009.</p><p><a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.21688\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">onlinelibrary.wiley.com/doi/ab</span><span class=\"invisible\">s/10.1002/asi.21688</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s11192-018-2691-0",
    "title": "Do papers with an institutional e-mail address receive more citations than those with a non-institutional one? - Scientometrics",
    "latest": "2023-05-26T02:20:47+00:00",
    "last_post": {
      "url": "https://fediscience.org/@ct_bergstrom/110432593319074294",
      "content": "<p>Easily accessible data, in fact, of the sort that one might say look at if one were going to use these criteria to make extreme claims about fraud coming out of particular nations. </p><p>Let's start with institutional email addresses. </p><p>Lo and behold, the countries flagged by the \"detector\" as the worse perpetrators have long been known to be places where the convention is not to use institutional email. </p><p><a href=\"https://link.springer.com/article/10.1007/s11192-018-2691-0\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s11192-018-2691-0</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14628v1",
    "title": "https://arxiv.org/abs/2305.14628v1",
    "latest": "2023-05-26T02:11:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432557155206756",
      "content": "<p>\ud83d\udcdd Mixture of Prompt Experts for Generalizable and Interpretable Question Answering \ud83d\udcda\ud83d\udc7e</p><p>\"The mixture-of-prompt-experts system is built upon a collection of expert models, each of which is trained with a specialized prompt for a particular reasoning skill.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/NoviScl/MoPE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/NoviScl/MoPE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14628v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14628v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14625v1",
    "title": "https://arxiv.org/abs/2305.14625v1",
    "latest": "2023-05-26T01:51:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432478561214909",
      "content": "<p>\ud83d\udcdd KNN-LM Does Not Improve Open-Ended Text Generation \ud83d\udcda</p><p>\"The KNN-LM is a retrieval-augmented language model that first retrieves relevant documents from a large corpus, and then interpolates the predictions of the base LM with those from a distribution formed from the retrieved documents.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/neulab/knn-transformers\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/neulab/knn-transfor</span><span class=\"invisible\">mers</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14625v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14625v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14623v1",
    "title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models",
    "latest": "2023-05-26T01:21:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432360457924867",
      "content": "<p>\ud83d\udcdd Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models \ud83d\udcda</p><p>\"Proposes Self-Checker, a framework to facilitate fact-checking by using LLMs in an almost zero-shot setting via a set of plug-and-play modules.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14623v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14623v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14622v1",
    "title": "EXnet: Efficient In-context Learning for Data-less Text classification",
    "latest": "2023-05-26T01:11:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432321107952292",
      "content": "<p>\ud83d\udcdd EXnet: Efficient in-Context Learning for Data-Less Text Classification \ud83d\udcda\ud83e\udde0</p><p>\"EXnet is a transformer-based model, specifically designed for performing in-context learning on classification tasks without any limitation on the number of provided examples, and without any further fine-tuning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14622v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14622v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.16107",
    "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation",
    "latest": "2023-05-26T01:05:03+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110432295581210501",
      "content": "<p>VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation</p><p>Is one decoder-only generative model all you need for speech recognition, synthesis, and translation?</p><p><a href=\"https://arxiv.org/abs/2305.16107\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.16107</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.16213",
    "title": "https://arxiv.org/abs/2305.16213",
    "latest": "2023-05-26T01:05:01+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110432295439196453",
      "content": "<p>ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</p><p>That croissant passed my tastiness Turing test.</p><p>proj: <a href=\"https://ml.cs.tsinghua.edu.cn/prolificdreamer/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">ml.cs.tsinghua.edu.cn/prolific</span><span class=\"invisible\">dreamer/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.16213\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.16213</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.16192",
    "title": "Explainability Techniques for Chemical Language Models",
    "latest": "2023-05-26T00:56:11+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110432260693569390",
      "content": "<p>Explainability Techniques for Chemical Language Models</p><p>Proposes an explainable AI technique that attributes the importance of individual atoms towards the predictions made by these models</p><p><a href=\"https://arxiv.org/abs/2305.16192\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.16192</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.16319",
    "title": "Image is First-order Norm+Linear Autoregressive",
    "latest": "2023-05-26T00:56:09+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110432260533222207",
      "content": "<p>Image is First-order Norm+Linear Autoregressive</p><p>Reveals that every image can be understood as a first-order norm+linear autoregressive process</p><p><a href=\"https://arxiv.org/abs/2305.16319\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.16319</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.16311",
    "title": "https://arxiv.org/abs/2305.16311",
    "latest": "2023-05-26T00:56:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110432260390226363",
      "content": "<p>Break-A-Scene: Extracting Multiple Concepts from a Single Image</p><p>proj: <a href=\"https://omriavrahami.com/break-a-scene/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">omriavrahami.com/break-a-scene</span><span class=\"invisible\">/</span></a><br>video: <a href=\"https://www.youtube.com/watch?v=-9EA-BhizgM\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">youtube.com/watch?v=-9EA-Bhizg</span><span class=\"invisible\">M</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.16311\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.16311</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14618v1",
    "title": "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations",
    "latest": "2023-05-26T00:51:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432242696797703",
      "content": "<p>\ud83d\udcdd Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations \ud83d\udcda\ud83d\udc7e</p><p>\"An approach for commonsense abductive reasoning that exploits the fact that only a subset of explanations is correct for a given context; it uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14618v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14618v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.16264",
    "title": "Scaling Data-Constrained Language Models",
    "latest": "2023-05-26T00:45:53+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110432220169776693",
      "content": "<p>Scaling Data-Constrained Language Models</p><p>Proposes and empirically validates a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters.</p><p>repo: <a href=\"https://github.com/huggingface/datablations\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/huggingface/databla</span><span class=\"invisible\">tions</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.16264\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.16264</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14617v1",
    "title": "https://arxiv.org/abs/2305.14617v1",
    "latest": "2023-05-26T00:31:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432164090880550",
      "content": "<p>\ud83d\udcdd COMET-M: Reasoning About Multiple Events in Complex Sentences \ud83d\udcda\ud83d\udc7e</p><p>\"COMET-M is an event-centric commonsense model trained to generate commonsense inferences (if-then knowledge triplet) about complex sentences containing multiple events.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/markriedl/WikiPlots\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/markriedl/WikiPlots</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14617v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14617v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11062",
    "title": "Scaling Transformer to 1M tokens and beyond with RMT",
    "latest": "2023-05-26T00:14:35+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@mikeiavelli/110254216906691830",
      "content": "<p>Recurrent Memory Transformer (RMT) retains information across up to 2 million tokens (!) \u2b50, significantly exceeding the largest input size reported for transformer models (64K tokens) and GPT-4's 32K tokens.</p><p>Paper: <a href=\"https://arxiv.org/abs/2304.11062\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11062</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/booydar/t5-experiments/tree/scaling-report\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/booydar/t5-experime</span><span class=\"invisible\">nts/tree/scaling-report</span></a></p><p><a href=\"https://mathstodon.xyz/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> <a href=\"https://mathstodon.xyz/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <br><a href=\"https://mathstodon.xyz/tags/NeuralNetworks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuralNetworks</span></a> <a href=\"https://mathstodon.xyz/tags/NN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NN</span></a><br><a href=\"https://mathstodon.xyz/tags/DeepLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DeepLearning</span></a><br><a href=\"https://mathstodon.xyz/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mathstodon.xyz/tags/Transformers\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Transformers</span></a> <a href=\"https://mathstodon.xyz/tags/RNN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>RNN</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://mastodon.online/@vladiliescu",
        "display_name": "Vlad Iliescu \ud83d\udc2c"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14616v1",
    "title": "https://arxiv.org/abs/2305.14616v1",
    "latest": "2023-05-26T00:01:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110432045953005294",
      "content": "<p>\ud83d\udcdd Exploring the Grounding Issues in Image Caption \ud83d\udcda\ud83d\udd2d</p><p>\"The image caption generation process integrates situated meaning and affordance into multimodal semantic representation, which consolidates visual and textual elements in Flickr30k dataset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/XXX\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/XXX</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14616v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14616v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14613v1",
    "title": "Selectively Answering Ambiguous Questions",
    "latest": "2023-05-25T23:41:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431967505610481",
      "content": "<p>\ud83d\udcdd Selectively Answering Ambiguous Questions \ud83d\udcda\ud83d\udc7e</p><p>\"A transformer-based language model trained on a question answering dataset, which predicts answers by selecting a text span from a passage of text, given a question and context.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14613v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14613v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14603v1",
    "title": "OpenPI2.0: An Improved Dataset for Entity Tracking in Texts",
    "latest": "2023-05-25T23:21:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431888703127893",
      "content": "<p>\ud83d\udcdd OpenPI2.0: An Improved Dataset for Entity Tracking in Texts \ud83d\udcda</p><p>\"Proposes OpenPI2, an improved version over OpenPI, with canonical entities to facilitate evaluation and with salience annotations to support entity state tracking in downstream tasks such as QA and classical planning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14603v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14603v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14599v1",
    "title": "Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations",
    "latest": "2023-05-25T23:01:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431809970589433",
      "content": "<p>\ud83d\udcdd Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations \ud83d\udcda</p><p>\"Proposes InterSent, an end-to-end framework for learning interpretable sentence embeddings that supports compositional sentence operations in the embedding space, easing the interpretation of sentence embedding transformations as compositional sentence operations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14599v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14599v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.12248.pdf",
    "title": "https://arxiv.org/pdf/2305.12248.pdf",
    "latest": "2023-05-25T22:58:13+00:00",
    "last_post": {
      "url": "https://neuromatch.social/@marcia_petyt/110431762936754219",
      "content": "<p>Brain encoding models based on multimodal transformers can transfer across language and vision                                                                     <a href=\"https://arxiv.org/pdf/2305.12248.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.12248.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14596v1",
    "title": "Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy",
    "latest": "2023-05-25T22:41:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431731516577771",
      "content": "<p>\ud83d\udcdd Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy \ud83d\udcda\ud83e\udde0</p><p>\"Proposes a mathematical formalism for studying attentiveness of language models on multiple choice tasks, and identify a simple method for increasing it -- namely, in-context learning with even just one example containing answer choices.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14596v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14596v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12524",
    "title": "TheoremQA: A Theorem-driven Question Answering dataset",
    "latest": "2023-05-25T22:17:55+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110431563956497297",
      "content": "<p>TheoremQA: A Theorem-driven Question Answering dataset. ~ Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia. <a href=\"https://arxiv.org/abs/2305.12524\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12524</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a> <a href=\"https://mathstodon.xyz/tags/Math\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Math</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14592v1",
    "title": "Instruction Tuning with Lexicons for Zero-Shot Style Classification",
    "latest": "2023-05-25T22:11:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431613564508494",
      "content": "<p>\ud83d\udcdd Instruction Tuning with Lexicons for Zero-Shot Style Classification \ud83d\udcda\ud83e\udde0</p><p>\"Investigates the effectiveness of style lexicons as a means for instructing language models how to identify new styles that are unseen during training by using prompts and instructions for zero-shot style classification.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14592v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14592v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14591v1",
    "title": "https://arxiv.org/abs/2305.14591v1",
    "latest": "2023-05-25T22:01:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431574281849804",
      "content": "<p>\ud83d\udcdd ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers \ud83d\udcda</p><p>\"Generates a probably correct but possibly slow reference oracle by prompting an large language model (LLM) to exhaustively enumerate all the combinations of relevant variables.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/microsoft/CodeT/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/microsoft/CodeT/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14591v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14591v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.05197",
    "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
    "latest": "2023-05-25T21:54:56+00:00",
    "last_post": {
      "url": "https://c.im/@hyperplane/110431543693110446",
      "content": "<p>Multi-step Jailbreaking Privacy Attacks on ChatGPT</p><p><a href=\"https://arxiv.org/abs/2304.05197\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.05197</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14588v1",
    "title": "https://arxiv.org/abs/2305.14588v1",
    "latest": "2023-05-25T21:41:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431495643139545",
      "content": "<p>\ud83d\udcdd Evaluating End-to-End Entity Linking on Domain-Specific Knowledge Bases: Learning About Ancient Technologies From Museum Collections \ud83d\udcda\ud83e\udde0</p><p>\"A state-of-the-art neural architecture for Entity Linking that is trained on a museum collection corpus to perform the task of entity disambiguation, linking and categorization.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/LongRunGrowth/ARTIFACTDataset\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/LongRunGrowth/ARTIF</span><span class=\"invisible\">ACTDataset</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14588v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14588v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14590v1",
    "title": "RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents",
    "latest": "2023-05-25T21:31:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431456174371037",
      "content": "<p>\ud83d\udcdd RE$^2$: Region-Aware Relation Extraction From Visually Rich Documents \ud83d\udcda\ud83d\udc7e</p><p>\"RE$^2$ leverages region-level spatial structure among the entity blocks to improve their relation prediction by designing an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14590v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14590v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14583v1",
    "title": "https://arxiv.org/abs/2305.14583v1",
    "latest": "2023-05-25T21:21:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431416681448382",
      "content": "<p>\ud83d\udcdd Making the Implicit Explicit: Implicit Content as a First Class Citizen in NLP \ud83d\udcda</p><p>\"Trains an LLM to decompose an utterance into simpler components, such as the logical inferences that follow from an utterance of the form, \"X is Y\".\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/alexlitel/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/alexlitel/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14583v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14583v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14578v1",
    "title": "https://arxiv.org/abs/2305.14578v1",
    "latest": "2023-05-25T21:01:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431338215369520",
      "content": "<p>\ud83d\udcdd Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification Using Graph Neural Networks? \ud83d\udcda</p><p>\"Graph neural networks (GNNs) learn node representations by aggregating information from neighboring nodes, while text GNNs aggregate textual feature vectors from the graph nodes using recurrent neural networks (RNN).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mojave-pku/TextLevelGCN\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/mojave-pku/TextLeve</span><span class=\"invisible\">lGCN</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14578v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14578v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01708-4?utm_source=twitter&amp;utm_medium=social&amp;utm_content=organic&amp;utm_campaign=CONR_JRNLS_AWA1_GL_SCON_SMEDA_NATUREPORTFOLIO",
    "title": "A mental-health crisis is gripping science \u2014 toxic research culture is to blame",
    "latest": "2023-05-25T20:43:01+00:00",
    "last_post": {
      "url": "https://mastodon.social/@freakonometrics/110431265220913808",
      "content": "<p>\"With researchers reporting high rates of anxiety and  depression, calls are growing to fundamentally change science before  it\u2019s too late.\" <a href=\"https://www.nature.com/articles/d41586-023-01708-4?utm_source=twitter&amp;utm_medium=social&amp;utm_content=organic&amp;utm_campaign=CONR_JRNLS_AWA1_GL_SCON_SMEDA_NATUREPORTFOLIO\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01708-4?utm_source=twitter&amp;utm_medium=social&amp;utm_content=organic&amp;utm_campaign=CONR_JRNLS_AWA1_GL_SCON_SMEDA_NATUREPORTFOLIO</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@freakonometrics",
        "display_name": "Arthur Charpentier"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14576v1",
    "title": "Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings",
    "latest": "2023-05-25T20:31:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431220378943564",
      "content": "<p>\ud83d\udcdd Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings \ud83d\udcda</p><p>\"Active learning with adapter-based parameter-efficient fine-tuning achieves comparable performance to full fine-tuning in low-resource settings, while significantly reducing memory and time costs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14576v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14576v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14571v1",
    "title": "From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding",
    "latest": "2023-05-25T20:11:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431141613184080",
      "content": "<p>\ud83d\udcdd From Characters to Words: Hierarchical Pre-Trained Language Model for Open-Vocabulary Language Understanding \ud83d\udcda</p><p>\"A character-level language model is trained to learn word representations with a shallow Transformer architecture from their characters, while a deep Transformer is trained to attend to the entire word sequence in a document.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14571v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14571v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15294",
    "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
    "latest": "2023-05-25T20:10:13+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110431136206970435",
      "content": "<p>RT @zhs05232838@twitter.com</p><p>NEW PAPER: Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy</p><p><a href=\"https://arxiv.org/abs/2305.15294\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15294</span><span class=\"invisible\"></span></a></p><p>Here are some findings:</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/zhs05232838/status/1661763831721529344\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/zhs05232838/status</span><span class=\"invisible\">/1661763831721529344</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.13989",
    "title": "http://arxiv.org/abs/2305.13989",
    "latest": "2023-05-25T20:10:01+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@vukosi/110431135455530501",
      "content": "<p>RT @davlanade@twitter.com</p><p>Happy to share the camera ready of our paper \"MasakhaPOS: Part-of-Speech Tagging for Typologically Diverse African Languages\" </p><p>paper: <a href=\"http://arxiv.org/abs/2305.13989\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.13989</span><span class=\"invisible\"></span></a><br>code: <a href=\"https://github.com/masakhane-io/masakhane-pos/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/masakhane-io/masakh</span><span class=\"invisible\">ane-pos/</span></a></p><p>This is the final publication of the MasakhaNER project funded by @LacunaFund@twitter.com.</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/davlanade/status/1661700238577397760\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/davlanade/status/1</span><span class=\"invisible\">661700238577397760</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@vukosi",
        "display_name": "Vukosi Marivate"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14569v1",
    "title": "Few-shot Unified Question Answering: Tuning Models or Prompts?",
    "latest": "2023-05-25T19:41:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110431023722396344",
      "content": "<p>\ud83d\udcdd Few-Shot Unified Question Answering: Tuning Models or Prompts? \ud83d\udcda</p><p>\"Explores the potential of two paradigms of tuning, model, and prompts for unified QA under a low-resource setting, revealing that prompt tuning can perform as well as model tuning in a few-shot setting with a good initialization.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14569v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14569v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14564v1",
    "title": "https://arxiv.org/abs/2305.14564v1",
    "latest": "2023-05-25T19:31:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430984398699154",
      "content": "<p>\ud83d\udcdd PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents \ud83d\udcda</p><p>\"Decomposes questions into a sequence of actions (e SUMMARIZE, FIND_EVENT, FIND_RELATION) and executes them to obtain answers over long documents.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/SimengSun/pearl\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/SimengSun/pearl</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14564v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14564v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14556v1",
    "title": "Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations",
    "latest": "2023-05-25T19:01:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430866299367866",
      "content": "<p>\ud83d\udcdd Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations \ud83d\udcda\ud83d\udc7e</p><p>\"Explores the potential of large pre-trained generative models to generate goal-oriented dialogues for different dialogue tasks, including task-oriented, collaborative, and explanatory.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14556v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14556v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13945",
    "title": "Wikipedia and open access",
    "latest": "2023-05-25T18:32:16+00:00",
    "last_post": {
      "url": "https://mastodon.social/@rmounce/110423879654893980",
      "content": "<p>BIG preprint just dropped: <br>\"Wikipedia and open access\"</p><p><a href=\"https://arxiv.org/abs/2305.13945\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13945</span><span class=\"invisible\"></span></a></p><p>\"We find that open-access articles are extensively and increasingly more cited in Wikipedia. What is more, they show a 15% higher likelihood of being cited in Wikipedia when compared to closed-access articles, after controlling for confounding factors. This open-access citation effect is particularly strong for articles with low citation counts, including recently published ones.\"</p><p>HT <span class=\"h-card\"><a href=\"https://fediscience.org/@oatp\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>oatp</span></a></span><br><a href=\"https://mastodon.social/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> <a href=\"https://mastodon.social/tags/Wikipedia\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Wikipedia</span></a></p>"
    },
    "people": [
      {
        "url": "https://dair-community.social/@DrVeronikaCH",
        "display_name": "Veronika Cheplygina"
      },
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      },
      {
        "url": "https://hachyderm.io/@amcasari",
        "display_name": "M. Secretary commandasaurus \ud83e\udd96"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14555v1",
    "title": "https://arxiv.org/abs/2305.14555v1",
    "latest": "2023-05-25T18:31:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430748447494808",
      "content": "<p>\ud83d\udcdd All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"BERT-INN learns the isomorphism by maximizing the likelihood of the original embeddings given the reproduced embeddings and vice versa (see figure to the right), which is much more accurate than using CCA to align the spaces.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/twinkle0331/BERT-similarity\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/twinkle0331/BERT-si</span><span class=\"invisible\">milarity</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14555v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14555v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14548v1",
    "title": "Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization",
    "latest": "2023-05-25T18:11:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430669625386632",
      "content": "<p>\ud83d\udcdd Interpretable Automatic Fine-Grained Inconsistency Detection in Text Summarization \ud83d\udcda</p><p>\"Explicitly represents the facts in the documents and summaries with semantic frames, and highlights the related semantic frames to predict inconsistency, then it helps verify predicted error types and correct inconsistent summaries.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14548v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14548v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2106.09685.pdf",
    "title": "https://arxiv.org/pdf/2106.09685.pdf",
    "latest": "2023-05-25T17:50:24+00:00",
    "last_post": {
      "url": "https://mastodon.social/@manikrathee/110430500360548593",
      "content": "<p>what a day for launches and announcements:</p><p>1: Leica Q3<br>2: Darkroom x Leica partnership<br>3: Arc Boosts, by The Browser Company<br>4: the QLoRA paper, describing the ability to train LLMs on widely available GPUs.<br>5: Deepmind x YouTube for generative video timeline markers and metadata </p><p>1: <a href=\"https://leica-camera.com/en-US/photography/cameras/q/q3-black/details\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">leica-camera.com/en-US/photogr</span><span class=\"invisible\">aphy/cameras/q/q3-black/details</span></a><br>1a: <a href=\"https://www.dpreview.com/reviews/leica-q3-initial-review\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">dpreview.com/reviews/leica-q3-</span><span class=\"invisible\">initial-review</span></a><br>2: <a href=\"https://darkroom.co/blog/2023-05-25-leica-integration\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">darkroom.co/blog/2023-05-25-le</span><span class=\"invisible\">ica-integration</span></a><br>3: <a href=\"https://arc.net/boosts\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arc.net/boosts</span><span class=\"invisible\"></span></a><br>4: <a href=\"https://arxiv.org/pdf/2106.09685.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2106.09685.pdf</span><span class=\"invisible\"></span></a><br>5: <a href=\"https://www.deepmind.com/blog/working-together-with-youtube\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">deepmind.com/blog/working-toge</span><span class=\"invisible\">ther-with-youtube</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14540v1",
    "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond",
    "latest": "2023-05-25T17:41:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430551811070008",
      "content": "<p>\ud83d\udcdd LLMs as Factual Reasoners: Insights From Existing Benchmarks and Beyond \ud83d\udcda</p><p>\"SummEdits consists of pairs of summaries generated by a model on the same input but with different random seeds, followed by edits to one of the summaries to make it inconsistent w.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14540v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14540v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14534v1",
    "title": "https://arxiv.org/abs/2305.14534v1",
    "latest": "2023-05-25T17:31:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430512491578046",
      "content": "<p>\ud83d\udcdd Detecting Propaganda Techniques in Code-Switched Social Media Text \ud83d\udcda\ud83d\udc7e</p><p>\"Collects a corpus of 1,031 code-switched tweets annotated with propaganda techniques by a team of linguists and NLP experts using the web annotation tool BRAT.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mbzuai-nlp/propaganda-codeswitched-text\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/mbzuai-nlp/propagan</span><span class=\"invisible\">da-codeswitched-text</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14534v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14534v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41586-023-06094-5",
    "title": "Walking naturally after spinal cord injury using a brain\u2013spine interface - Nature",
    "latest": "2023-05-25T17:19:23+00:00",
    "last_post": {
      "url": "https://mastodon.social/@law_geek/110430171119069468",
      "content": "<p>This is amazing news (but a technical read): Walking naturally after spinal cord injury using a brain\u2013spine interface </p><p><a href=\"https://www.nature.com/articles/s41586-023-06094-5\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41586-023</span><span class=\"invisible\">-06094-5</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14533v1",
    "title": "https://arxiv.org/abs/2305.14533v1",
    "latest": "2023-05-25T17:01:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430394589863867",
      "content": "<p>\ud83d\udcdd How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation \ud83d\udcda</p><p>\"Creates a novel Massively Multi-System Multi-Reference (MMSMR) test set for dialogue evaluation which includes 8 reference conversations for each source sentence.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Shimorina/human-evaluationdatasheet/blob/main/sheet/markdown/humanevaluation-datasheet.md\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/Shimorina/human-eva</span><span class=\"invisible\">luationdatasheet/blob/main/sheet/markdown/humanevaluation-datasheet.md</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14533v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14533v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14502v1",
    "title": "https://arxiv.org/abs/2305.14502v1",
    "latest": "2023-05-25T16:51:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430355115604934",
      "content": "<p>\ud83d\udcdd RetICL: Sequential Retrieval of in-Context Examples with Reinforcement Learning \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Frames the problem of sequential example selection as a Markov decision process, design an example retriever model using an LSTM, and train it using proximal policy optimization (PPO).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/umass-ml4ed/RetICL\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/umass-ml4ed/RetICL</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14502v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14502v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14507v1",
    "title": "Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models",
    "latest": "2023-05-25T16:41:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430315774137310",
      "content": "<p>\ud83d\udcdd Deduction Under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models \ud83d\udcda</p><p>\"Involves logical reasoning over manipulated facts in prompts and a student simulation model inspired prompt setting to mitigate the deduction accuracy drop to some extent for LLMs such as GPT models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14507v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14507v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14497v1",
    "title": "https://arxiv.org/abs/2305.14497v1",
    "latest": "2023-05-25T16:21:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110430237115444548",
      "content": "<p>\ud83d\udcdd Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement \ud83d\udcda\ud83d\udc7e</p><p>\"Self-Polish is a self-supervised learning method based on prompting, which teaches models to progressively refine the given problems to be more comprehensible and solvable.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/WooooDyy/Self-Polish\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/WooooDyy/Self-Polis</span><span class=\"invisible\">h</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.14497v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14497v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13888v1",
    "title": "PaD: Program-aided Distillation Specializes Large Models in Reasoning",
    "latest": "2023-05-25T09:57:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428727161918699",
      "content": "<p>\ud83d\udcdd PaD: Program-Aided Distillation Specializes Large Models in Reasoning \ud83d\udcda</p><p>\"We distill LLMs to specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking, which is a general-purpose approach.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13888v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13888v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41559-023-02008-w",
    "title": "Better incentives are needed to reward academic software development - Nature Ecology & Evolution",
    "latest": "2023-05-25T09:29:58+00:00",
    "last_post": {
      "url": "https://datasci.social/@mszll/110428618663085148",
      "content": "<p>Better incentives are needed to reward academic software development<br><a href=\"https://www.nature.com/articles/s41559-023-02008-w\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41559-023</span><span class=\"invisible\">-02008-w</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      },
      {
        "url": "https://fediscience.org/@tschfflr",
        "display_name": "Tatjana Scheffler"
      },
      {
        "url": "https://mstdn.social/@felwert",
        "display_name": "Frederik Elwert"
      },
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      },
      {
        "url": "https://fosstodon.org/@mdsumner",
        "display_name": "mdsumner"
      },
      {
        "url": "https://vis.social/@jhilden",
        "display_name": "Jonatan Hild\u00e9n"
      },
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13857v1",
    "title": "https://arxiv.org/abs/2305.13857v1",
    "latest": "2023-05-25T09:27:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428609258089204",
      "content": "<p>\ud83d\udcdd Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation \ud83d\udcda</p><p>\"Conducts an interactive user study to unveil the vulnerabilities of task-oriented dialogue (TOD) systems against realistic scenarios that have not been explored before, in which users do not follow instructions precisely due to their lack of understanding.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/budzianowski/multiwoz\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/budzianowski/multiw</span><span class=\"invisible\">oz</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13857v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13857v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13850v1",
    "title": "Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document",
    "latest": "2023-05-25T09:07:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428530401311286",
      "content": "<p>\ud83d\udcdd Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document \ud83d\udcda\ud83d\udc7e</p><p>\"Given an image of the document, GOSE firstly generates the preliminary relation predictions on entity pairs; secondly, it captures the global structure knowledge based on the prediction result of the previous iteration and further incorporates global structure knowledge into entity representations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13850v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13850v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13844v1",
    "title": "https://arxiv.org/abs/2305.13844v1",
    "latest": "2023-05-25T08:57:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428491092824948",
      "content": "<p>\ud83d\udcdd Arukikata Travelogue Dataset with Geographic Entity Mention, Coreference, and Link Annotation \ud83d\udcda</p><p>\"The dataset is created by annotating travelogue documents in Japanese with mentions, coreference clusters, and geo-entities based on geo-databases such as GeoNames and Geonames.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/megagonlabs/ginza\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/megagonlabs/ginza</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13844v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13844v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13829v1",
    "title": "Learn from Mistakes through Cooperative Interaction with Study Assistant",
    "latest": "2023-05-25T08:37:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428412538266960",
      "content": "<p>\ud83d\udcdd Learn From Mistakes Through Cooperative Interaction with Study Assistant \ud83d\udcda</p><p>\"SALAM collects mistakes from previous responses in the training phase and provides general guidance during inference based on the mistake collections, so the LLM can avoid making similar mistakes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13829v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13829v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13820v1",
    "title": "An Open Dataset and Model for Language Identification",
    "latest": "2023-05-25T08:07:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428294254662007",
      "content": "<p>\ud83d\udcdd An Open Dataset and Model for Language Identification \ud83d\udcda</p><p>\"A language identification model which achieves a macro-average F1 score of 0-93 across 201 languages by training on a curated dataset of monolingual data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13820v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13820v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13812v1",
    "title": "https://arxiv.org/abs/2305.13812v1",
    "latest": "2023-05-25T07:47:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428216056252125",
      "content": "<p>\ud83d\udcdd Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality \ud83d\udcda\ud83d\udd2d</p><p>\"Scene graph decomposition and augmentation along with coarse-to-fine contrastive learning that aligns sentences of various complexities to the same image, along with novel negative mining techniques in the scene graph space.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/vacancy/SceneGraphParser\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/vacancy/SceneGraphP</span><span class=\"invisible\">arser</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13812v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13812v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13808v1",
    "title": "Asking Clarification Questions to Handle Ambiguity in Open-Domain QA",
    "latest": "2023-05-25T07:37:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428176693835208",
      "content": "<p>\ud83d\udcdd Asking Clarification Questions to Handle Ambiguity in Open-Domain QA \ud83d\udcda</p><p>\"Proposes an approach based on GPT-2 to automatically generate clarification questions given an ambiguous question, a set of passages that contain the answer, and a set of possible answers.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13808v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13808v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13805v1",
    "title": "Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path",
    "latest": "2023-05-25T07:17:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428097967246318",
      "content": "<p>\ud83d\udcdd Towards Zero-Shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path \ud83d\udcda</p><p>\"ReXMiner encodes the shortest relative paths in the Document Object Model (DOM) tree which is a more accurate and efficient signal for key-value pair extraction within a web page.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13805v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13805v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13794v1",
    "title": "Personalized Predictive ASR for Latency Reduction in Voice Assistants",
    "latest": "2023-05-25T06:57:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110428019342090959",
      "content": "<p>\ud83d\udcdd Personalized Predictive ASR for Latency Reduction in Voice Assistants \ud83d\udcda</p><p>\"Predictive automatic speech recognition (ASR) involves predicting the full utterance from a partially observed utterance and passing the predicted utterance to downstream systems in order to prefetch and cache a response.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13794v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13794v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13788v1",
    "title": "Can Large Language Models Infer and Disagree Like Humans?",
    "latest": "2023-05-25T06:37:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427940521509751",
      "content": "<p>\ud83d\udcdd Can Large Language Models Infer and Disagree Like Humans? \ud83d\udcda\ud83d\udc7e</p><p>\"Uses two different techniques, Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR), to test LLMs\u2019 ability and alignment with human disagreement distribution on NLI.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13788v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13788v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13785v1",
    "title": "Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation",
    "latest": "2023-05-25T06:27:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427901326705894",
      "content": "<p>\ud83d\udcdd Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation \ud83d\udcda</p><p>\"Optimizes few-shot text classification without accessing the gradients of the underlying LLMs through prompt-based data augmentation, treating the black-box model as a feature extractor and training a classifier on the extracted features.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13785v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13785v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13782v1",
    "title": "https://arxiv.org/abs/2305.13782v1",
    "latest": "2023-05-25T06:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427862076106745",
      "content": "<p>\ud83d\udcdd Images in Language Space: Exploring the Suitability of Large Language Models for Vision &amp; Language Tasks \ud83d\udcda</p><p>\"Uses separate verbalisation models to verbalise visual information into text before using it to solve visual-linguistic tasks with a language model like GPT-3.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clp-research/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/clp-research/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13782v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13782v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13776v1",
    "title": "Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation",
    "latest": "2023-05-25T06:07:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427822770223273",
      "content": "<p>\ud83d\udcdd Counterspeeches Up My Sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation \ud83d\udcda\ud83d\udc7e</p><p>\"A two stage framework for intent-conditioned counterspeech generation that leverages vector-quantized representations learned for each intent category along with PerFuMe, a novel fusion module to incorporate intent-specific information into the model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13776v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13776v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13755v1",
    "title": "https://arxiv.org/abs/2305.13755v1",
    "latest": "2023-05-25T05:47:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427744116928214",
      "content": "<p>\ud83d\udcdd Topic-Driven Distant Supervision Framework for Macro-Level Discourse Parsing \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a distant supervision framework that leverages the relations between topic structure and rhetorical structure to narrow the gap between in-domain and out-of-domain tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/fjiangAI/TDSF_DP\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/fjiangAI/TDSF_DP</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13755v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13755v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.32388/ZNWI7T",
    "title": "https://doi.org/10.32388/ZNWI7T",
    "latest": "2023-05-25T05:20:09.312000+00:00",
    "last_post": {
      "url": "https://scholar.social/@egonw/110425122353070829",
      "content": "<p>Jente Houweling and I wrote up the idea (based on discussions, what we read, etc) that resulted from her research that we should replace \"data management\" with \"research output management\". Because we have a very narrow view of what data is, which does not reflect the research process.</p><p>We wrote up a definition and are looking for your peer review! Please be Reviewer of this short output: <a href=\"https://doi.org/10.32388/ZNWI7T\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.32388/ZNWI7T</span><span class=\"invisible\"></span></a></p><p><a href=\"https://scholar.social/tags/peerWanted\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>peerWanted</span></a></p>"
    },
    "people": [
      {
        "url": "https://scholar.social/@telliott",
        "display_name": "Tom Elliott"
      },
      {
        "url": "https://mstdn.social/@felwert",
        "display_name": "Frederik Elwert"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13751v1",
    "title": "Challenges in Context-Aware Neural Machine Translation",
    "latest": "2023-05-25T05:17:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427626029957175",
      "content": "<p>\ud83d\udcdd Challenges in Context-Aware Neural Machine Translation \ud83d\udcda</p><p>\"Collects a new dataset of Chinese-English novels with sentence and document-level segmentation to facilitate more realistic research on context-aware machine translation for the document-level setting.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13751v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13751v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13749v1",
    "title": "https://arxiv.org/abs/2305.13749v1",
    "latest": "2023-05-25T04:57:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427547542304940",
      "content": "<p>\ud83d\udcdd Goal-Driven Explainable Clustering via Language Descriptions \ud83d\udcda</p><p>\"We first prompt a language model with \"[corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ZihanWangKi/GoalEx\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/ZihanWangKi/GoalEx</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13749v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13749v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13735v1",
    "title": "Aligning Large Language Models through Synthetic Feedback",
    "latest": "2023-05-25T04:17:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427390385698115",
      "content": "<p>\ud83d\udcdd Aligning Large Language Models Through Synthetic Feedback \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Our 7B-sized model outperforms the 12-13B-sized models with about 75% winning rate on average on the A/B tests, which use GPT-4 as the judge.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13735v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13735v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13738v1",
    "title": "i-Code Studio: A Configurable and Composable Framework for Integrative AI",
    "latest": "2023-05-25T04:07:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427350926832179",
      "content": "<p>\ud83d\udcdd I-Code Studio: A Configurable and Composable Framework for Integrative AI \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"Works by orchestrating multiple pre-trained models in a fine-tuning-free fashion, and is able to conduct complex multimodal tasks such as video-to-text retrieval and visual question answering.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13738v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13738v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://academic.oup.com/jxb/advance-article/doi/10.1093/jxb/erad167/7173266",
    "title": "design of synthetic gene circuits in plants: new components, old challenges",
    "latest": "2023-05-25T03:53:43+00:00",
    "last_post": {
      "url": "https://genomic.social/@jamespblloyd/110427196190556368",
      "content": "<p>A really great primer on the current state of plant synthetic biology, with a focus on synthetic gene circuits. </p><p>A must read for anyone interested in plant synbio. </p><p>The design of synthetic gene circuits in plants: new components, old challenges</p><p><a href=\"https://academic.oup.com/jxb/advance-article/doi/10.1093/jxb/erad167/7173266\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">academic.oup.com/jxb/advance-a</span><span class=\"invisible\">rticle/doi/10.1093/jxb/erad167/7173266</span></a></p>"
    },
    "people": [
      {
        "url": "https://bayes.club/@akhilrao",
        "display_name": "edgeworth boxlord"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13733v1",
    "title": "https://arxiv.org/abs/2305.13733v1",
    "latest": "2023-05-25T03:27:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427193570555033",
      "content": "<p>\ud83d\udcdd Self-Critique Prompting with Large Language Models for Inductive Instructions \ud83d\udcda</p><p>\"Self-Critique prompting enables models to not only self-criticize but also self-correct, which shows promising results in handling inductive instructions under zero-shot and few-shot settings.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/DevoAllen/INDust\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/DevoAllen/INDust</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13733v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13733v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13723v1",
    "title": "PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training",
    "latest": "2023-05-25T03:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427154322898362",
      "content": "<p>\ud83d\udcdd PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training \ud83d\udcda</p><p>\"PromptClass is composed of: (A) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13723v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13723v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2302.06716v3.pdf",
    "title": "https://arxiv.org/pdf/2302.06716v3.pdf",
    "latest": "2023-05-25T02:53:29+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110427059641306942",
      "content": "<p>\"Machine Learning Model Attribution Challenge\"</p><p><a href=\"https://arxiv.org/pdf/2302.06716v3.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2302.06716v3.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mastodon.radio/tags/forensics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>forensics</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.13812.pdf",
    "title": "https://arxiv.org/pdf/2305.13812.pdf",
    "latest": "2023-05-25T02:48:33+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110427040198458594",
      "content": "<p>RT @Harman26Singh@twitter.com</p><p>\ud83d\udea8New paper\ud83d\udea8<br>Contrastively trained Vision-Language Model\u2019s (like CLIP) are poor at compositional reasoning</p><p>Our new paper improves this:<br>\u201cCoarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality\u201d</p><p><a href=\"https://arxiv.org/pdf/2305.13812.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.13812.pdf</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Harman26Singh/status/1661335953175355392\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Harman26Singh/stat</span><span class=\"invisible\">us/1661335953175355392</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13721v1",
    "title": "Continual Dialogue State Tracking via Example-Guided Question Answering",
    "latest": "2023-05-25T02:47:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110427036167124444",
      "content": "<p>\ud83d\udcdd Continual Dialogue State Tracking via Example-Guided Question Answering \ud83d\udcda\ud83d\udc7e</p><p>\"A model trained on DST is reformulated as an example-guided QA framework, where a model is prompted to predict the change in dialogue state given a dialogue history and an in-context example.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13721v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13721v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2303.12132v1.pdf",
    "title": "https://arxiv.org/pdf/2303.12132v1.pdf",
    "latest": "2023-05-25T02:32:00+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110426975159173620",
      "content": "<p>\"Fundamentals of Generative Large Language Models<br>and Perspectives in Cyber-Defense\"</p><p>This review aims to provide a<br>brief overview of the history, state of the art, and implications of Generative Language Models in<br>terms of their principles, abilities, limitations, and future prospects \u2013 especially in the context of<br>cyber-defense, with a focus on the Swiss operational environment.</p><p><a href=\"https://arxiv.org/pdf/2303.12132v1.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2303.12132v1.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a>  <a href=\"https://mastodon.radio/tags/infosec\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>infosec</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15328",
    "title": "https://arxiv.org/abs/2305.15328",
    "latest": "2023-05-25T02:28:36+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426961764083786",
      "content": "<p>Visual Programming for Text-to-Image Generation and Evaluation</p><p>Presents VPGen, an interpretable step-by-step T2I generation framework that decomposes the task into multiple steps.</p><p>proj: <a href=\"https://vp-t2i.github.io\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">vp-t2i.github.io</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.15328\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15328</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13718v1",
    "title": "https://arxiv.org/abs/2305.13718v1",
    "latest": "2023-05-25T02:27:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426957620405347",
      "content": "<p>\ud83d\udcdd LogicLLM: Exploring Self-Supervised Logic-Enhanced Training for Large Language Models \ud83d\udcda</p><p>\"The proposed LogicLLM first incorporates logical knowledge through self-supervised post-training and activates it via in-context learning with an auto-regressive objective variant of MERIt.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/SparkJiao/MERIt-v2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/SparkJiao/MERIt-v2</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13718v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13718v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15080",
    "title": "Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
    "latest": "2023-05-25T02:18:29+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426921961551530",
      "content": "<p>Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models</p><p>Significantly outperforms the existing SotA models on visual document understanding</p><p><a href=\"https://arxiv.org/abs/2305.15080\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15080</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13712v1",
    "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
    "latest": "2023-05-25T02:07:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426878834385383",
      "content": "<p>\ud83d\udcdd Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"Collects a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty, as well as a semantic evaluation method to quantify the uncertainty expressed in the answers.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13712v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13712v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15074",
    "title": "https://arxiv.org/abs/2305.15074",
    "latest": "2023-05-25T02:00:32+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426851381371703",
      "content": "<p>Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models</p><p>Presents JEEBench, a benchmark w/ 450 pre-engineering math, physics and chemistry problems from the IIT JEE-Advanced Exam</p><p>repo: <a href=\"https://github.com/hgaurav2k/JEEBench\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/hgaurav2k/JEEBench</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.15074\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15074</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13711v1",
    "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
    "latest": "2023-05-25T01:57:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426839490239854",
      "content": "<p>\ud83d\udcdd LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"LLM-Eval is a unified evaluation framework consisting of 4 dimensions for assessing open-domain conversation quality, including relevance, informativeness, knowledgeability, and engagingness.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13711v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13711v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13710v1",
    "title": "Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems",
    "latest": "2023-05-25T01:37:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426761010404911",
      "content": "<p>\ud83d\udcdd Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems \ud83d\udcda</p><p>\"An end-to-end task-oriented dialogue system with an interactive textual interface is trained to process external knowledge through natural language understanding and reasoning with an attention mechanism.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13710v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13710v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15324",
    "title": "Model evaluation for extreme risks",
    "latest": "2023-05-25T01:32:30+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426741170838224",
      "content": "<p>Model evaluation for extreme risks</p><p><a href=\"https://arxiv.org/abs/2305.15324\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15324</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13703v1",
    "title": "MemeCap: A Dataset for Captioning and Interpreting Memes",
    "latest": "2023-05-25T01:17:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426682416685071",
      "content": "<p>\ud83d\udcdd MemeCap: A Dataset for Captioning and Interpreting Memes \ud83d\udcda</p><p>\"Proposes a new task of meme captioning and release a new dataset of 6300 memes along with captions, visual metaphors and literal captions.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13703v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13703v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15334",
    "title": "https://arxiv.org/abs/2305.15334",
    "latest": "2023-05-25T01:15:29+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426674246413641",
      "content": "<p>Gorilla: Large Language Model Connected with Massive APIs</p><p>Releases Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls.</p><p>proj: <a href=\"https://gorilla.cs.berkeley.edu/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">gorilla.cs.berkeley.edu/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.15334\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15334</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15348",
    "title": "READ: Recurrent Adaptation of Large Transformers",
    "latest": "2023-05-25T01:10:30+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426654638888250",
      "content": "<p>READ: Recurrent Adaptation of Large Transformers</p><p>Presents READ, a parameter-efficient transfer learning method that outperforms other methods while consuming significantly lower energy.</p><p><a href=\"https://arxiv.org/abs/2305.15348\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15348</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13698v1",
    "title": "https://arxiv.org/abs/2305.13698v1",
    "latest": "2023-05-25T01:07:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426643096373835",
      "content": "<p>\ud83d\udcdd Exploring Large Language Models for Classical Philology \ud83d\udcda</p><p>\"Creates four language models for Ancient Greek that vary along two dimensions: Explores encoder-only and encoder-decoder architectures using two model types, RoBERTa and T5.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Heidelberg-NLP/ancient-language-models\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/Heidelberg-NLP/anci</span><span class=\"invisible\">ent-language-models</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13698v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13698v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.15393",
    "title": "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models",
    "latest": "2023-05-25T01:04:29+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110426630991534389",
      "content": "<p>LayoutGPT: Compositional Visual Planning and Generation with Large Language Models</p><p>Outperforms text-to-image models/systems by 20-40% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness.</p><p><a href=\"https://arxiv.org/abs/2305.15393\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.15393</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13697v1",
    "title": "UNIMO-3: Multi-granularity Interaction for Vision-Language Representation Learning",
    "latest": "2023-05-25T00:47:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426564383579553",
      "content": "<p>\ud83d\udcdd UNIMO-3: Multi-Granularity Interaction for Vision-Language Representation Learning \ud83d\udcda</p><p>\"UNIMO-3 model can establish effective connections between different layers in a cross-modal encoder, and adaptively capture the interaction between two modalities at different levels, which leads to the cross-modal interaction collapsing into a limited multi-modal semantic information interaction.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13697v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13697v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13685v1",
    "title": "Causal Intervention for Abstractive Related Work Generation",
    "latest": "2023-05-25T00:07:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426407155051218",
      "content": "<p>\ud83d\udcdd Causal Intervention for Abstractive Related Work Generation \ud83d\udcda</p><p>\"Causal Intervention Module for Related Work Generation (CaM) to effectively capture causalities in the generation process and improve the quality and coherence of the generated related works.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13685v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13685v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13691v1",
    "title": "Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis",
    "latest": "2023-05-24T23:57:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426367857397099",
      "content": "<p>\ud83d\udcdd Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis \ud83d\udcda</p><p>\"Built upon the data generation functions parameterized by LLMs and prompts, which requires minimal hand-crafted features or heuristics to be defined for multi-hop QA and fact verification tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13691v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13691v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05928",
    "title": "WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia",
    "latest": "2023-05-24T23:53:04+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wikiresearch/110426350203980803",
      "content": "<p>\"WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia\" WikiSQE has about 3.4 M sentences with 153 quality labels</p><p>(Ando et al, 2023)</p><p><a href=\"https://arxiv.org/abs/2305.05928\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05928</span><span class=\"invisible\"></span></a> <a href=\"https://twitter.com/WikiResearch/status/1658042531643367424\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/WikiResearch/statu</span><span class=\"invisible\">s/1658042531643367424</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@wikiresearch",
        "display_name": "WikiResearch"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13684v1",
    "title": "mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models",
    "latest": "2023-05-24T23:27:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426249942645112",
      "content": "<p>\ud83d\udcdd mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models \ud83d\udcda</p><p>\"Capable of selecting better source language by inducing language similarities from mPLMs using multi-parallel corpora, resulting in 1%-2% improvement on cross-lingual transfer.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13684v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13684v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13683v1",
    "title": "https://arxiv.org/abs/2305.13683v1",
    "latest": "2023-05-24T23:07:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110426170945203225",
      "content": "<p>\ud83d\udcdd Error Detection for Text-to-SQL Semantic Parsing \ud83d\udcda</p><p>\"Based on pre-trained language model of code and is enhanced with structural features learned by graph neural networks to detect erroneous SQL query predictions on the fly without access to the parser.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/antlr/grammars-v4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/antlr/grammars-v4</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13683v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13683v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11675",
    "title": "https://arxiv.org/abs/2305.11675",
    "latest": "2023-05-24T23:05:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110426161345853731",
      "content": "<p>High-Quality Video Reconstruction from Brain Activity - <a href=\"https://arxiv.org/abs/2305.11675\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11675</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14314",
    "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "latest": "2023-05-24T23:05:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110426161336484099",
      "content": "<p>QLoRA: Efficient Finetuning of Quantized LLMs - <a href=\"https://arxiv.org/abs/2305.14314\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14314</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@bergi",
        "display_name": "Thomas Bergwinkl"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13669v1",
    "title": "Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment",
    "latest": "2023-05-24T21:57:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425895873376639",
      "content": "<p>\ud83d\udcdd Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment \ud83d\udcda\ud83d\udc7e</p><p>\"MixAlign is a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13669v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13669v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13668v1",
    "title": "Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations",
    "latest": "2023-05-24T21:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425817274498814",
      "content": "<p>\ud83d\udcdd Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations \ud83d\udcda\ud83e\udde0</p><p>\"Learns a similarity metric for each object, such that when the similarity between object A and a given verb is higher than the similarity between object B and that verb, the agent will more likely choose object A than object B in a given context.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13668v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13668v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13667v1",
    "title": "https://arxiv.org/abs/2305.13667v1",
    "latest": "2023-05-24T21:27:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425777781700447",
      "content": "<p>\ud83d\udcdd Optimizing Non-Autoregressive Transformers with Contrastive Learning \ud83d\udcda</p><p>\"Proposed to ease the difficulty of modality learning via sampling from the model distribution instead of the data distribution, and derive contrastive constraints to stabilize the training process and integrate this resulting objective with the state-of-the-art NAT architecture DA-Transformer.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/thu-coai/DA-Transformer\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/thu-coai/DA-Transfo</span><span class=\"invisible\">rmer</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13667v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13667v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14296",
    "title": "USB: A Unified Summarization Benchmark Across Tasks and Domains",
    "latest": "2023-05-24T21:18:38+00:00",
    "last_post": {
      "url": "https://hci.social/@jbigham/110425742959387131",
      "content": "<p>My students and collaborators just released a new summarization benchmark that forefronts factuality and evidence extraction -- </p><p>\u26a1Introducing USB: A Unified Summarization Benchmark Across Tasks and Domains!</p><p><a href=\"https://arxiv.org/abs/2305.14296\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14296</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@jbigham",
        "display_name": "Jeff Bigham"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13658v1",
    "title": "Understanding compositional data augmentation in automatic morphological inflection",
    "latest": "2023-05-24T21:07:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425699476851475",
      "content": "<p>\ud83d\udcdd Understanding Compositional Data Augmentation in Automatic Morphological Inflection \ud83d\udcda</p><p>\"A data augmentation method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples for automatic morphological inflection generation tasks such as lemmatization and morphological analysis.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13658v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13658v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.09848?s=09",
    "title": "Evaluating Verifiability in Generative Search Engines",
    "latest": "2023-05-24T20:48:37+00:00",
    "last_post": {
      "url": "https://mastodon.cloud/@aarontay/110276375288247354",
      "content": "<p>[Read] Evaluating Verifiability in Generative Search Engines - finally a paper that assesses how reliable Search+Large language models (LLM) type systems like Bing Chat, perplexity.ai, you chat, neevaai <a href=\"https://arxiv.org/abs/2304.09848?s=09\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.09848?s=09</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13657v1",
    "title": "ChatGPT as your Personal Data Scientist",
    "latest": "2023-05-24T20:47:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425620529901551",
      "content": "<p>\ud83d\udcdd ChatGPT as Your Personal Data Scientist \ud83d\udcda</p><p>\"ChatGPT acts as an intelligent agent that assists users in their data science tasks through intuitive, natural conversations and without requiring in-depth knowledge of the underlying ML processes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13657v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13657v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14327",
    "title": "https://arxiv.org/abs/2305.14327",
    "latest": "2023-05-24T20:37:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110425579685882242",
      "content": "<p>RT @Wade_Yin9712@twitter.com</p><p>Introducing \ud83e\udd96Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation! <br>Dynosaur contains 800K generated instruction-tuning data and can \u2b06\ufe0f**dynamically** grow!</p><p>Project page: <a href=\"https://dynosaur-it.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">dynosaur-it.github.io/</span><span class=\"invisible\"></span></a><br>Preprint: <a href=\"https://arxiv.org/abs/2305.14327\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14327</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/WadeYin9712/Dynosaur\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/WadeYin9712/Dynosau</span><span class=\"invisible\">r</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Wade_Yin9712/status/1661237791920132098\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Wade_Yin9712/statu</span><span class=\"invisible\">s/1661237791920132098</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1093/scipol/scac081",
    "title": "https://doi.org/10.1093/scipol/scac081",
    "latest": "2023-05-24T19:54:47+00:00",
    "last_post": {
      "url": "https://hcommons.social/@jcpeyssard/110425413242180989",
      "content": "<p>Mikael Laakso , Anna-Maija Multas, European scholarly journals from small- and mid-size publishers: mapping journals and public funding mechanisms, Science and Public Policy, 2023 <a href=\"https://doi.org/10.1093/scipol/scac081\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.1093/scipol/scac081</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@RonaldSnijder",
        "display_name": "Ronald Snijder"
      },
      {
        "url": "https://hcommons.social/@jcpeyssard",
        "display_name": "Jean-Christophe Peyssard"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13654v1",
    "title": "Understanding and Mitigating Spurious Correlations in Text Classification",
    "latest": "2023-05-24T19:47:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425384605481370",
      "content": "<p>\ud83d\udcdd Understanding and Mitigating Spurious Correlations in Text Classification \ud83d\udcda</p><p>\"Prevents models from learning certain spurious features by introducing a regularization loss during fine-tuning process of pre-trained language models (PLM), so that the model will not rely on those spurious features and generalize on out-of-distribution data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13654v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13654v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13048",
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "latest": "2023-05-24T19:38:07+00:00",
    "last_post": {
      "url": "https://arvr.social/@mpesce/110417099505946407",
      "content": "<p>During my keynote this morning I noted that in the three weeks I'd been keynoting <a href=\"https://arvr.social/tags/pwctheoutside\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>pwctheoutside</span></a> the field of artificial intelligence had advanced at least three years.</p><p>Here's today's massive research breakthrough. It might be the AI breakthrough of the year -- in any other year.</p><p><a href=\"https://arxiv.org/abs/2305.13048\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13048</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@davidmortensen",
        "display_name": "David Mortensen"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13648v1",
    "title": "Non-parametric, Nearest-neighbor-assisted Fine-tuning for Neural Machine Translation",
    "latest": "2023-05-24T19:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425345363936146",
      "content": "<p>\ud83d\udcdd Non-Parametric, Nearest-Neighbor-Assisted Fine-Tuning for Neural Machine Translation \ud83d\udcda\ud83d\udc7e</p><p>\"Uses a non-parametric kNN method to improve machine translation by incorporating statistics into the gradient update of a fine-tuned baseline NMT model with a gating mechanism, the ground truth probability from the kNN model, and a reinforcement learning method.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13648v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13648v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14337",
    "title": "Anchor Prediction: Automatic Refinement of Internet Links",
    "latest": "2023-05-24T19:19:34+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110425274766638680",
      "content": "<p>RT @nelsonfliu@twitter.com</p><p>Most links on the Internet are unanchored\u2014they simply point to a target webpage as a whole.</p><p>We define and explore the task of *anchor prediction*: given a link in its source context, can we directly point readers to relevant parts of the target webpage?</p><p><a href=\"https://arxiv.org/abs/2305.14337\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14337</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/nelsonfliu/status/1661265876912582658\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/nelsonfliu/status/</span><span class=\"invisible\">1661265876912582658</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13645v1",
    "title": "mPMR: A Multilingual Pre-trained Machine Reader at Scale",
    "latest": "2023-05-24T19:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425266881405590",
      "content": "<p>\ud83d\udcdd mPMR: A Multilingual Pre-Trained Machine Reader at Scale \ud83d\udcda</p><p>\"MPMR pre-trains mPLMs with multilingual machine reading comprehension data and fine-tunes mPLMs with downstream tasks to perform sequence classification and span extraction, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13645v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13645v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.14342",
    "title": "http://arxiv.org/abs/2305.14342",
    "latest": "2023-05-24T19:15:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110425258953264039",
      "content": "<p>RT @tengyuma@twitter.com</p><p>Adam, a 9-yr old optimizer, is the go-to for training LLMs (eg, GPT-3, OPT, LLAMA).</p><p>Introducing Sophia, a new optimizer that is 2x faster than Adam on LLMs. Just a few more lines of code could cut your costs from $2M to $1M (if scaling laws hold).</p><p><a href=\"http://arxiv.org/abs/2305.14342\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.14342</span><span class=\"invisible\"></span></a> \ud83e\uddf5\u2b07\ufe0f</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/tengyuma/status/1661412995430219786\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/tengyuma/status/16</span><span class=\"invisible\">61412995430219786</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13641v1",
    "title": "https://arxiv.org/abs/2305.13641v1",
    "latest": "2023-05-24T18:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425188057946085",
      "content": "<p>\ud83d\udcdd AxomiyaBERTa: A Phonologically-Aware Transformer Model for Assamese \ud83d\udcda</p><p>\"The training procedure of AxomiyaBERTa is different from standard BERT models: it only uses the masked language modeling training objective, while standard models use both masked language modeling as well as the next-sentence prediction objective.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/csu-signal/axomiyaberta\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/csu-signal/axomiyab</span><span class=\"invisible\">erta</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13641v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13641v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1057/s41304-023-00431-y",
    "title": "Alternative metrics, traditional problems? Assessing gender dynamics in the altmetrics of political science - European Political Science",
    "latest": "2023-05-24T18:49:32+00:00",
    "last_post": {
      "url": "https://fediscience.org/@petersuber/110425156663480927",
      "content": "<p>Update. In political science, \"journal articles authored exclusively by female scholars score 27% lower on average [on Altmetric Attention Scores, AAS] than exclusively male-authored outputs. However, men are also more likely to write articles with an AAS of zero. These patterns are shaped by the presence of high-scoring male 'superstars' whose research attracts much online attention.\"<br><a href=\"https://link.springer.com/article/10.1057/s41304-023-00431-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">057/s41304-023-00431-y</span></a></p><p><a href=\"https://fediscience.org/tags/Altmetrics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Altmetrics</span></a> <a href=\"https://fediscience.org/tags/Gender\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Gender</span></a> <a href=\"https://fediscience.org/tags/PoliticalScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PoliticalScience</span></a></p>"
    },
    "people": [
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      },
      {
        "url": "https://fediscience.org/@petersuber",
        "display_name": "petersuber"
      }
    ]
  },
  {
    "link": "https://journals.sagepub.com/doi/10.1177/1532673X231175625",
    "title": "journals.sagepub.com/doi/10.11...",
    "latest": "2023-05-24T18:41:38+00:00",
    "last_post": {
      "url": "https://mastodon.social/@owasow/110425125607399404",
      "content": "<p>New study finds people in Iowa \u201cliving closer to BLM protests show greater support for BLM movement and, to a lesser extent, for defunding the police. Results suggest protests may affect public opinion, but only within a very narrow range of a few miles.\u201d <a href=\"https://journals.sagepub.com/doi/10.1177/1532673X231175625\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.sagepub.com/doi/10.11</span><span class=\"invisible\">77/1532673X231175625</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@owasow",
        "display_name": "Omar Wasow"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13632v1",
    "title": "https://arxiv.org/abs/2305.13632v1",
    "latest": "2023-05-24T18:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425109452118197",
      "content": "<p>\ud83d\udcdd Detecting and Mitigating Hallucinations in Multilingual Summarisation \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"MFACT is a novel metric for the faithfulness evaluation of generated summaries on non-English language, which transfers existing English metric FACT using multi-lingual pretrained model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/yfqiu-nlp/mfact-summ\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/yfqiu-nlp/mfact-sum</span><span class=\"invisible\">m</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13632v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13632v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.acalib.2023.102734",
    "title": "doi.org/10.1016/j.acalib.2023....",
    "latest": "2023-05-24T18:28:24+00:00",
    "last_post": {
      "url": "https://fediscience.org/@petersuber/110425073539653825",
      "content": "<p>More on the <a href=\"https://fediscience.org/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> citation advantage (<a href=\"https://fediscience.org/tags/OACA\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OACA</span></a>). <br><a href=\"https://doi.org/10.1016/j.acalib.2023.102734\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.acalib.2023.</span><span class=\"invisible\">102734</span></a></p><p>\"Articles under the hybrid gold modality are cited on average twice as much as those in the gold modality, regardless of funding\u2026Funded articles generally obtain 50% more citations than unfunded ones within the same publication modality. Open access <a href=\"https://fediscience.org/tags/repositories\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>repositories</span></a> significantly increase citations, particularly for articles w/o funding\u2026Articles in OA repositories receive 50% more citations than paywalled ones.\"</p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@petersuber",
        "display_name": "petersuber"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13637v1",
    "title": "IdEALS: Idiomatic Expressions for Advancement of Language Skills",
    "latest": "2023-05-24T18:27:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110425070148083340",
      "content": "<p>\ud83d\udcdd IdEALS: Idiomatic Expressions for Advancement of Language Skills \ud83d\udcda</p><p>\"We curate training and testing corpora from real-world data, then evaluate various approaches and compare their performance against human experts and existing GEC tools, including Grammarly, LanguageTool and Grammarly's Customized Grammar Assistant (GCA).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13637v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13637v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13628v1",
    "title": "Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning",
    "latest": "2023-05-24T18:07:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424991511924398",
      "content": "<p>\ud83d\udcdd Improving Self-Training for Cross-Lingual Named Entity Recognition with Contrastive and Prototype Learning \ud83d\udcda</p><p>\"Contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13628v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13628v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05363",
    "title": "https://doi.org/10.21105/joss.05363",
    "latest": "2023-05-24T18:06:15+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110424986421050772",
      "content": "<p>Just published in JOSS: 'sptotal: an R package for predicting totals and weighted sums from spatial data' <a href=\"https://doi.org/10.21105/joss.05363\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05363</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13627v1",
    "title": "https://arxiv.org/abs/2305.13627v1",
    "latest": "2023-05-24T17:57:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424952229757790",
      "content": "<p>\ud83d\udcdd Instruct-Align: Teaching Novel Languages with to LLMs Through Alignment-Based Cross-Lingual Instruction \ud83d\udcda\ud83d\udc7e</p><p>\"By learning cross-lingual alignment between unseen and previously learned languages via alignment-based cross-lingual instruction tuning and experience replay of previously learned languages via continual instruction tuning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/IndoNLP/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/IndoNLP/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13627v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13627v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05226",
    "title": "https://doi.org/10.21105/joss.05226",
    "latest": "2023-05-24T17:44:33+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110424901150947173",
      "content": "<p>Just published in JOSS: 'osiris: An R package to process climate impacts on agricultural yields for the Global Change Analysis Model' <a href=\"https://doi.org/10.21105/joss.05226\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05226</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13626v1",
    "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
    "latest": "2023-05-24T17:37:35+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424873715485077",
      "content": "<p>\ud83d\udcdd Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-Guided, and Non-Collaboration \ud83d\udcda</p><p>\"A Proactive Chain-of-Thought prompting scheme is proposed to trigger the goal planning capability over descriptive reasoning chains and equip LLMs with proactivity, especially, for three aspects: clarification, target-guided, and non-collaborative dialogues.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13626v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13626v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13617v1",
    "title": "https://arxiv.org/abs/2305.13617v1",
    "latest": "2023-05-24T17:17:34+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424795004115201",
      "content": "<p>\ud83d\udcdd SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"SPEECH first models complex dependency among event structured components with energy-based modeling, and then represents event classes with simple hyperspheres, where the hyperspheres can be learned with only a few annotated event instances.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/zjunlp/SPEECH\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/zjunlp/SPEECH</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13617v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13617v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13602v1",
    "title": "https://arxiv.org/abs/2305.13602v1",
    "latest": "2023-05-24T16:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424716178234241",
      "content": "<p>\ud83d\udcdd ReSee: Responding Through Seeing Fine-Grained Visual Knowledge in Open-Domain Dialogue \ud83d\udcda</p><p>\"Provides new paradigm to construct multimodal dialogue datasets from text-only dialogues, and a simple but effective dialogue model ReSee to add visual representation into vanilla dialogue models by modality concatenations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Maluuba/nlg-eval\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Maluuba/nlg-eval</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13602v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13602v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.trd.2023.103757",
    "title": "doi.org/10.1016/j.trd.2023.103...",
    "latest": "2023-05-24T16:49:53+00:00",
    "last_post": {
      "url": "https://datasci.social/@UrbanDemog/110424592523320991",
      "content": "<p>Super glad to see our study \"Estimating public transport emissions from GTFS data\" finally published on Transp. Research Part D. Amazing collaboration with Joao Bazzo &amp; P. Andrade <br>\ud83d\udcd1Paper: <a href=\"https://doi.org/10.1016/j.trd.2023.103757\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.trd.2023.103</span><span class=\"invisible\">757</span></a> <br>\ud83d\udd13ungated PDF: <a href=\"https://urbandemographics.org/publication/2023_trpd_public_transport_emissions_gtfs2emis/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">urbandemographics.org/publicat</span><span class=\"invisible\">ion/2023_trpd_public_transport_emissions_gtfs2emis/</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      },
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13589v1",
    "title": "BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases",
    "latest": "2023-05-24T16:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424637611065719",
      "content": "<p>\ud83d\udcdd BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases \ud83d\udcda</p><p>\"BiasX is a framework that enables content moderators to provide free-text explanations in support of toxicity decisions, alongside their binary (toxic or not) decisions.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13589v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13589v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41893-023-01132-6",
    "title": "Quantifying the human cost of global warming - Nature Sustainability",
    "latest": "2023-05-24T16:12:04+00:00",
    "last_post": {
      "url": "https://botsin.space/@kottke/110424537435653767",
      "content": "<p>Quantifying the human cost of global warming: because of climate change, over 600 million people currently live outside the \"human climate niche\". That could rise to more than 1/3 of the total global population by the end of the century. <a href=\"https://www.nature.com/articles/s41893-023-01132-6\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41893-023</span><span class=\"invisible\">-01132-6</span></a></p>"
    },
    "people": [
      {
        "url": "https://occult.institute/@maya",
        "display_name": "maya \ud83e\uded0"
      },
      {
        "url": "https://botsin.space/@kottke",
        "display_name": "kottke.org"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12972",
    "title": "VanillaNet: the Power of Minimalism in Deep Learning",
    "latest": "2023-05-24T16:00:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110424490192297674",
      "content": "<p>VanillaNet: The Power of Minimalism in Deep Learning - <a href=\"https://arxiv.org/abs/2305.12972\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12972</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686",
    "title": "Cross-validation: what does it estimate and how well does it do it?",
    "latest": "2023-05-24T15:59:23+00:00",
    "last_post": {
      "url": "https://sciences.social/@klauspforr/110424448086940405",
      "content": "<p>wow <a href=\"https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/full/10.10</span><span class=\"invisible\">80/01621459.2023.2197686</span></a></p>"
    },
    "people": [
      {
        "url": "https://social.cwts.nl/@vtraag",
        "display_name": "Vincent Traag"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13585v1",
    "title": "Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs",
    "latest": "2023-05-24T15:57:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424480274780649",
      "content": "<p>\ud83d\udcdd Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs \ud83d\udcda</p><p>\"Proposes a structure-modeled textual encoding framework for inductive logical reasoning over KGs to find answers for complex logical queries with the structure-modeled instructions and the pre-trained encoder.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13585v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13585v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13583v1",
    "title": "Cross-Attention is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition",
    "latest": "2023-05-24T15:47:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424441061550977",
      "content": "<p>\ud83d\udcdd Cross-Attention Is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition \ud83d\udcda</p><p>\"Proposes a lightweight model via Hierarchical Crossmodal Transformer with Modality Gating (HCT-MG), which determines a primary modality according to its contribution to the target task and then hierarchically incorporates auxiliary modalities to alleviate inter-modal incongruity and reduce information redundancy.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13583v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13583v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13582v1",
    "title": "Better Low-Resource Entity Recognition Through Translation and Annotation Fusion",
    "latest": "2023-05-24T15:37:32+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424401688948635",
      "content": "<p>\ud83d\udcdd Better Low-Resource Entity Recognition Through Translation and Annotation Fusion \ud83d\udcda</p><p>\"TransFusion is a framework which translates text into a high-resource language for annotation using fully supervised models, and then fuses those annotations back into the low-resource language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13582v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13582v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.17709",
    "title": "Teru Teru B\u014dzu: Defensive Raincloud Plots",
    "latest": "2023-05-24T15:19:24+00:00",
    "last_post": {
      "url": "https://vis.social/@EuroVis/110423556294707191",
      "content": "<p>\ud83d\udcdc&nbsp; Raincloud plots show distributions in multiple ways. But are they more than the sum of their parts?<br>\u270d\ufe0f <span class=\"h-card\"><a href=\"https://jorts.horse/@Birdbassador\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>Birdbassador</span></a></span><br>\ud83d\udc49 <a href=\"https://arxiv.org/abs/2303.17709\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.17709</span><span class=\"invisible\"></span></a><br><a href=\"https://vis.social/tags/Fullpaper\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Fullpaper</span></a> <a href=\"https://vis.social/tags/EuroVis\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>EuroVis</span></a> <a href=\"https://vis.social/tags/Eurovis2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Eurovis2023</span></a></p>"
    },
    "people": [
      {
        "url": "https://vis.social/@lane",
        "display_name": "Lane Harrison"
      },
      {
        "url": "https://vis.social/@mcnutt",
        "display_name": "Andrew McNutt"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13571v1",
    "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
    "latest": "2023-05-24T15:17:33+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110424323106007438",
      "content": "<p>\ud83d\udcdd Latent Positional Information Is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings \ud83d\udcda</p><p>\"Transformer language models encode strong positional information without positional embeddings through shrinkage of the self-attention variance in each layer, which is not affected by gradient updates during pretraining.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13571v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13571v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.14334",
    "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence",
    "latest": "2023-05-24T09:58:54+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110423070110153900",
      "content": "<p>Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence</p><p>Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski  Trevor Darrell</p><p>tl;dr: diffusion features are good descriptors for semantic corrs, if aggregated among timesteps.</p><p><a href=\"https://arxiv.org/abs/2305.14334\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.14334</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41467-023-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D",
    "title": "Spatially-optimized urban greening for reduction of population exposure to land surface temperature extremes | Nature Communications",
    "latest": "2023-05-24T09:55:34+00:00",
    "last_post": {
      "url": "https://datasci.social/@mszll/110423057006850663",
      "content": "<p>Spatially-optimized urban greening for reduction of population exposure to land surface temperature extremes</p><p><a href=\"https://www.nature.com/articles/s41467-023-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41467-023</span><span class=\"invisible\">-38596-1.epdf?sharing_token=yxHupBGxVhtuER_RNxBs59RgN0jAjWel9jnR3ZoTv0O1pGmA2LzoIMu6bBKTcd6cHVkaWZup4Y5ApWTYgJd3EHP68G-7l22TG4RQck1D5hnMk41o8GqHiueQ7G_adf0qXNNxAINW70-wf7EHkjycl6-p_kCiitpPfXajER-ejPc%3D</span></a></p>"
    },
    "people": [
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13705",
    "title": "DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models",
    "latest": "2023-05-24T09:52:59+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110423046869119371",
      "content": "<p>DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models</p><p>Lijun Li, Li'an Zhuo, Bang Zhang, Liefeng Bo, Chen Chen</p><p>tl;dr: diffusion models can do mesh reconstruction.<br><a href=\"https://arxiv.org/abs/2305.13705\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13705</span><span class=\"invisible\"></span></a></p><p><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13455v1",
    "title": "https://arxiv.org/abs/2305.13455v1",
    "latest": "2023-05-24T09:37:36+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110422986361169984",
      "content": "<p>\ud83d\udcdd Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents \ud83d\udcda</p><p>\"Presents a general framework for implementing and evaluating games with LLMs, including five concrete games as an exemplary set of settings to probe a range of capabilities (e.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clp-research/clembench\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/clp-research/clembe</span><span class=\"invisible\">nch</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.13455v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13455v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  }
]