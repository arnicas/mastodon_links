[
  {
    "link": "https://arxiv.org/abs/2305.12074v1",
    "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
    "latest": "2023-05-23T10:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417412880049300",
      "content": "<p>\ud83d\udcdd DisCo: Distilled Student Models Co-Training for Semi-Supervised Text Mining \ud83d\udcda</p><p>\"A novel co-training technique to optimize multiple small student models generated from a large PLM using knowledge distillation under diversified views: model and data views produced by different distillation strategies and input augmentations, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12074v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12074v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12147",
    "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4",
    "latest": "2023-05-23T10:00:05+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110417412101749441",
      "content": "<p>LogiCoT: Logical Chain-of-Thought instruction-tuning data collection with GPT-4. ~ Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang. <a href=\"https://arxiv.org/abs/2305.12147\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12147</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/GPT4\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT4</span></a> <a href=\"https://mathstodon.xyz/tags/Logic\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Logic</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11252v1",
    "title": "Brain-inspired learning in artificial neural networks: a review",
    "latest": "2023-05-23T09:47:20+00:00",
    "last_post": {
      "url": "https://mastodon.social/@achterbrain/110417362350083227",
      "content": "<p>New <a href=\"https://mastodon.social/tags/review\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>review</span></a> led by Samuel Schmidgall:<br>'Brain-inspired learning in artificial neural networks'<br><a href=\"https://arxiv.org/abs/2305.11252v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11252v1</span><span class=\"invisible\"></span></a></p><p>This was a great <a href=\"https://mastodon.social/tags/collaboration\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>collaboration</span></a> supported by OpenBioML's new <a href=\"https://mastodon.social/tags/NeuroAI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuroAI</span></a> initiative! Looking forward to many more projects coming out of this <a href=\"https://mastodon.social/tags/open\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>open</span></a> collaborative <a href=\"https://mastodon.social/tags/research\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>research</span></a> community!</p><p><a href=\"https://mastodon.social/tags/neuroscience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>neuroscience</span></a> <a href=\"https://mastodon.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@achterbrain",
        "display_name": "Jascha Achterberg"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12057v1",
    "title": "https://arxiv.org/abs/2305.12057v1",
    "latest": "2023-05-23T09:40:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417334069138419",
      "content": "<p>\ud83d\udcdd Accurate Knowledge Distillation with N-Best Reranking \ud83d\udcda</p><p>\"Leverages a diverse set of models, including publicly available large pretrained models, to provide more accurate pseudo-labels for training student models via n-best reranking.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/fairseq/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/fa</span><span class=\"invisible\">irseq/</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12057v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12057v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.12073v1.pdf",
    "title": "https://arxiv.org/pdf/2305.12073v1.pdf",
    "latest": "2023-05-23T09:14:35+00:00",
    "last_post": {
      "url": "https://mastodon.social/@tiago_ribeiro/110417142228641639",
      "content": "<p>\"GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance\"</p><p>\"Our findings reinforce the exceptional performance of the GELU activation function, which attains the highest test accuracy and lowest test loss among the activation functions investigated. Other activation functions, such as Hardswish and ReLU6, exhibit commendable performance as well...\"</p><p><a href=\"https://mastodon.social/tags/GELU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GELU</span></a> <a href=\"https://mastodon.social/tags/ReLU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReLU</span></a> <a href=\"https://mastodon.social/tags/HardShrink\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>HardShrink</span></a> <a href=\"https://mastodon.social/tags/leakyReLU\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>leakyReLU</span></a> <a href=\"https://mastodon.social/tags/ReLU6\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ReLU6</span></a></p><p>\ud83d\udd17<a href=\"https://arxiv.org/pdf/2305.12073v1.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.12073v1.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12029v1",
    "title": "https://arxiv.org/abs/2305.12029v1",
    "latest": "2023-05-23T09:00:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417177104587367",
      "content": "<p>\ud83d\udcdd MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Introduces two models based on BERT and RoBERTa as baselines on this dataset, which can be used as a benchmark for future research to tackle this task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/huashen218/MultiTurnCleanup.git\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/huashen218/MultiTur</span><span class=\"invisible\">nCleanup.git</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12029v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12029v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12027v1",
    "title": "Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings",
    "latest": "2023-05-23T08:30:12+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110417059049321121",
      "content": "<p>\ud83d\udcdd Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings \ud83d\udcda\ud83d\udc7e</p><p>\"DUCK infuses prior knowledge on entity types in entity representations by learning to place entities of similar type close to each other in the embedding space, using boxes and hyperspheres.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12027v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12027v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13047",
    "title": "Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media",
    "latest": "2023-05-23T08:22:59+00:00",
    "last_post": {
      "url": "https://mastodon.social/@IndrekIbrus/110417030386609914",
      "content": "<p>The applied research project that we carried out with Ekspress Grupp has became an article. We studied how to use machine learning to study political positioning of media outlets, especially in small languages. Our case was articles on immigration - we studied polarisation around this topic among newspapers. The approach could be adopted in all countries. Preprint linked. The main author is Mark Mets, PhD student. He was helped by Andres Karjus, me and Maximilian Schich. <a href=\"https://arxiv.org/abs/2305.13047\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13047</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12018v1",
    "title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases",
    "latest": "2023-05-23T08:00:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416940920589699",
      "content": "<p>\ud83d\udcdd BOLT: Fast Energy-Based Controlled Text Generation with Tunable Biases \ud83d\udcda</p><p>\"Proposes BOLT which relies on tunable biases to adjust output logits directly to achieve fast convergence in controlled generation while maintaining the generator's autoregressive nature to assert a strong control on token-wise conditional dependencies and fluent output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12018v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12018v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12001v1",
    "title": "https://arxiv.org/abs/2305.12001v1",
    "latest": "2023-05-23T07:40:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416862157122454",
      "content": "<p>\ud83d\udcdd OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models \ud83d\udcda</p><p>\"Entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/metaseq\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/me</span><span class=\"invisible\">taseq</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12001v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12001v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12196",
    "title": "Experimental results from applying GPT-4 to an unpublished formal language",
    "latest": "2023-05-23T07:27:50+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@Jose_A_Alonso/110416776676268021",
      "content": "<p>Experimental results from applying GPT-4 to an unpublished formal language. ~ Gregor vom Scheidt (@GregorVScheidt). <a href=\"https://arxiv.org/abs/2305.12196\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12196</span><span class=\"invisible\"></span></a> <a href=\"https://mathstodon.xyz/tags/ChatGPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ChatGPT</span></a> <a href=\"https://mathstodon.xyz/tags/GPT4\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GPT4</span></a> <a href=\"https://mathstodon.xyz/tags/FunctionalProgramming\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>FunctionalProgramming</span></a> <a href=\"https://mathstodon.xyz/tags/Logic\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Logic</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11326",
    "title": "Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data",
    "latest": "2023-05-23T07:27:07+00:00",
    "last_post": {
      "url": "https://fediscience.org/@JordiCabot/110416810949851518",
      "content": "<p>Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data</p><p>&lt;- From a CSV to a fully functional chatbot </p><p>Paper: <a href=\"https://arxiv.org/abs/2305.11326\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11326</span><span class=\"invisible\"></span></a><br>Summary: <a href=\"https://livablesoftware.com/chatbots-open-data-project/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">livablesoftware.com/chatbots-o</span><span class=\"invisible\">pen-data-project/</span></a></p><p><a href=\"https://fediscience.org/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://fediscience.org/tags/csv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>csv</span></a> <a href=\"https://fediscience.org/tags/OpenData\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenData</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@JordiCabot",
        "display_name": "Jordi Cabot"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12000v1",
    "title": "Deep Learning Approaches to Lexical Simplification: A Survey",
    "latest": "2023-05-23T07:10:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416744250292836",
      "content": "<p>\ud83d\udcdd Deep Learning Approaches to Lexical Simplification: A Survey \ud83d\udcda</p><p>\"LS is the lexical component of Text Simplification (TS) with the aim of making texts more accessible to various target populations such as children, people with language and learning disorders, foreigners or people with low literacy.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.12000v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12000v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2109.06096",
    "title": "https://arxiv.org/abs/2109.06096",
    "latest": "2023-05-23T06:38:47+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416620928970772",
      "content": "<p>As always a small model behaves just like an undertrained model<br><a href=\"https://twitter.com/LChoshen/status/1506245430912430091\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/LChoshen/status/15</span><span class=\"invisible\">06245430912430091</span></a><br><a href=\"https://arxiv.org/abs/2109.06096\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2109.06096</span><span class=\"invisible\"></span></a><br>So you can also pick an undertrained model, which would be better than a trained model in detecting who generated the text.</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2301.11305",
    "title": "https://arxiv.org/abs/2301.11305",
    "latest": "2023-05-23T06:33:49+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416601360081415",
      "content": "<p>First the problem, given a text you want to know whether a human wrote it. You've been in NLP lately I am sure a teacher, sister, nephew etc. called and told you they suspect someone handed them a GPT text.<br>Problem: how can you tell<br>The approach<br>Randomly replace words<br>Then see how much it changed the sentence probability\\likelihood</p><p>presented by <br><a href=\"https://arxiv.org/abs/2301.11305\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2301.11305</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09859",
    "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
    "latest": "2023-05-23T06:33:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416598590927915",
      "content": "<p>Opposite scaling law: detection of machine-generated text is done better by smaller models</p><p>Everyone (outside <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a>...) is afraid GPT would cheat for them, which pushes for detection methods</p><p><a href=\"https://arxiv.org/abs/2305.09859\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09859</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <a href=\"https://sigmoid.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11993v1",
    "title": "https://arxiv.org/abs/2305.11993v1",
    "latest": "2023-05-23T06:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416586994729304",
      "content": "<p>\ud83d\udcdd Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis \ud83d\udcda</p><p>\"A specialised Flan-T5 language model is trained to take usage examples and a usage cluster as input and output a natural language definition of that usage cluster as its output.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ltgoslo/definition_modeling\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/ltgoslo/definition_</span><span class=\"invisible\">modeling</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11993v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11993v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11979v1",
    "title": "https://arxiv.org/abs/2305.11979v1",
    "latest": "2023-05-23T05:30:12+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416351236599275",
      "content": "<p>\ud83d\udcdd A Weak Supervision Approach for Few-Shot Aspect Based Sentiment \ud83d\udcda</p><p>\"Uses weak supervision on unlabeled data using the output of a simple baseline, and then fine-tune a pre-trained sequence-to-sequence model on the generated dataset.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/robertvacareanu/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/robertvacareanu/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11979v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11979v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.10559",
    "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
    "latest": "2023-05-23T05:19:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416307592130037",
      "content": "<p>We also have seen claims ICL is quite similar to taking a gradient step<br>arxiv.org/abs/2211.15661<br><a href=\"https://arxiv.org/abs/2212.10559\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2212.10559</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2202.12837",
    "title": "http://arxiv.org/abs/2202.12837",
    "latest": "2023-05-23T05:18:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416304404784912",
      "content": "<p>So we have seen papers showing that models gain a lot from seeing examples (ICL) with random labels<br><a href=\"http://arxiv.org/abs/2202.12837\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2202.12837</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09731",
    "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
    "latest": "2023-05-23T05:17:36+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@LChoshen/110416301704135043",
      "content": "<p>In-Context-Learning == gradient descent or disregards labels completely?!<br>Why not both?</p><p>Models recognize the task but also learn it<br>&amp; The benefits of actual learning grow with # examples and model size</p><p> <br><a href=\"https://arxiv.org/abs/2305.09731\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09731</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@LChoshen",
        "display_name": "Leshem Choshen"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11952v1",
    "title": "Self-QA: Unsupervised Knowledge Guided Language Model Alignment",
    "latest": "2023-05-23T05:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416233204460998",
      "content": "<p>\ud83d\udcdd Self-Qa: Unsupervised Knowledge Guided Language Model Alignment \ud83d\udcda</p><p>\"Self-QA replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge, enabling the model to generate a larger quantity of correct and domain-specific instruction data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11952v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11952v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11938v1",
    "title": "https://arxiv.org/abs/2305.11938v1",
    "latest": "2023-05-23T04:40:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416154477268328",
      "content": "<p>\ud83d\udcdd XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages \ud83d\udcda</p><p>\"XTREME-UP is a benchmark defined by three criteria: its focus on the scarce-data scenario rather than zero-shot (rather than few-shot); its focus on user-centric tasks rather than linguistic probing tasks; and its focus on under-represented languages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/google-research/xtreme-up\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/google-research/xtr</span><span class=\"invisible\">eme-up</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11938v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11938v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.02819",
    "title": "GPT detectors are biased against non-native English writers",
    "latest": "2023-05-23T04:32:53+00:00",
    "last_post": {
      "url": "https://river.group.lt/@rgb/110408082519926313",
      "content": "<p><a href=\"https://arxiv.org/abs/2304.02819\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.02819</span><span class=\"invisible\"></span></a><br>GPT detectors are biased against non-native English writers</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11916v1",
    "title": "F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks",
    "latest": "2023-05-23T04:20:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110416075706791889",
      "content": "<p>\ud83d\udcdd F-Pabee: Flexible-Patience-Based Early Exiting for Single-Label and Multi-Label Text Classification Tasks \ud83d\udcda</p><p>\"A Flexible-Patience-Based Early Exiting Method (F-PABEE) has been proposed to alleviate the problems mentioned above for single-label classification (SLC) and multi-label classification (MLC) tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11916v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11916v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11862v1",
    "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models",
    "latest": "2023-05-23T03:42:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415926410383122",
      "content": "<p>\ud83d\udcdd Reducing Sequence Length by Predicting Edit Operations with Large Language Models \ud83d\udcda</p><p>\"A local sequence transduction task is represented as a sequence of edit operations on the input text, which is learned by instruction tuning for large language models (LLMs).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11862v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11862v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11826v1",
    "title": "STOAT: Structured Data to Analytical Text With Controls",
    "latest": "2023-05-23T03:22:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415847795639417",
      "content": "<p>\ud83d\udcdd STOAT: Structured Data to Analytical Text with Controls \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a reasoning category aware table-to-text generation model with vector quantization to infuse the reasoning category in the output descriptions and also to control the reasoning category generation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11826v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11826v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.12234",
    "title": "The Damage to Lunar Orbiting Spacecraft Caused by the Ejecta of Lunar Landers",
    "latest": "2023-05-23T02:41:41+00:00",
    "last_post": {
      "url": "https://bayes.club/@akhilrao/110415688626841875",
      "content": "<p>Dope new space paper: <a href=\"https://arxiv.org/abs/2305.12234\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.12234</span><span class=\"invisible\"></span></a></p><p>Seems like lunar orbital externalities are worse than lunar surface externalities; unlike Earth orbit, a lunar launch tax might be better than a lunar satellite tax</p>"
    },
    "people": [
      {
        "url": "https://bayes.club/@akhilrao",
        "display_name": "edgeworth boxlord"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13048",
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "latest": "2023-05-23T02:12:40+00:00",
    "last_post": {
      "url": "https://genomic.social/@PhilippBayer/110415566437178829",
      "content": "<p>'RWKV: Reinventing RNNs for the Transformer Era'<br><a href=\"https://arxiv.org/abs/2305.13048\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13048</span><span class=\"invisible\"></span></a></p><p>'Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters.'</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@davidmortensen",
        "display_name": "David Mortensen"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11626v1",
    "title": "CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search",
    "latest": "2023-05-23T02:12:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415572481058584",
      "content": "<p>\ud83d\udcdd CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search \ud83d\udcda</p><p>\"A combination of GraphCodeBERT pre-training and cross-consistency training of a language model across different programming languages using XCD dataset as supervision signal.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11626v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11626v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11625v1",
    "title": "Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets",
    "latest": "2023-05-23T02:02:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415533142570361",
      "content": "<p>\ud83d\udcdd Searching by Code: A New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets \ud83d\udcda</p><p>\"SnippeR uses a single encoder for both the code snippet and the natural language query, and learns to rank the candidate answer by the cosine similarity of the two.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11625v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11625v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11744v1",
    "title": "Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval",
    "latest": "2023-05-23T01:52:58+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_ir/110415139983596635",
      "content": "<p>\ud83d\udcdd Inference-Time Re-Ranker Relevance Feedback for Neural Information Retrieval \ud83d\udcbf\ud83d\udcda</p><p>\"We update the retriever's query vector using a lightweight inference-time distillation of the re-ranker's prediction for that instance and perform a second retrieval with the updated query vector.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/IR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>IR</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11744v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11744v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://recsys.social/@karlhigley",
        "display_name": "Karl Higley"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11596v1",
    "title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation",
    "latest": "2023-05-23T01:32:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415415119788630",
      "content": "<p>\ud83d\udcdd Mitigating Backdoor Poisoning Attacks Through the Lens of Spurious Correlation \ud83d\udcda</p><p>\"Works by filtering out instances with potentially problematic correlations between features and labels, and using a subset of the rest to retrain the models and remove backdoor behaviours.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CR</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11596v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11596v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13252",
    "title": "\"According to ...\" Prompting Language Models Improves Quoting from Pre-Training Data",
    "latest": "2023-05-23T01:17:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415356090447013",
      "content": "<p>\u201cAccording to . . . \u201d Prompting Language Models Improves Quoting from Pre-Training Data</p><p><a href=\"https://arxiv.org/abs/2305.13252\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13252</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11553v1",
    "title": "https://arxiv.org/abs/2305.11553v1",
    "latest": "2023-05-23T01:12:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415336656025295",
      "content": "<p>\ud83d\udcdd Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information \ud83d\udcda</p><p>\"Considers each abstract as a recurrent cycle of sentences and place segmentation boundaries by greedily optimizing the NMI score between premises and conclusions in each cycle of sentences.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mediacloud/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mediacloud/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11553v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11553v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13304",
    "title": "https://arxiv.org/abs/2305.13304",
    "latest": "2023-05-23T01:12:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415336394152761",
      "content": "<p>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text</p><p>Generates a paragraph at each timestep and updates its language-based long-short term memory stored on the hard drive and the prompt, respectively. </p><p>repo: <a href=\"https://github.com/aiwaves-cn/RecurrentGPT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/aiwaves-cn/Recurren</span><span class=\"invisible\">tGPT</span></a> <br>abs: <a href=\"https://arxiv.org/abs/2305.13304\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13304</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13281",
    "title": "LM vs LM: Detecting Factual Errors via Cross Examination",
    "latest": "2023-05-23T01:08:06+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415320584865733",
      "content": "<p>LM vs LM: Detecting Factual Errors via Cross Examination</p><p>Prompting to make LM talk to itself like cross examination improves its ability to detect factual errors, sometimes by a large margin.</p><p><a href=\"https://arxiv.org/abs/2305.13281\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13281</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13292",
    "title": "VideoLLM: Modeling Video Sequence with Large Language Models",
    "latest": "2023-05-23T01:03:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415301110279075",
      "content": "<p>VideoLLM: Modeling Video Sequence with Large Language Models</p><p>Leverages the sequence reasoning capabilities of pre-trained LLMs from NLP for video sequence understanding.</p><p><a href=\"https://arxiv.org/abs/2305.13292\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13292</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.13301",
    "title": "Training Diffusion Models with Reinforcement Learning",
    "latest": "2023-05-23T01:00:10+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110415289384934263",
      "content": "<p>Training Diffusion Models with Reinforcement Learning</p><p>Presents an RL-based framework for training denoising diffusion models to directly optimize a variety of reward functions</p><p><a href=\"https://arxiv.org/abs/2305.13301\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.13301</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2303.12712.pdf",
    "title": "https://arxiv.org/pdf/2303.12712.pdf",
    "latest": "2023-05-23T00:42:46+00:00",
    "last_post": {
      "url": "https://techhub.social/@freemanwd/110415146468249466",
      "content": "<p>Microsoft researchers wonder if GPT4 is a form of AGI based on several experiments <br><a href=\"https://arxiv.org/pdf/2303.12712.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2303.12712.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@andrey",
        "display_name": "Andrey Kurenkov"
      },
      {
        "url": "https://fosstodon.org/@amueller",
        "display_name": "Andreas Mueller"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://genart.social/@tca",
        "display_name": "tiago"
      },
      {
        "url": "https://mastodon.social/@bruces",
        "display_name": "Bruce Sterling @bruces"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11449v1",
    "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast",
    "latest": "2023-05-23T00:32:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415179239334587",
      "content": "<p>\ud83d\udcdd Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-Tuning Slow and Fast \ud83d\udcda</p><p>\"Proposes a method named Fine-tuning slow and fast with four training policies to improve the performance gap for multilingual tasks, by reducing forgetting and improving the fine-tuning process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11449v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11449v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11251v1",
    "title": "Computational thematics: Comparing algorithms for clustering the genres of literary fiction",
    "latest": "2023-05-23T00:12:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415100611809059",
      "content": "<p>\ud83d\udcdd Computational Thematics: Comparing Algorithms for Clustering the Genres of Literary Fiction \ud83d\udcda</p><p>\"S are applied to a corpus of books belonging to four pre-tagged genres of fiction (Fantasy, Horror, Science Fiction, and Romance), and are then validated against the \"ground truth\" genre labels.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11251v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11251v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11243v1",
    "title": "Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses",
    "latest": "2023-05-23T00:02:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110415061229184234",
      "content": "<p>\ud83d\udcdd Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses \ud83d\udcda\ud83d\udc7e</p><p>\"LaMDA is a large language model (LM), a type of AModels trained to predict the next word given a sequence of words, that was trained on a large number of webpages and books.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11243v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11243v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11231v1",
    "title": "Recent Trends in Unsupervised Summarization",
    "latest": "2023-05-22T23:22:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414903863283183",
      "content": "<p>\ud83d\udcdd Recent Trends in Unsupervised Summarization \ud83d\udcda</p><p>\"Unsupervised approaches learn to summarize from the input data and do not need labeled datasets for training, unlike supervised approaches that require labeled datasets for training the models to summarize the content.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11231v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11231v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2302.12173",
    "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
    "latest": "2023-05-22T23:05:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110414836730125554",
      "content": "<p>Compromising LLM-Integrated Applications with Indirect Prompt Injection - <a href=\"https://arxiv.org/abs/2302.12173\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2302.12173</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@raphaelmilliere",
        "display_name": "Rapha\u00ebl Milli\u00e8re"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206v1",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T23:02:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414825637860258",
      "content": "<p>\ud83d\udcdd LIMA: Less Is More for Alignment \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"LIMA is a 65B parameter language model trained for 150B tokens, using a supervised loss, from 1,000 carefully curated prompts and their responses.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11206v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11186v1",
    "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
    "latest": "2023-05-22T22:52:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414785973568218",
      "content": "<p>\ud83d\udcdd Compress, Then Prompt: Improving Accuracy-Efficiency Trade-Off of LLM Inference with Transferable Prompt \ud83d\udcda\ud83e\udde0</p><p>\"A prompt learning paradigm that cultivates an additive prompt over a compressed LLM to bolster their accuracy is presented and empirically tested in this work, shedding light on new possibilities for enhancing the balance between accuracy and efficiency in LLM inference.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11186v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11186v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11719v1",
    "title": "https://arxiv.org/abs/2305.11719v1",
    "latest": "2023-05-22T21:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414463541864356",
      "content": "<p>\ud83d\udcdd Information Screening Whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling \ud83d\udd2d\ud83d\udcda</p><p>\"Represents the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ChocoWu/MRE-ISE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/ChocoWu/MRE-ISE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11719v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11719v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/74djc/",
    "title": "http://osf.io/74djc/",
    "latest": "2023-05-22T21:04:05+00:00",
    "last_post": {
      "url": "https://botsin.space/@EdArXivBot/110395685386849135",
      "content": "<p>Unlocking Financial Success: Empowering Higher Ed Students and Developing Financial Literacy Interventions at Scale <a href=\"http://osf.io/74djc/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/74djc/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@dougholton",
        "display_name": "Doug Holton"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11845v1",
    "title": "https://arxiv.org/abs/2305.11845v1",
    "latest": "2023-05-22T20:18:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414180509844595",
      "content": "<p>\ud83d\udcdd RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"RxnScribe is an end-to-end model that takes as input a chemical reaction diagram and outputs a structured representation in a machine-readable format.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/thomas0809/RxnScribe\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/thomas0809/RxnScrib</span><span class=\"invisible\">e</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11845v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11845v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11806v1",
    "title": "https://arxiv.org/abs/2305.11806v1",
    "latest": "2023-05-22T19:54:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110414086009856181",
      "content": "<p>\ud83d\udcdd The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics \ud83d\udcda</p><p>\"Develops and compare several explanation methods and demonstrate their effectiveness for interpreting state-of-the-art neural metrics based on BERT and ROBERTA fine-tuned on sentence-level human judgments from the WMT Metrics Shared Task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Unbabel/COMET\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Unbabel/COMET</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11806v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11806v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11790v1",
    "title": "Prompting with Pseudo-Code Instructions",
    "latest": "2023-05-22T19:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413944317609304",
      "content": "<p>\ud83d\udcdd Prompting with Pseudo-Code Instructions \ud83d\udcda</p><p>\"A pre-trained language model is fine-tuned using a prompt in the form of pseudocode instructions for a task in addition to the natural language instruction for the task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11790v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11790v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11789v1",
    "title": "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach",
    "latest": "2023-05-22T19:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413897105704111",
      "content": "<p>\ud83d\udcdd Solving NLP Problems Through Human-System Collaboration: A Discussion-Based Approach \ud83d\udcda</p><p>\"Creates a dataset with a human-in-the-loop setup where an annotator can discuss a natural language inference task with a chatbot and receive feedback.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11789v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11789v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41593-023-01333-4",
    "title": "Addressing the ethical and societal challenges posed by genome-wide association studies of behavioral and brain-related traits - Nature Neuroscience",
    "latest": "2023-05-22T18:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413833974408684",
      "content": "<p>Ethical, societal implications of genome-wide behavioral, brain-related traits - <a href=\"https://www.nature.com/articles/s41593-023-01333-4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41593-023</span><span class=\"invisible\">-01333-4</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778v1",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T18:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413802737748919",
      "content": "<p>\ud83d\udcdd Cross-Lingual Supervision Improves Large Language Models Pre-Training \ud83d\udcda\ud83e\udde0</p><p>\"A pre-training objective mixing a self-supervised language modeling and a supervised machine translation objectives, including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11778v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11761v1",
    "title": "ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation",
    "latest": "2023-05-22T18:30:09+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413755777159926",
      "content": "<p>\ud83d\udcdd ReSeTOX: Re-Learning Attention Weights for Toxicity Mitigation in Machine Translation \ud83d\udcda</p><p>\"Works by dynamically adjusting the key-value self-attention weights and re-evaluates the beam search hypotheses in case of identified added toxicity during the inference process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11761v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11761v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/u26ze/",
    "title": "http://osf.io/u26ze/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645261557408",
      "content": "<p>Gender and retention patterns among U.S. faculty <a href=\"http://osf.io/u26ze/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u26ze/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/u5kd2/",
    "title": "http://osf.io/u5kd2/",
    "latest": "2023-05-22T18:02:02+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645213145496",
      "content": "<p>Power users: Canadian sex workers' use of technology post COVID <a href=\"http://osf.io/u5kd2/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/u5kd2/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/7s9g4/",
    "title": "http://osf.io/7s9g4/",
    "latest": "2023-05-22T18:02:01+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413645169835083",
      "content": "<p>Intelligent transport design with a dual focus <a href=\"http://osf.io/7s9g4/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/7s9g4/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/pqzux/",
    "title": "http://osf.io/pqzux/",
    "latest": "2023-05-22T17:56:07+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110413621983917418",
      "content": "<p>Lithic raw materials provenance in the Caribbean islands: flint, jasper, obsidian and so on. <a href=\"http://osf.io/pqzux/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/pqzux/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11062",
    "title": "Scaling Transformer to 1M tokens and beyond with RMT",
    "latest": "2023-05-22T17:53:06+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@mikeiavelli/110254216906691830",
      "content": "<p>Recurrent Memory Transformer (RMT) retains information across up to 2 million tokens (!) \u2b50, significantly exceeding the largest input size reported for transformer models (64K tokens) and GPT-4's 32K tokens.</p><p>Paper: <a href=\"https://arxiv.org/abs/2304.11062\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11062</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/booydar/t5-experiments/tree/scaling-report\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/booydar/t5-experime</span><span class=\"invisible\">nts/tree/scaling-report</span></a></p><p><a href=\"https://mathstodon.xyz/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> <a href=\"https://mathstodon.xyz/tags/ML\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ML</span></a> <br><a href=\"https://mathstodon.xyz/tags/NeuralNetworks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NeuralNetworks</span></a> <a href=\"https://mathstodon.xyz/tags/NN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NN</span></a><br><a href=\"https://mathstodon.xyz/tags/DeepLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DeepLearning</span></a><br><a href=\"https://mathstodon.xyz/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mathstodon.xyz/tags/Transformers\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Transformers</span></a> <a href=\"https://mathstodon.xyz/tags/RNN\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>RNN</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://mastodon.online/@vladiliescu",
        "display_name": "Vlad Iliescu \ud83d\udc2c"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01702-w",
    "title": "When will global warming actually hit the landmark 1.5 \u00baC limit?",
    "latest": "2023-05-22T17:50:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110413598086407934",
      "content": "<p>When will global warming hit the landmark 1.5 \u00baC limit? - <a href=\"https://www.nature.com/articles/d41586-023-01702-w\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01702-w</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11554",
    "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
    "latest": "2023-05-22T17:23:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110413493153748217",
      "content": "<p>RT @Ber18791531@twitter.com</p><p>\ud83e\udd14ChatGPT plug-in is impressive, but is \"learning tools in the context\" the ultimate solution?<br>Check out our ToolkenGPT (<a href=\"https://arxiv.org/abs/2305.11554\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11554</span><span class=\"invisible\"></span></a>), which can handle massive tools and understand them better with new \"toolken\" embeddings.</p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/Ber18791531/status/1660520584382672897\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/Ber18791531/status</span><span class=\"invisible\">/1660520584382672897</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11595v1",
    "title": "Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate",
    "latest": "2023-05-22T17:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413425257031910",
      "content": "<p>\ud83d\udcdd Diving Into the Inter-Consistency of Large Language Models: An Insightful Analysis Through Debate \ud83d\udcda\ud83d\udc7e</p><p>\"Designs a formal debate framework with three-stage debate including the fair debate, mismatched debate, and roundtable debate stages for inter-consistency learning between LLMs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11595v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11595v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11579v1",
    "title": "https://arxiv.org/abs/2305.11579v1",
    "latest": "2023-05-22T16:42:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413331017830359",
      "content": "<p>\ud83d\udcdd Speech-Text Dialog Pre-Training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment \ud83d\udcda\ud83d\udc7e</p><p>\"First pre-trains on speech-text dialogs via response selection task and temporal position prediction task, and then fine-tunes the pre-trained model on downstream STD tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/AlibabaResearch/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/AlibabaResearch/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11579v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11579v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05335",
    "title": "https://doi.org/10.21105/joss.05335",
    "latest": "2023-05-22T16:30:36+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110413285723110427",
      "content": "<p>Just published in JOSS: 'chombo-discharge: An AMR code for gas discharge simulations in complex geometries' <a href=\"https://doi.org/10.21105/joss.05335\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05335</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1016/j.jtrangeo.2023.103590",
    "title": "doi.org/10.1016/j.jtrangeo.202...",
    "latest": "2023-05-22T16:19:41+00:00",
    "last_post": {
      "url": "https://datasci.social/@UrbanDemog/110413146017535927",
      "content": "<p>New paper \"Evaluating the impact of public transport travel time inaccuracy and variability on socio-spatial inequalities in accessibility\", out in the Journal of Transport Geography. A great study led by Kaue Braga.<br>Paper: <a href=\"https://doi.org/10.1016/j.jtrangeo.2023.103590\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1016/j.jtrangeo.202</span><span class=\"invisible\">3.103590</span></a><br>\ud83d\udd13ungated PDF: <a href=\"https://www.urbandemographics.org/publication/2023_jtg_public_transport_travel_time_inaccuracy_variability/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">urbandemographics.org/publicat</span><span class=\"invisible\">ion/2023_jtg_public_transport_travel_time_inaccuracy_variability/</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11564v1",
    "title": "https://arxiv.org/abs/2305.11564v1",
    "latest": "2023-05-22T16:06:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110413189526338211",
      "content": "<p>\ud83d\udcdd Decouple Knowledge From Paramters for Plug-and-Play Language Modeling \ud83d\udcda\ud83d\udc7e</p><p>\"The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Hannibal046/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Hannibal046/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11564v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11564v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11438v1",
    "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
    "latest": "2023-05-22T09:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411679334789432",
      "content": "<p>\ud83d\udcdd Phonetic and Prosody-Aware Self-Supervised Learning Approach for Non-Native Fluency Scoring \ud83d\udcda</p><p>\"A self-supervised approach for automatic fluency scoring is proposed that takes into account the phonetic and prosody awareness during both pre-training and fine-tuning stages to improve the performance of the model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11438v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11438v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11426v1",
    "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
    "latest": "2023-05-22T09:18:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411584964283083",
      "content": "<p>\ud83d\udcdd Post Hoc Explanations of Language Models Can Improve Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"AMPLIFY constructs natural language rationales from post hoc explanations to provide corrective signals to LLMs during in-context learning, resulting in prediction accuracy improvements of about 10-25% over a wide range of tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11426v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11426v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://osf.io/preprints/socarxiv/a7udw/",
    "title": "osf.io/preprints/socarxiv/a7ud...",
    "latest": "2023-05-22T09:02:43+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@rockberta/110411524554763096",
      "content": "<p>Hey there! New preprint where we use contextualized topic models, automated linguistic feature extraction and predictive modeling to look at how online communication by the European Commission has changed over time \ud83d\udcc3\ud83e\udd16\ud83d\udcc8</p><p>TL;DR: we show that EC has radically reshaped its projected online identity, moving towards less technocratic topics &amp; style, defining a more \"unique\" profile among comparable institutions &amp; aligning better with its audience.</p><p><a href=\"https://osf.io/preprints/socarxiv/a7udw/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">osf.io/preprints/socarxiv/a7ud</span><span class=\"invisible\">w/</span></a> <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/polsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>polsci</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@rockberta",
        "display_name": "Roberta Rocca"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.09848",
    "title": "Evaluating Verifiability in Generative Search Engines",
    "latest": "2023-05-22T08:52:24+00:00",
    "last_post": {
      "url": "https://qoto.org/@tedherman/110250616501896621",
      "content": "<p><span class=\"h-card\"><a href=\"https://fediscience.org/@muratdemirbas\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>muratdemirbas</span></a></span> This paper discusses the problem of verifying results of generative transformers - <a href=\"https://arxiv.org/abs/2304.09848\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.09848</span><span class=\"invisible\"></span></a> </p><p>My question: wasn't blockchain supposed to solve all the problems of verifiability?</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@Yarin",
        "display_name": "Yarin :verified: :verified:"
      },
      {
        "url": "https://sigmoid.social/@Riedl",
        "display_name": "Mark Riedl"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://aleph.land/@mtriclot",
        "display_name": "Mathieu Triclot"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11374v1",
    "title": "https://arxiv.org/abs/2305.11374v1",
    "latest": "2023-05-22T08:18:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411349164932293",
      "content": "<p>\ud83d\udcdd Characterizing Tradeoffs Between Teaching via Language and Demonstrations in Multi-Agent Systems \ud83d\udcda</p><p>\"Finds that teaching by demonstration is more effective in the simplest settings, but language is more effective as task difficulty increases, due to its ability to generalize more effectively to unseen scenarios.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/dharakyu/language-and-demos\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/dharakyu/language-a</span><span class=\"invisible\">nd-demos</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11374v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11374v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11364v1",
    "title": "https://arxiv.org/abs/2305.11364v1",
    "latest": "2023-05-22T08:06:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411301875994659",
      "content": "<p>\ud83d\udcdd Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"Presents LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets, which clusters text along syntactic, lexical, and semantic axes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/PAIR-code/interpretability\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/PAIR-code/interpret</span><span class=\"invisible\">ability</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11364v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11364v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11355v1",
    "title": "https://arxiv.org/abs/2305.11355v1",
    "latest": "2023-05-22T07:54:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411254744145126",
      "content": "<p>\ud83d\udcdd MD3: The Multi-Dialect Dataset of Dialogues \ud83d\udcda</p><p>\"A dataset of conversational speech from three major English-speaking countries: India, Nigeria, and the United States, with more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/nehasinha/Taboo/blob/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/nehasinha/Taboo/blo</span><span class=\"invisible\">b/</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11355v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11355v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09137",
    "title": "https://arxiv.org/abs/2305.09137",
    "latest": "2023-05-22T07:42:47+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381332490398862",
      "content": "<p>Pre-Training to Learn in Context</p><p>Proposes PICL, a framework to enhance the LMs' in-context learning ability by pre-training on \"intrinsic tasks\" in the general plain-text corpus using the simple LM objective.</p><p>repo: <a href=\"https://github.com/thu-coai/PICL\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/thu-coai/PICL</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.09137\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09137</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11334v1",
    "title": "Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs",
    "latest": "2023-05-22T07:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110411113215008062",
      "content": "<p>\ud83d\udcdd Writing Your Own Book: A Method for Going From Closed to Open Book QA to Improve Robustness and Performance of Smaller LLMs \ud83d\udcda\ud83d\udc7e</p><p>\"Introduces two novel methods, Tree-Search and Self-contextualizing Question-Answering, designed to enhance the performance of large language models (LLMs) in question-answering tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11334v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11334v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11326v1",
    "title": "Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data",
    "latest": "2023-05-22T06:42:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410971702391662",
      "content": "<p>\ud83d\udcdd Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data \ud83d\udcda</p><p>\"Proposes a method to generate a chatbot from tabular data sources using natural language processing (NLP) and machine learning (ML) techniques, and to deploy it to offer citizens interactive access to public data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11326v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11326v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11317v1",
    "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
    "latest": "2023-05-22T06:30:08+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410924603280206",
      "content": "<p>\ud83d\udcdd Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation \ud83d\udcda</p><p>\"A large-scale language model that was trained on an open-domain corpus was used to edit the input prompt to improve the quality and consistency of image outputs by the state-of-the-art Text-to-Image (T2I) models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11317v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11317v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11315v1",
    "title": "https://arxiv.org/abs/2305.11315v1",
    "latest": "2023-05-22T05:42:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410735679563419",
      "content": "<p>\ud83d\udcdd Improving Toponym Resolution with Better Candidate Generation, Transformer-Based Reranking, and Two-Stage Resolution \ud83d\udcda</p><p>\"GeoNorm first uses information retrieval techniques to generate a list of candidate entries from the geospatial ontology, then reranks the candidate entries using a transformer-based neural network that incorporates information from the ontology such as the entry's population.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/clulab/geonorm\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/clulab/geonorm</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11315v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11315v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125",
    "title": "Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity",
    "latest": "2023-05-22T05:27:17+00:00",
    "last_post": {
      "url": "https://mastodon.social/@devezer/110373663709370598",
      "content": "<p>And while epistemic pluralism may not be perfect or guarantee immediate progress, it should protect us from far worse excesses. Incidentally in our 2019 paper our computational model of  scientific process suggests that epistemic diversity is powerful in that it prevents worst-case scenarios and allows for scientific discovery while avoiding many traps. <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0216125</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@FroehlichMarcel",
        "display_name": "Marcel Fr\u00f6hlich"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11255v1",
    "title": "https://arxiv.org/abs/2305.11255v1",
    "latest": "2023-05-22T05:18:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110410641360816775",
      "content": "<p>\ud83d\udcdd Reasoning Implicit Sentiment with Chain-of-Thought Prompting \ud83d\udcda</p><p>\"A three-step prompting principle is designed for THOR to guide the model to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/scofield7419/THOR-ISA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/scofield7419/THOR-I</span><span class=\"invisible\">SA</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11255v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11255v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11778",
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
    "latest": "2023-05-22T04:26:07+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110410436966499091",
      "content": "<p>Cross-Lingual Supervision improves Large Language Models Pre-training</p><p>Pretraining LLMs on a mixture of a regular LM dataset + cross-lingual parallel data yields models with better in-context learning abilities. </p><p><a href=\"https://arxiv.org/abs/2305.11778\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11778</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09612",
    "title": "Large Language Models are Built-in Autoregressive Search Engines",
    "latest": "2023-05-22T03:42:39+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381297041837722",
      "content": "<p>Large Language Models are Built-in Autoregressive Search Engines</p><p>When providing a few Query-URL pairs as in-context demonstrations, LLMs can generate Web URLs where ~90% of the corresponding documents contain correct answers to open-domain questions.</p><p><a href=\"https://arxiv.org/abs/2305.09612\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09612</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2102.07350.pdf",
    "title": "https://arxiv.org/pdf/2102.07350.pdf",
    "latest": "2023-05-22T02:49:21+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110410056412505522",
      "content": "<p>also see \"Prompt Programming for Large Language Models:<br>Beyond the Few-Shot Paradigm\"</p><p> Informed by this more encompassing theory of<br>prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its<br>own natural language prompts for a range of tasks</p><p><a href=\"https://arxiv.org/pdf/2102.07350.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2102.07350.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://mastodon.radio/tags/prompts\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>prompts</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.11430.pdf",
    "title": "https://arxiv.org/pdf/2305.11430.pdf",
    "latest": "2023-05-22T02:22:01+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110409948972775616",
      "content": "<p>\"TELeR: A General Taxonomy of LLM Prompts for Benchmarking<br>Complex Tasks\"</p><p><a href=\"https://arxiv.org/pdf/2305.11430.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.11430.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.00118",
    "title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",
    "latest": "2023-05-22T01:44:35+00:00",
    "last_post": {
      "url": "https://paquita.masto.host/@no_tan_incendiario/110343931803514302",
      "content": "<p>Han descubierto que ChatGPT ha \"\"memorizado\"\" m\u00e1s de 100 libros. A ver cu\u00e1ndo las editoriales afilan sus cuchillos. <a href=\"https://arxiv.org/abs/2305.00118\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.00118</span><span class=\"invisible\"></span></a> La lista de libros: <a href=\"https://docs.google.com/spreadsheets/d/1jW7EhsNjIGDMoK2JidyDD7UXH9N0NpEJfWFEj05_LC4/edit#gid=0\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">docs.google.com/spreadsheets/d</span><span class=\"invisible\">/1jW7EhsNjIGDMoK2JidyDD7UXH9N0NpEJfWFEj05_LC4/edit#gid=0</span></a> (fuente: <a href=\"https://github.com/bamman-group/gpt4-books\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/bamman-group/gpt4-b</span><span class=\"invisible\">ooks</span></a> )</p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@dbamman",
        "display_name": "David Bamman"
      },
      {
        "url": "https://fediscience.org/@UlrichJunker",
        "display_name": "Ulrich Junker"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch (moderator)"
      },
      {
        "url": "https://mastodon.social/@jose_eduardo",
        "display_name": "Jos\u00e9 Eduardo Gonz\u00e1lez"
      },
      {
        "url": "https://sigmoid.social/@roban",
        "display_name": "Roban Hultman Kramer"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09515",
    "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
    "latest": "2023-05-22T01:42:35+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110381269465767328",
      "content": "<p>AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation</p><p>On text summarization, NMT, and common sense generation, AR-Diffusion outperforms existing diffusion LMs and that it can be 100\u00d7 \u223c 600\u00d7 faster when achieving comparable results</p><p><a href=\"https://arxiv.org/abs/2305.09515\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09515</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11206",
    "title": "LIMA: Less Is More for Alignment",
    "latest": "2023-05-22T01:36:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409769364451473",
      "content": "<p>LIMA: Less Is More for Alignment</p><p>Presents LIMA, a 65B LLaMa language model fine-tuned on only 1k curated samples w/o RLHF. </p><p>Outputs from LIMA are either equivalent or strictly preferred to GPT-4 or Bard in 43% or 58% of cases, respectively.</p><p><a href=\"https://arxiv.org/abs/2305.11206\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11206</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01521-z",
    "title": "Humans and algorithms work together \u2014 so study them together",
    "latest": "2023-05-22T01:27:48+00:00",
    "last_post": {
      "url": "https://social.coop/@natematias/110350424967237041",
      "content": "<p>The US Supreme Court will soon decide if YouTube should be held responsible for allegedly permitting the platform\u2019s recommender algorithms to promote content that radicalized terrorists who killed Nohemi Gonzalez.</p><p>Whatever the court concludes, the case highlights an urgent question: how can we govern adaptive algorithms that continually change in response to people\u2019s behavior? The problem? Scientists can't currently answer this question.</p><p>See my new article in <span class=\"h-card\"><a href=\"https://sciencemastodon.com/@nature\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>nature</span></a></span> </p><p><a href=\"https://www.nature.com/articles/d41586-023-01521-z\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01521-z</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://social.coop/@natematias",
        "display_name": "J. Nathan Matias \ud83e\udda3"
      },
      {
        "url": "https://mstdn.social/@kissane",
        "display_name": "Erin Kissane"
      },
      {
        "url": "https://mastodon.social/@markcmarino",
        "display_name": "Mark C Marino"
      },
      {
        "url": "https://mastodon.social/@tiffanycli",
        "display_name": "Tiffany Li"
      },
      {
        "url": "https://mastodon.social/@JoranJongerling",
        "display_name": "Joran"
      },
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://fosstodon.org/@simon_goRin",
        "display_name": "Simon Gorin"
      },
      {
        "url": "https://hcommons.social/@julsraemy",
        "display_name": "Julien A. Raemy"
      },
      {
        "url": "https://social.coop/@dphiffer",
        "display_name": "Dan Phiffer"
      },
      {
        "url": "https://dair-community.social/@timnitGebru",
        "display_name": "Timnit Gebru (she/her)"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11738",
    "title": "https://arxiv.org/abs/2305.11738",
    "latest": "2023-05-22T01:24:23+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409722329971431",
      "content": "<p>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</p><p>repo: <a href=\"https://github.com/microsoft/ProphetNet/tree/master/CRITIC\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/ProphetNe</span><span class=\"invisible\">t/tree/master/CRITIC</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11738\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11738</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11675",
    "title": "https://arxiv.org/abs/2305.11675",
    "latest": "2023-05-22T01:18:22+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409698691968971",
      "content": "<p>Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity</p><p>proj: <a href=\"https://mind-video.com/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">mind-video.com/</span><span class=\"invisible\"></span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11675\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11675</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11598",
    "title": "Introspective Tips: Large Language Model for In-Context Decision Making",
    "latest": "2023-05-22T00:53:21+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409600292246753",
      "content": "<p>Introspective Tips: Large Language Model for In-Context Decision Making</p><p><a href=\"https://arxiv.org/abs/2305.11598\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11598</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11841",
    "title": "How Does Generative Retrieval Scale to Millions of Passages?",
    "latest": "2023-05-22T00:40:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409549149527403",
      "content": "<p>How Does Generative Retrieval Scale to Millions of Passages?</p><p>Finds that the use of synthetic queries as a document representation strategy is the only approach that remained effective as they scaled up the corpus size using MS MARCO passages.</p><p><a href=\"https://arxiv.org/abs/2305.11841\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11841</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11846",
    "title": "https://arxiv.org/abs/2305.11846",
    "latest": "2023-05-22T00:37:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409537321914267",
      "content": "<p>Any-to-Any Generation via Composable Diffusion</p><p>Present CoDi, a novel generative model capable of generating any combination of output modalities from any combination of input modalities.</p><p>proj: <a href=\"https://codi-gen.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">codi-gen.github.io/</span><span class=\"invisible\"></span></a> <br>repo: <a href=\"https://github.com/microsoft/i-Code/tree/main/i-Code-V3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/microsoft/i-Code/tr</span><span class=\"invisible\">ee/main/i-Code-V3</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11846\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11846</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11854",
    "title": "https://arxiv.org/abs/2305.11854",
    "latest": "2023-05-22T00:35:22+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409529572549613",
      "content": "<p>Multimodal Web Navigation with Instruction-Finetuned Foundation Models</p><p>Their 3B model outperforms the SoTA, PaLM-540B, on the WebShop benchmark. </p><p>Will make their 347K high-quality demonstrations publicly available.</p><p>proj: <a href=\"https://sites.google.com/view/mm-webnav/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">sites.google.com/view/mm-webna</span><span class=\"invisible\">v/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.11854\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11854</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11863",
    "title": "Scaling laws for language encoding models in fMRI",
    "latest": "2023-05-22T00:30:20+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110409509792875032",
      "content": "<p>Scaling laws for language encoding models in fMRI</p><p>Increasing scale in both models and data yields incredibly effective models of language processing in the brain, enabling better scientific understanding as well as applications such as decoding.</p><p><a href=\"https://arxiv.org/abs/2305.11863\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11863</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01612-x",
    "title": "For chemists, the AI revolution has yet to happen",
    "latest": "2023-05-21T23:45:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110409331655826124",
      "content": "<p>For chemists, the AI revolution has yet to happen - <a href=\"https://www.nature.com/articles/d41586-023-01612-x\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01612-x</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://osf.io/preprints/metaarxiv/fhdbs/",
    "title": "osf.io/preprints/metaarxiv/fhd...",
    "latest": "2023-05-21T23:39:40+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mcaleerp/110407615890521356",
      "content": "<p>Interesting paper in MetaArXiv: \u201cPreregistration in practice: A comparison of preregistered and non-preregistered studies in psychology\u201c. Main finding is whilst PreReg studies are more likely to state power analysis, inconsistent with previous findings, PreReg and non PreReg studies have similar amount of positive findings. Suggesting either PreReg doesn\u2019t reduce issues or issues are reduced in non PreReg studies these days. </p><p>Link: <a href=\"https://osf.io/preprints/metaarxiv/fhdbs/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">osf.io/preprints/metaarxiv/fhd</span><span class=\"invisible\">bs/</span></a></p><p><a href=\"https://mastodon.social/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a> <a href=\"https://mastodon.social/tags/MetaScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MetaScience</span></a> <a href=\"https://mastodon.social/tags/Psychology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Psychology</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@lakens",
        "display_name": "Daniel Lakens"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.15056",
    "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks",
    "latest": "2023-05-21T22:23:10+00:00",
    "last_post": {
      "url": "https://qoto.org/@tedherman/110409008416499631",
      "content": "<p>Mechanical Turks considered in danger due to LLM/Chat tech. <a href=\"https://arxiv.org/abs/2303.15056\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.15056</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2205.09459",
    "title": "Neural Network Architecture Beyond Width and Depth",
    "latest": "2023-05-21T20:30:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110408564914038079",
      "content": "<p>Neural Network Architecture Beyond Width and Depth - <a href=\"https://arxiv.org/abs/2205.09459\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2205.09459</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s11229-023-04158-7",
    "title": "Epistemic diversity and industrial selection bias - Synthese",
    "latest": "2023-05-21T17:08:11+00:00",
    "last_post": {
      "url": "https://fediscience.org/@ct_bergstrom/110397512520315053",
      "content": "<p>Corporations can influence the directions and conclusions of academic research without corrupting the beliefs of any individual scientist. In a phenomenon known as industry selection bias, companies direct funding and/or data toward scientists who already support the research approaches or technological solutions favored by industry. </p><p>A new paper out in Synthese expands on the earlier Holman and Bruner model of this scenario to illustrate how this process works.</p><p><a href=\"https://link.springer.com/article/10.1007/s11229-023-04158-7\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s11229-023-04158-7</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://nerdculture.de/@dornhaus",
        "display_name": "Anna Dornhaus"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11162",
    "title": "High-Performance Graph Databases That Are Portable, Programmable, and Scale to Hundreds of Thousands of Cores",
    "latest": "2023-05-21T16:30:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110407621156510444",
      "content": "<p>High-Performance Graph Databases, Scaling to Hundreds of Thousands of Cores - <a href=\"https://arxiv.org/abs/2305.11162\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11162</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07759",
    "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
    "latest": "2023-05-21T16:23:02+00:00",
    "last_post": {
      "url": "https://mastodon.online/@jchyip/110386344853645107",
      "content": "<p><a href=\"https://mastodon.online/tags/TinyStories\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>TinyStories</span></a>: How Small Can Language Models Be and Still Speak Coherent English <a href=\"https://arxiv.org/abs/2305.07759\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07759</span><span class=\"invisible\"></span></a> <a href=\"https://mastodon.online/tags/ai\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ai</span></a> <a href=\"https://mastodon.online/tags/llm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>llm</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@bergi",
        "display_name": "Thomas Bergwinkl"
      }
    ]
  },
  {
    "link": "https://frontiersin.org/articles/10.3389/fpubh.2023.1221666/full",
    "title": "frontiersin.org/articles/10.33...",
    "latest": "2023-05-21T15:25:21+00:00",
    "last_post": {
      "url": "https://noc.social/@CaulfieldTim/110402373651558195",
      "content": "<p>Often referenced \"masks do harm\" study retracted.  </p><p>See: <a href=\"https://frontiersin.org/articles/10.3389/fpubh.2023.1221666/full\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">frontiersin.org/articles/10.33</span><span class=\"invisible\">89/fpubh.2023.1221666/full</span></a></p><p>Editors: \"...complaints were valid ... the article does not meet the standards of editorial &amp; scientific soundness...\"</p><p>Will likely live on as a zombie &amp; the retraction will become part of a conspiracy narrative.</p><p><a href=\"https://noc.social/tags/publichealth\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>publichealth</span></a> <a href=\"https://noc.social/tags/masks\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>masks</span></a> <a href=\"https://noc.social/tags/scicomm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>scicomm</span></a> <a href=\"https://noc.social/tags/vaccineswork\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>vaccineswork</span></a> <a href=\"https://noc.social/tags/science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>science</span></a> <a href=\"https://noc.social/tags/retraction\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>retraction</span></a></p>"
    },
    "people": [
      {
        "url": "https://twit.social/@glennf",
        "display_name": "Glenn Fleishman"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41586-020-2487-2",
    "title": "Native American gene flow into Polynesia predating Easter Island settlement - Nature",
    "latest": "2023-05-21T15:17:03+00:00",
    "last_post": {
      "url": "https://social.sanfranciscan.org/objects/8971a192-bac6-4122-b369-24fca39e744f",
      "content": "<p>I\u2019ve always considered the sweet potato in Polynesia to be the \u201csmoking gun\u201d when it came to Pre-Columbian contact between the Americas and the \u201cOld World\u201d.</p><p>Now, genetic evidence appears to back up this theory. The article below concludes:</p><p>\u201c[We] find strong genetic evidence for pre-Columbian human trans-Pacific voyaging contact (at the turn of the twelfth century), contemporaneous with the Polynesian voyages of discovery in the remote eastern Pacific.\u201c</p><p><a href=\"https://www.nature.com/articles/s41586-020-2487-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">https://www.nature.com/articles/s41586-020-2487-2</a></p>"
    },
    "people": [
      {
        "url": "https://thefolklore.cafe/@juergen_hubert",
        "display_name": "J\u00fcrgen Hubert"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      },
      {
        "url": "https://vis.social/@mgiraldo",
        "display_name": "@mgiraldo"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s42254-023-00595-y",
    "title": "A new frontier for Hopfield networks - Nature Reviews Physics",
    "latest": "2023-05-21T14:52:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110402192452069793",
      "content": "<p>RT @DimaKrotov@twitter.com</p><p>Recent advances in Hopfield networks of associative memory may be the guiding theoretical principle for designing novel large scale neural architectures. I explain my enthusiasm about these ideas in the article \u2b07\ufe0f\u2b07\ufe0f\u2b07\ufe0f. Please let me know what you think. <a href=\"https://www.nature.com/articles/s42254-023-00595-y\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s42254-023</span><span class=\"invisible\">-00595-y</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/DimaKrotov/status/1659526803155714053\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/DimaKrotov/status/</span><span class=\"invisible\">1659526803155714053</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21248/jlcl.36.2023.236",
    "title": "doi.org/10.21248/jlcl.36.2023....",
    "latest": "2023-05-21T14:44:49+00:00",
    "last_post": {
      "url": "https://mastodon.social/@fussballinguist/110406313988183627",
      "content": "<p>Our paper on \"Keyness in Song Lyrics\" is out: Jan Langenhorst, Yannick Frommherz and I discuss keyword analysis as an approach to computational stylistics by the example of song lyrics as highly clumpy data. We show that traditional bag-of-words-approached fail because of the repetitiveness of song lyrics and explore the advances of dispersion-oriented methods. <a href=\"https://mastodon.social/tags/openaccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>openaccess</span></a> <a href=\"https://doi.org/10.21248/jlcl.36.2023.236\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.21248/jlcl.36.2023.</span><span class=\"invisible\">236</span></a>.</p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@sascha_wolfer",
        "display_name": "Sascha Wolfer"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1004226",
    "title": "SARS-CoV-2 transmission with and without mask wearing or air cleaners in schools in Switzerland: A modeling study of epidemiological, environmental, and molecular data",
    "latest": "2023-05-21T14:37:30+00:00",
    "last_post": {
      "url": "https://mastodon.social/@mackayim2022/110406167335561539",
      "content": "<p>SARS-CoV-2 transmission with and without mask wearing or air cleaners in schools in Switzerland: A modeling study of epidemiological, environmental, and molecular data. <br><a href=\"https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1004226\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosmedicine</span><span class=\"invisible\">/article?id=10.1371/journal.pmed.1004226</span></a></p>"
    },
    "people": [
      {
        "url": "https://zirk.us/@mrxmrt",
        "display_name": "Marcus"
      },
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11072v1",
    "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
    "latest": "2023-05-21T09:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405994038173144",
      "content": "<p>\ud83d\udcdd Self-Supervised Fine-Tuning for Improved Content Representations by Speaker-Invariant Clustering \ud83d\udcda</p><p>\"Proposes speaker-invariant clustering (Spin) that disentangles speaker information and preserves content representations for pre-trained networks with just 45 minutes of fine-tuning on a single GPU.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11072v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11072v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11068v1",
    "title": "ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph",
    "latest": "2023-05-21T09:12:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405899711320680",
      "content": "<p>\ud83d\udcdd ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph \ud83d\udcda\ud83d\udc7e</p><p>\"Orkg-Leaderboards uses a combination of machine-learning and rule-based approaches to extract Task-Dataset-Metric tuples from scholarly papers in AI domain.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11068v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11068v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11038v1",
    "title": "https://arxiv.org/abs/2305.11038v1",
    "latest": "2023-05-21T08:12:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405663906727945",
      "content": "<p>\ud83d\udcdd Learning in-Context Learning for Named Entity Recognition \ud83d\udcda</p><p>\"A new extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, and a meta-function pre-training algorithm is proposed to pre-train PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/chen700564/metaner-icl\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/chen700564/metaner-</span><span class=\"invisible\">icl</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11038v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11038v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08596",
    "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
    "latest": "2023-05-21T07:55:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110405596165938435",
      "content": "<p>DarkBERT: A Language Model for the Dark Side of the Internet - <a href=\"https://arxiv.org/abs/2305.08596\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08596</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.11082",
    "title": "Fundamental Limitations of Alignment in Large Language Models",
    "latest": "2023-05-21T07:01:50+00:00",
    "last_post": {
      "url": "https://scholar.social/@joakinen/110266044734622865",
      "content": "<p>\"We propose a theoretical approach called Behavior Expectation Bounds (BEB) to investigate several inherent characteristics and limitations of <a href=\"https://scholar.social/tags/alignment\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>alignment</span></a> in <a href=\"https://scholar.social/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a>. We prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt.\"</p><p>Fundamental Limitations of Alignment in Large Language Models<br><a href=\"https://arxiv.org/abs/2304.11082\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.11082</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11029v1",
    "title": "https://arxiv.org/abs/2305.11029v1",
    "latest": "2023-05-21T07:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405380817866757",
      "content": "<p>\ud83d\udcdd Uncertainty Guided Label Denoising for Document-Level Distant Relation Extraction \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a document-level distant relation extraction framework, UGDRE, with uncertainty guided label denoising (UGLD) for reducing noise and retaining high-quality labels.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/QiSun123/UGDRE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/QiSun123/UGDRE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11029v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11029v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/full/10.1080/0950236X.2012.638759",
    "title": "tandfonline.com/doi/full/10.10...",
    "latest": "2023-05-21T06:37:28+00:00",
    "last_post": {
      "url": "https://ravenation.club/@mpe/110405291140999411",
      "content": "<p>It's not open access, but the Textual Practice issue on Amis's _Money_ from a few years ago, edited by my colleague Joe Brooker, is worth a read. <br><a href=\"https://www.tandfonline.com/doi/full/10.1080/0950236X.2012.638759\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/full/10.10</span><span class=\"invisible\">80/0950236X.2012.638759</span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@mpe",
        "display_name": "Martin Paul Eve"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/doi/epdf/10.1080/17513472.2023.2191572",
    "title": "tandfonline.com/doi/epdf/10.10...",
    "latest": "2023-05-21T06:36:16.343000+00:00",
    "last_post": {
      "url": "https://ciberlandia.pt/@villares/110123876632324473",
      "content": "<p>Research Article<br>Knitted origami<br>Elizabeth L. Wilmer</p><p>\"Techniques are presented for embedding horizontal, vertical, and45\u25e6diagonal crease lines into garter stitch knitted fabric. While theseare mostly based on standard lace knitting stitches, the horizon-tal creases use double-cable-crossed elongated stitches in a non-standard way. This crease library suffices to knit a model of a squaretwist, a foundational origami tessellation unit.\"</p><p><a href=\"https://www.tandfonline.com/doi/epdf/10.1080/17513472.2023.2191572\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/doi/epdf/10.10</span><span class=\"invisible\">80/17513472.2023.2191572</span></a></p>"
    },
    "people": [
      {
        "url": "https://ravenation.club/@AlgoCompSynth",
        "display_name": "AlgoCompSynth by znmeb #MaskUp"
      },
      {
        "url": "https://vis.social/@Justin_Lind",
        "display_name": "Justin Lind"
      },
      {
        "url": "https://dair-community.social/@trochee",
        "display_name": "Jeremy Kahn"
      },
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      },
      {
        "url": "https://mstdn.social/@felwert",
        "display_name": "Frederik Elwert"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11023v1",
    "title": "https://arxiv.org/abs/2305.11023v1",
    "latest": "2023-05-21T06:24:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405239283755794",
      "content": "<p>\ud83d\udcdd Generalized Multiple Intent Conditioned Slot Filling \ud83d\udcda</p><p>\"Casts slot filling as a JSON generation task and approach it using a language model trained on pre-training datasets and an in-domain dataset generated with GPT-3.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/reinfer/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/reinfer/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11023v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11023v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11016v1",
    "title": "https://arxiv.org/abs/2305.11016v1",
    "latest": "2023-05-21T05:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110405097640223081",
      "content": "<p>\ud83d\udcdd Silver Syntax Pre-Training for Cross-Domain Relation Extraction \ud83d\udcda</p><p>\"Exploits the affinity between syntactic structure and semantic RE, and identify the syntactic relations which are closely related to RE by being on the shortest dependency path between two entities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mainlp/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mainlp/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11016v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11016v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11000v1",
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "latest": "2023-05-21T05:12:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404955920701559",
      "content": "<p>\ud83d\udcdd SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities \ud83d\udcda</p><p>\"A three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.11000v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11000v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10998v1",
    "title": "The Web Can Be Your Oyster for Improving Large Language Models",
    "latest": "2023-05-21T04:36:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404814488183160",
      "content": "<p>\ud83d\udcdd The Web Can Be Your Oyster for Improving Large Language Models \ud83d\udcda</p><p>\"Proposes a web-augmented LLM UNIWEB which learns over 16 knowledge-intensive tasks in a unified text-to-text format.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10998v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10998v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10992v1",
    "title": "How does the task complexity of masked pretraining objectives affect downstream performance?",
    "latest": "2023-05-21T04:00:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404672752845374",
      "content": "<p>\ud83d\udcdd How Does the Task Complexity of Masked Pretraining Objectives Affect Downstream Performance? \ud83d\udcda\ud83d\udc7e</p><p>\"Pretraining a model with a masked language modeling task and fine-tuning it on downstream tasks such as sentiment analysis or natural language understanding tasks shows the state-of-the-art result.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10992v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10992v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10991v1",
    "title": "Less is More! A slim architecture for optimal language translation",
    "latest": "2023-05-21T03:00:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404436705280754",
      "content": "<p>\ud83d\udcdd Less Is More! A Slim Architecture for Optimal Language Translation \ud83d\udcda\ud83d\udc7e</p><p>\"A gating mechanism to remove excess parameters in softmax attention, and a hierarchical word embedding scheme to boost performance of the embedding layer while also reducing parameter counts by a factor of 3.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10991v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10991v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10985v1",
    "title": "https://arxiv.org/abs/2305.10985v1",
    "latest": "2023-05-21T02:48:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404389675586755",
      "content": "<p>\ud83d\udcdd Multi-CrossRE a Multi-Lingual Multi-Domain Dataset for Relation Extraction \ud83d\udcda</p><p>\"Multi-CrossRE is a multi-lingual RE dataset created by translating CrossRE (Bassignana and Plank, 2022) into different languages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mainlp/CrossRE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mainlp/CrossRE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10985v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10985v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10951v1",
    "title": "https://arxiv.org/abs/2305.10951v1",
    "latest": "2023-05-21T01:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110404153958070478",
      "content": "<p>\ud83d\udcdd Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation \ud83d\udcda</p><p>\"Self-training uses an existing ASR system to generate transcriptions of untranscribed speech, which are then combined with existing human-transcribed speech to improve performance.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Bartelds/asr-augmentation\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/Bartelds/asr-augmen</span><span class=\"invisible\">tation</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10951v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10951v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://link.springer.com/article/10.1007/s00239-019-09896-2",
    "title": "Ageing Throughout History: The Evolution of Human Lifespan - Journal of Molecular Evolution",
    "latest": "2023-05-21T01:25:49+00:00",
    "last_post": {
      "url": "https://mastodon.social/@freakonometrics/110404065675005494",
      "content": "<p>\"Ageing Throughout History: The Evolution of Human Lifespan\" <a href=\"https://link.springer.com/article/10.1007/s00239-019-09896-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">link.springer.com/article/10.1</span><span class=\"invisible\">007/s00239-019-09896-2</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@freakonometrics",
        "display_name": "Arthur Charpentier"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10930v1",
    "title": "https://arxiv.org/abs/2305.10930v1",
    "latest": "2023-05-21T01:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403965242101491",
      "content": "<p>\ud83d\udcdd On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation \ud83d\udcda\ud83d\udc7e</p><p>\"Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/chenllliang/Off-Target-MNMT}{https://github.com/chenllliang/Off-Target-MNMT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/chenllliang/Off-Tar</span><span class=\"invisible\">get-MNMT}{https://github.com/chenllliang/Off-Target-MNMT</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10930v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10930v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10927v1",
    "title": "Causal Document-Grounded Dialogue Pre-training",
    "latest": "2023-05-21T00:36:18+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403870937916053",
      "content": "<p>\ud83d\udcdd Causal Document-Grounded Dialogue Pre-Training \ud83d\udcda</p><p>\"A causally-complete dataset construction strategy and a causally-perturbed pre-training strategy to capture the causal relationships between variables and optimize the overall casual effect, achieving considerable and consistent improvements in the fully-supervised, low-resource, few-shot, and zero-shot settings.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10927v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10927v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281010",
    "title": "The king\u2019s spice cabinet\u2013Plant remains from Gribshunden, a 15th century royal shipwreck in the Baltic Sea",
    "latest": "2023-05-20T23:27:00+00:00",
    "last_post": {
      "url": "https://xoxo.zone/@lauraehall/110397333505819975",
      "content": "<p>1) The shipwreck of the 1495 medieval Danish warship Gribshunden turned out to have incredibly well-preserved plant remains, including expensive spices like saffron, peppercorns, ginger, and almond.</p><p>It's a \u201csubstantially complete royal medieval pantry\u201d and is \"[one of] the most fabulous discoveries of spices in any archaeological context, on land or sea\"</p><p>View the paper here: <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281010\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0281010</span></a></p><p><a href=\"https://xoxo.zone/tags/Medieval\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Medieval</span></a> <a href=\"https://xoxo.zone/tags/Gribshunden\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Gribshunden</span></a> <a href=\"https://xoxo.zone/tags/Shipwreck\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Shipwreck</span></a> <a href=\"https://xoxo.zone/tags/Archaeology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Archaeology</span></a> <a href=\"https://xoxo.zone/tags/MaritimeArchaeology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MaritimeArchaeology</span></a> <a href=\"https://xoxo.zone/tags/Viking\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Viking</span></a> <a href=\"https://xoxo.zone/tags/Vikings\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Vikings</span></a> <a href=\"https://xoxo.zone/tags/Histodons\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Histodons</span></a></p>"
    },
    "people": [
      {
        "url": "https://xoxo.zone/@lauraehall",
        "display_name": "Laura E. Hall"
      },
      {
        "url": "https://kolektiva.social/@aredridel",
        "display_name": "Mx. Aria Stewart"
      },
      {
        "url": "https://everything.happens.horse/@vruba",
        "display_name": "Charlie Loyd"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01705-7",
    "title": "China overtakes United States on contribution to research in Nature Index",
    "latest": "2023-05-20T23:25:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110403590743895835",
      "content": "<p>China overtakes United States on contribution to research in Nature Index - <a href=\"https://www.nature.com/articles/d41586-023-01705-7\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01705-7</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10920v1",
    "title": "https://arxiv.org/abs/2305.10920v1",
    "latest": "2023-05-20T23:12:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403540459568637",
      "content": "<p>\ud83d\udcdd Emergent Communication with Attention \ud83d\udcda\ud83d\udc7e</p><p>\"Attention allows the agents to learn to focus on particular concepts in the environment and align with their partner on the concepts they focus on, which leads to a more compositional and interpretable emergent language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/zalandoresearch/fashion-mnist\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/zalandoresearch/fas</span><span class=\"invisible\">hion-mnist</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10920v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10920v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07922",
    "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
    "latest": "2023-05-20T23:09:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110375897971452835",
      "content": "<p>CodeT5+: Open Code Large Language Models for Code Understanding and Generation</p><p>The instruction-tuned CodeT5+ 16B achieves new SoTA of 35.0% pass@1 on the HumanEval code generation task against other open code LLMs.</p><p><a href=\"https://arxiv.org/abs/2305.07922\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07922</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10907v1",
    "title": "Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation",
    "latest": "2023-05-20T22:24:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403351800700620",
      "content": "<p>\ud83d\udcdd Take a Break in the Middle: Investigating Subgoals Towards Hierarchical Script Generation \ud83d\udcda</p><p>\"The goal is first decomposed into a few steps and then each step is further decomposed into subgoals, until the leaf nodes (i,e, primitive actions) are reached.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10907v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10907v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10866v1",
    "title": "TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition",
    "latest": "2023-05-20T21:24:18+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110403115970890544",
      "content": "<p>\ud83d\udcdd TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition \ud83d\udcda</p><p>\"Proposed to design auxiliary tasks with enlightened prompt learning for Implicit Discourse Relation Recognition task based on the joint training of three prompt learning tasks with shared argument representation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10866v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10866v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10847v1",
    "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
    "latest": "2023-05-20T20:00:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402785326190004",
      "content": "<p>\ud83d\udcdd Large Language Models Can Be Guided to Evade AI-Generated Text Detection \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts, which enables LLMs to evade existing plagiarism detectors.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10847v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10847v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10468",
    "title": "Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence",
    "latest": "2023-05-20T19:46:43+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110402732285772796",
      "content": "<p>[R] Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence</p><p><a href=\"https://arxiv.org/abs/2305.10468\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10468</span><span class=\"invisible\"></span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://arxiv.org/abs/2305.10468\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://arxiv.org/a</span><span class=\"invisible\">bs/2305.10468</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08596?utm_source=tldrai",
    "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
    "latest": "2023-05-20T19:30:02+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110402666666468743",
      "content": "<p>RT @neuroecology@twitter.com</p><p>\"We need to solve the alignment problem before a MegaCorp creates an AI that turns us all into paperclips\"</p><p>Meanwhile, academics:</p><p><a href=\"https://arxiv.org/abs/2305.08596?utm_source=tldrai\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">arxiv.org/abs/2305.08596?utm_s</span><span class=\"invisible\">ource=tldrai</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/neuroecology/status/1659998392817266690\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/neuroecology/statu</span><span class=\"invisible\">s/1659998392817266690</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10845v1",
    "title": "https://arxiv.org/abs/2305.10845v1",
    "latest": "2023-05-20T19:24:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402644030334523",
      "content": "<p>\ud83d\udcdd TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model \ud83d\udcda</p><p>\"Consists of two passes: the first pass uses a non-incremental model, and the second pass uses an incremental policy to revise its prediction based on the first pass.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/pkhdipraja/tapir\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/pkhdipraja/tapir</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10845v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10845v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10833v1",
    "title": "Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants",
    "latest": "2023-05-20T18:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402455092024406",
      "content": "<p>\ud83d\udcdd Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants \ud83d\udcda</p><p>\"Works on a dataset of flowers and plants with the help of different transformer-based models to distinguish between metaphoric and literal flower and plant names using transfer learning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10833v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10833v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10819v1",
    "title": "https://arxiv.org/abs/2305.10819v1",
    "latest": "2023-05-20T18:00:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402313723964201",
      "content": "<p>\ud83d\udcdd CLEME: Debiasing Multi-Reference Evaluation for Grammatical Error Correction \ud83d\udcda</p><p>\"CLEME first constructs consistent chunk sequences for each sentence, then computes F$_{0,5}$ scores based on the chunks, where the boundaries of grammatical errors are determined automatically.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/yejh123/CLEME\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/yejh123/CLEME</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10819v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10819v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/zpgdf/",
    "title": "http://osf.io/zpgdf/",
    "latest": "2023-05-20T17:56:43+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110402299702954366",
      "content": "<p>Measuring Backsliding with Observables: Observable-to-Subjective Score Mapping (OSM) <a href=\"http://osf.io/zpgdf/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/zpgdf/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2203.02155.pdf",
    "title": "https://arxiv.org/pdf/2203.02155.pdf",
    "latest": "2023-05-20T17:28:51+00:00",
    "last_post": {
      "url": "https://mastodon.online/@slashdottir/110402189991015772",
      "content": "<p>nvm, I think I found it</p><p><a href=\"https://arxiv.org/pdf/2203.02155.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2203.02155.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv ML / AI"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10786v1",
    "title": "https://arxiv.org/abs/2305.10786v1",
    "latest": "2023-05-20T17:12:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402124888334936",
      "content": "<p>\ud83d\udcdd Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings \ud83d\udcda</p><p>\"Diagonal Attention Pooling (Ditto) alleviates the anisotropy problem and improve pre-trained models on STS tasks with model-based importance estimations and weighted average of word representations.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/princeton-nlp/SimCSE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/princeton-nlp/SimCS</span><span class=\"invisible\">E</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10786v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10786v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.08298",
    "title": "Symbol tuning improves in-context learning in language models",
    "latest": "2023-05-20T16:51:30+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110375823308376698",
      "content": "<p>Symbol tuning improves in-context learning in language models</p><p>Presents symbol tuning\u2014finetuning LMs on in-context input\u2013label pairs where natural language labels (e.g., \u201cpositive sentiment\u201d) are replaced with arbitrary symbols (e.g., \u201cfoo/bar\u201d).</p><p><a href=\"https://arxiv.org/abs/2305.08298\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.08298</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10736v1",
    "title": "Counterfactual Debiasing for Generating Factually Consistent Text Summaries",
    "latest": "2023-05-20T16:48:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110402030532597530",
      "content": "<p>\ud83d\udcdd Counterfactual Debiasing for Generating Factually Consistent Text Summaries \ud83d\udcda\ud83d\udc7e</p><p>\"Constructs causal graphs for abstractive text summarization and identify the intrinsic causes of factual inconsistency, i,e, language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10736v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10736v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.15004.pdf",
    "title": "https://arxiv.org/pdf/2304.15004.pdf",
    "latest": "2023-05-20T16:20:16+00:00",
    "last_post": {
      "url": "https://dair-community.social/@timnitGebru/110398594256377809",
      "content": "<p>\"we present an al-<br>ternative explanation for emergent abilities: that for a particular task and model<br>family, when analyzing fixed model outputs, one can choose a metric which leads<br>to the inference of an emergent ability or another...which does not..our alternative suggests that..claims of emergent abilities are creations of the researcher\u2019s analyses, not fundamental changes in model behavior on specific tasks with scale\"</p><p>Rylan Schaeffer, Brando Miranda, &amp; Sanmi Koyejo</p><p><a href=\"https://arxiv.org/pdf/2304.15004.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.15004.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@eliocamp",
        "display_name": "Elio Campitelli"
      },
      {
        "url": "https://masto.ai/@cheng",
        "display_name": "Cheng Soon Ong"
      },
      {
        "url": "https://dair-community.social/@timnitGebru",
        "display_name": "Timnit Gebru (she/her)"
      },
      {
        "url": "https://vis.social/@futuraprime",
        "display_name": "Evan Hensleigh"
      },
      {
        "url": "https://wandering.shop/@janellecshane",
        "display_name": "Janelle Shane"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10824v1",
    "title": "https://arxiv.org/abs/2305.10824v1",
    "latest": "2023-05-20T16:16:26+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_ir/110401841834778788",
      "content": "<p>\ud83d\udcdd Integrating Item Relevance in Training Loss for Sequential Recommender Systems \ud83d\udcbf\ud83d\udc7e</p><p>\"Proposes a relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise in user interactions by leveraging the relevance among them.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/IR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>IR</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/pmixer/SASRec.pytorch\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/pmixer/SASRec.pytor</span><span class=\"invisible\">ch</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10824v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10824v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://recsys.social/@karlhigley",
        "display_name": "Karl Higley"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41598-023-32559-8",
    "title": "Efficient shallow learning as an alternative to deep learning - Scientific Reports",
    "latest": "2023-05-20T15:46:21+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110401787124130864",
      "content": "<p>[D] Efficient shallow learning as an alternative to deep learning</p><p><a href=\"https://www.nature.com/articles/s41598-023-32559-8\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41598-023</span><span class=\"invisible\">-32559-8</span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://www.nature.com/articles/s41598-023-32559-8\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://www.nature.</span><span class=\"invisible\">com/articles/s41598-023-32559-8</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10731v1",
    "title": "https://arxiv.org/abs/2305.10731v1",
    "latest": "2023-05-20T15:36:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110401747455686015",
      "content": "<p>\ud83d\udcdd Analyzing Norm Violations in Live-Stream Chat \ud83d\udcda</p><p>\"Collects a dataset of over 4,500 moderated Twitch chat comments and annotate each comment according to a taxonomy of norm violations specific to live-streams.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/TwitchIO/TwitchIO\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/TwitchIO/TwitchIO</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10731v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10731v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10713v1",
    "title": "https://arxiv.org/abs/2305.10713v1",
    "latest": "2023-05-20T15:24:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110401700309778533",
      "content": "<p>\ud83d\udcdd Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency \ud83d\udcda\ud83e\udde0</p><p>\"Quantifies the robustness of the model towards its parameter perturbations and is based on the flatness metric from statistical learning theory, which has been shown to be effective in quantifying generalization and robustness properties of models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/shadowkiller33/flatness\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/shadowkiller33/flat</span><span class=\"invisible\">ness</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10713v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10713v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10160",
    "title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
    "latest": "2023-05-20T15:08:59+00:00",
    "last_post": {
      "url": "https://techhub.social/@arthurh/110401640169356571",
      "content": "<p>While I understand the problem of data contamination, I thought it would be easy to detect using the difference in perplexity between the train and the test split <a href=\"https://techhub.social/tags/nlp\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>nlp</span></a> <a href=\"https://techhub.social/tags/ai\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ai</span></a> <a href=\"https://techhub.social/tags/llm\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>llm</span></a></p><p>[2305.10160] Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks <a href=\"https://arxiv.org/abs/2305.10160\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10160</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://techhub.social/@arthurh",
        "display_name": "Arthur"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10709v1",
    "title": "NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing",
    "latest": "2023-05-20T14:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110401558676983973",
      "content": "<p>\ud83d\udcdd NoisywikiHow: A Benchmark for Learning with Real-World Noisy Labels in Natural Language Processing \ud83d\udcda\ud83d\udc7e</p><p>\"We explicitly construct multiple sources of label noise to imitate human errors throughout the annotation, replicating real-world noise, whose corruption is affected by both ground-truth labels and instances.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10709v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10709v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2101.04719",
    "title": "https://arxiv.org/abs/2101.04719",
    "latest": "2023-05-20T14:47:31+00:00",
    "last_post": {
      "url": "https://hci.social/@upol/110397497407073819",
      "content": "<p>\ud83c\udf81Starter pack to avoid Explainability Washing:</p><p>1) Don\u2019t just assert it\u2019s an explanation, justify how it is so: focus on the why-question [B,C]</p><p>2) Identify *who* the users are + *how* it helps them to *do* something [A, B]</p><p>3) Focus on *who* is opening the black-box, not just on opening it [A]</p><p>[A] The Who in XAI: <a href=\"https://twitter.com/UpolEhsan/status/1421142478229540867?t=0QmE6_gigEIAsdVYWMUYCQ&amp;s=19\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/UpolEhsan/status/1</span><span class=\"invisible\">421142478229540867?t=0QmE6_gigEIAsdVYWMUYCQ&amp;s=19</span></a></p><p>[B] Expanding Explainability: <a href=\"https://arxiv.org/abs/2101.04719\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2101.04719</span><span class=\"invisible\"></span></a></p><p>[C] XAI + Social Science: <a href=\"https://www.sciencedirect.com/science/article/pii/S0004370218305988\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">sciencedirect.com/science/arti</span><span class=\"invisible\">cle/pii/S0004370218305988</span></a></p><p>5/5 (n=5)</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      },
      {
        "url": "https://hci.social/@andresmh",
        "display_name": "Andr\u00e9s Monroy-Hern\u00e1ndez"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10601v1",
    "title": "https://arxiv.org/abs/2305.10601v1",
    "latest": "2023-05-20T09:48:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110400379093908849",
      "content": "<p>\ud83d\udcdd Tree of Thoughts: Deliberate Problem Solving with Large Language Models \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Language models are being increasingly used for a wide range of problem solving tasks, but are confined to making token-level left-to-right decisions during inference, and can fall short when initial decisions play a pivotal role or when the problem is not well defined.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/ysymyth/tree-of-thought-llm\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/ysymyth/tree-of-tho</span><span class=\"invisible\">ught-llm</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10601v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10601v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10588v1",
    "title": "A Better Way to Do Masked Language Model Scoring",
    "latest": "2023-05-20T08:48:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110400143124504188",
      "content": "<p>\ud83d\udcdd A Better Way to Do Masked Language Model Scoring \ud83d\udcda</p><p>\"Estimating the log-likelihood of a given sentence under a masked language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10588v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10588v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10568v1",
    "title": "From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?",
    "latest": "2023-05-20T08:12:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110400001467812777",
      "content": "<p>\ud83d\udcdd From Chocolate Bunny to Chocolate Crocodile: Do Language Models Understand Noun Compounds? \ud83d\udcda</p><p>\"The outputs from GPT-3 often have significant overlap with a large web corpus, but the parroting strategy is less beneficial for novel noun compounds and for noun-noun compounds than more compositional compounds like adjective-noun and verb-object compounds.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10568v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10568v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10561v1",
    "title": "Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search",
    "latest": "2023-05-20T07:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399859886252548",
      "content": "<p>\ud83d\udcdd Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search \ud83d\udcda</p><p>\"ISI-Clear uses an existing state-of-the-art event extraction system (TriggerWild) to detect and classify event triggers in documents.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10561v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10561v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.09144",
    "title": "http://arxiv.org/abs/2305.09144",
    "latest": "2023-05-20T07:05:27+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@BenjaminHan/110399738810535290",
      "content": "<p>8/</p><p>[2] Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Jiawei Chen, Tianshu Wang, and Le Sun. 2023. Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. <a href=\"http://arxiv.org/abs/2305.09144\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.09144</span><span class=\"invisible\"></span></a> </p><p>[3] Hermann Ebbinghaus. 2013. Memory: a contribution to experimental psychology. Annals of neurosciences, 20(4):155\u2013156. <a href=\"http://dx.doi.org/10.5214/ans.0972.7531.200408\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"ellipsis\">dx.doi.org/10.5214/ans.0972.75</span><span class=\"invisible\">31.200408</span></a> </p><p><a href=\"https://sigmoid.social/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a> <a href=\"https://sigmoid.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://sigmoid.social/tags/MachineLearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MachineLearning</span></a> <a href=\"https://sigmoid.social/tags/Paper\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Paper</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10557v1",
    "title": "Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations",
    "latest": "2023-05-20T07:00:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399718341880996",
      "content": "<p>\ud83d\udcdd Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations \ud83d\udcda</p><p>\"Regularizes self-attention to be more symmetric, which is expected to enhance APE models' understanding of the target language by encouraging the model to pay attention to more words within the sentence than just the words in the source.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10557v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10557v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10519v1",
    "title": "Statistical Knowledge Assessment for Generative Language Models",
    "latest": "2023-05-20T06:00:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399482432950391",
      "content": "<p>\ud83d\udcdd Statistical Knowledge Assessment for Generative Language Models \ud83d\udcda\ud83e\udde0</p><p>\"Our proposed knowledge assessment framework, which includes latent variables and the KaRR metric, quantifies a model's knowledge by computing the probability of generating factually correct text from diverse forms.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10519v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10519v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10496v1",
    "title": "https://arxiv.org/abs/2305.10496v1",
    "latest": "2023-05-20T05:36:13+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399387940976125",
      "content": "<p>\ud83d\udcdd Incorporating Attribution Importance for Improving Faithfulness Metrics \ud83d\udcda\ud83d\udc7e</p><p>\"Soft-faithfulness is a soft erasure criterion that replaces hard erasure criterion in computing faithfulness metrics, which better reflects how faithful an attribution is to the model prediction.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/casszhao/SoftFaith\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/casszhao/SoftFaith</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10496v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10496v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10448v1",
    "title": "Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding",
    "latest": "2023-05-20T04:36:14+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110399152083963645",
      "content": "<p>\ud83d\udcdd Sequence-to-Sequence Pre-Training with Unified Modality Masking for Visual Document Understanding \ud83d\udcda\ud83d\udc7e</p><p>\"Adopts a transformer encoder-decoder architecture and pre-trains the model with unified masking across three modalities: text, image, and layout, and adopts both modality-specific instruction and the mixture of modality experts strategy to better capture each modality.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10448v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10448v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/vn5tr/",
    "title": "http://osf.io/vn5tr/",
    "latest": "2023-05-20T03:41:07+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110398935367821350",
      "content": "<p>Inoculating against Persuasion by Male Supremacy Messages: The Moderating Roles of Propaganda Form and Subtlety <a href=\"http://osf.io/vn5tr/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/vn5tr/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/epn2c/",
    "title": "http://osf.io/epn2c/",
    "latest": "2023-05-20T03:41:07+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110398935398590929",
      "content": "<p>Six Months in Exile: A New Life of Russian Emigrants <a href=\"http://osf.io/epn2c/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/epn2c/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10447v1",
    "title": "The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring",
    "latest": "2023-05-20T03:36:15+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398916256178738",
      "content": "<p>\ud83d\udcdd The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring \ud83d\udcda\ud83d\udc7e</p><p>\"Uses a dynamic loss function that creates an incentive for the model to predict with the correct distribution, as well as predicting the correct values using a modified mean square error loss.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10447v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10447v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/8n2gj/",
    "title": "http://osf.io/8n2gj/",
    "latest": "2023-05-20T03:32:21+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110398900870962571",
      "content": "<p>Visual autoethnography of daily sounds <a href=\"http://osf.io/8n2gj/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/8n2gj/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/c8qr4/",
    "title": "http://osf.io/c8qr4/",
    "latest": "2023-05-20T03:32:20+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110398900849602124",
      "content": "<p>The Echo of Neighborhood Disadvantage: Multigenerational Contextual Hardship and Adult Income for Whites, Blacks, and Latinos <a href=\"http://osf.io/c8qr4/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/c8qr4/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10438v1",
    "title": "https://arxiv.org/abs/2305.10438v1",
    "latest": "2023-05-20T03:12:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398822015564378",
      "content": "<p>\ud83d\udcdd IMAGINATOR: Pre-Trained Image+Text Joint Embeddings Using Word-Level Grounding of Images \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"Joint Embedding IMAGINATOR is jointly trained on three individual representations: (i) object-object co-location, (ii) word-object co-location, and (iii) word-object correlation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/varunakk/IMAGINATOR\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/varunakk/IMAGINATOR</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10438v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10438v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10436v1",
    "title": "SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues",
    "latest": "2023-05-20T02:12:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398586015737803",
      "content": "<p>\ud83d\udcdd SmartPhone: Exploring Keyword Mnemonic with Auto-Generated Verbal and Visual Cues \ud83d\udcda</p><p>\"Leverages large language models to generate cues based on a combination of three factors: (i) the new word, (ii) the keyword, and (iii) the cue-type.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10436v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10436v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/pdf/10.1145/3479860",
    "title": "dl.acm.org/doi/pdf/10.1145/347...",
    "latest": "2023-05-20T02:05:23+00:00",
    "last_post": {
      "url": "https://hci.social/@sarahgilbert/110395673665571480",
      "content": "<p><span class=\"h-card\"><a href=\"https://social.coop/@natematias\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>natematias</span></a></span> Dipto Das, a doc candidate at UC Boulder, has been researching colonialism on Quora. Perhaps his work will be helpful? <br> <a href=\"https://dl.acm.org/doi/pdf/10.1145/3479860\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/347</span><span class=\"invisible\">9860</span></a><br><a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517600\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/fullHtml/10.114</span><span class=\"invisible\">5/3491102.3517600</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517600",
    "title": "dl.acm.org/doi/fullHtml/10.114...",
    "latest": "2023-05-20T02:05:23+00:00",
    "last_post": {
      "url": "https://hci.social/@sarahgilbert/110395673665571480",
      "content": "<p><span class=\"h-card\"><a href=\"https://social.coop/@natematias\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>natematias</span></a></span> Dipto Das, a doc candidate at UC Boulder, has been researching colonialism on Quora. Perhaps his work will be helpful? <br> <a href=\"https://dl.acm.org/doi/pdf/10.1145/3479860\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/347</span><span class=\"invisible\">9860</span></a><br><a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517600\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/fullHtml/10.114</span><span class=\"invisible\">5/3491102.3517600</span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10435v1",
    "title": "Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",
    "latest": "2023-05-20T01:36:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398444492867803",
      "content": "<p>\ud83d\udcdd Generative Pre-Trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions \ud83d\udcda\ud83d\udc7e</p><p>\"Provides a detailed overview of the Generative Pre-trained Transformer, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10435v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10435v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10434v1",
    "title": "https://arxiv.org/abs/2305.10434v1",
    "latest": "2023-05-20T01:24:16+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398397256873838",
      "content": "<p>\ud83d\udcdd Learning the Visualness of Text Using Large Vision-Language Models \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Proposes a fine-tuning strategy that adapts large vision-language models like CLIP to the task of scoring text visualness from text input alone by modifying contrastive learning objective to map text identified as non-visual and visual to a common NULL image and its corresponding image.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/pymupdf/PyMuPDF\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/pymupdf/PyMuPDF</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10434v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10434v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10433v1",
    "title": "https://arxiv.org/abs/2305.10433v1",
    "latest": "2023-05-20T00:36:10+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110398208130523515",
      "content": "<p>\ud83d\udcdd Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback \ud83d\udcda</p><p>\"Provides a mechanism to improve the reliability of toxic language datasets by centering the evaluator's values in the dataset creation process through a human-in-the-loop process that involves the evaluator in dataset curation via an iterative feedback cycle.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/SI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/HKUST-KnowComp/MLMA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/HKUST-KnowComp/MLMA</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10433v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10433v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10355v1",
    "title": "https://arxiv.org/abs/2305.10355v1",
    "latest": "2023-05-19T23:30:25+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110397949604896968",
      "content": "<p>\ud83d\udcdd Evaluating Object Hallucination in Large Vision-Language Models \ud83d\udd2d\ud83d\udcda</p><p>\"POPE is a polling-based query mechanism for evaluation, which can better reflect the object hallucination problem of large vision-language models by filtering out the impact of different input instructions and generation styles.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/RUCAIBox/POPE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/RUCAIBox/POPE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10355v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10355v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09807v1",
    "title": "https://arxiv.org/abs/2305.09807v1",
    "latest": "2023-05-19T21:30:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110397478087456156",
      "content": "<p>\ud83d\udcdd On Dataset Transferability in Active Learning for Transformers \ud83e\udde0\ud83d\udcda</p><p>\"Studies the transferability of the actively acquired datasets from the perspective of the similarity between queries from different models used for active learning on the same task and find that models with more similar active sequences result in more transferable datasets.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/fjelenic/al-transfer\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/fjelenic/al-transfe</span><span class=\"invisible\">r</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.09807v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09807v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10429v1",
    "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
    "latest": "2023-05-19T20:42:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110397289254856860",
      "content": "<p>\ud83d\udcdd DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining \ud83d\udcda\ud83e\udde0</p><p>\"DoReMi first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10429v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10429v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10427v1",
    "title": "Accelerating Transformer Inference for Translation via Parallel Decoding",
    "latest": "2023-05-19T20:06:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110397147711324485",
      "content": "<p>\ud83d\udcdd Accelerating Transformer Inference for Translation via Parallel Decoding \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Jacobi and Gauss-Seidel parallel decoding algorithms reframe the standard greedy decoding in MT leveraging fixed-point iteration methods to speed up inference without training or modifications while retaining translation quality.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10427v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10427v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.09620",
    "title": "AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys",
    "latest": "2023-05-19T19:31:19+00:00",
    "last_post": {
      "url": "https://mastodon.social/@cbail/110396994037297923",
      "content": "<p>Can Generative AI help us deal with missing data? Interesting new paper: <a href=\"https://arxiv.org/abs/2305.09620\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.09620</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10425v1",
    "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback",
    "latest": "2023-05-19T19:18:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110396958808094878",
      "content": "<p>\ud83d\udcdd SLiC-HF: Sequence Likelihood Calibration with Human Feedback \ud83d\udcda\ud83d\udc7e</p><p>\"Sequence Likelihood Calibration uses a simple modification to supervised learning that allows models to better learn from human feedback without the need for RL algorithms and without the need for human preference data for the model being optimized.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10425v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10425v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10973",
    "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold",
    "latest": "2023-05-19T18:45:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110396827400173185",
      "content": "<p>Drag Your GAN: Interactive Point-Based Manipulation of Images - <a href=\"https://arxiv.org/abs/2305.10973\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10973</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07558",
    "title": "Measuring Progress in Fine-grained Vision-and-Language Understanding",
    "latest": "2023-05-19T18:21:25+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110369937023196434",
      "content": "<p>Measuring Progress in Fine-grained Vision-and-Language Understanding</p><p>Finds that X-VLM consistently outperforms other <br>VL baselines, and that modelling innovations can impact performance more than scaling Web data.</p><p><a href=\"https://arxiv.org/abs/2305.07558\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07558</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10403v1",
    "title": "PaLM 2 Technical Report",
    "latest": "2023-05-19T18:18:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110396723061735952",
      "content": "<p>\ud83d\udcdd PaLM 2 Technical Report \ud83d\udcda\ud83d\udc7e</p><p>\"The PaLM 2 model is a Transformer-based model trained using a mixture of objectives: autoregressive language modeling, masked language modeling, and sentence order prediction (SOP).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10403v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10403v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.11030",
    "title": "Is Our Organization Actually Measuring Productivity? How Contrasting Organizational and Individual Measures of Engineering Success is an Opportunity to Drive Engineering Transformation",
    "latest": "2023-05-19T18:05:46+00:00",
    "last_post": {
      "url": "https://mastodon.social/@grimalkina/110396673029522948",
      "content": "<p>We've got a new preprint out!! We give an example of using our Three Layer approach to \"productivity metrics\" in an exercise with leaders, managers, and IC developers, and how it reveals important insights about where engineers disagree with their organizations and want to suggest better. </p><p><a href=\"https://arxiv.org/abs/2305.11030\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.11030</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@grimalkina",
        "display_name": "Cat Hicks"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10400v1",
    "title": "What You See is What You Read? Improving Text-Image Alignment Evaluation",
    "latest": "2023-05-19T17:42:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110396581396918372",
      "content": "<p>\ud83d\udcdd What You See Is What You Read? Improving Text-Image Alignment Evaluation \ud83d\udcda\ud83d\udd2d</p><p>\"Generates a set of questions about the image, and then evaluate how well these questions are answered by a given text-image pair using a visual question answering model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10400v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10400v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10387v1",
    "title": "https://arxiv.org/abs/2305.10387v1",
    "latest": "2023-05-19T17:06:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110396439763513750",
      "content": "<p>\ud83d\udcdd Elaborative Simplification as Implicit Questions Under Discussion \ud83d\udcda</p><p>\"Elaborative simplification is viewed through the lens of the Question Under Discussion (QUD) framework, providing a robust way to investigate what writers elaborate upon, how they elaborate, and how elaborations fit into the discourse context.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/sheffwb/elabQUD\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/sheffwb/elabQUD</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10387v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10387v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.10429",
    "title": "http://arxiv.org/abs/2305.10429",
    "latest": "2023-05-19T17:03:59+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110396430066556118",
      "content": "<p>RT @sangmichaelxie@twitter.com</p><p>Should LMs train on more books, news, or web data?</p><p>Introducing DoReMi\ud83c\udfb6, which optimizes the data mixture with a small 280M model.</p><p>Our data mixture makes 8B Pile models train 2.6x faster, get +6.5% few-shot acc, and get lower pplx on *all* domains!</p><p>\ud83e\uddf5\u2b07\ufe0f <a href=\"http://arxiv.org/abs/2305.10429\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.10429</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/sangmichaelxie/status/1659230705165975552\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/sangmichaelxie/sta</span><span class=\"invisible\">tus/1659230705165975552</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.00924",
    "title": "HasChor: Functional Choreographic Programming for All (Functional Pearl)",
    "latest": "2023-05-19T16:21:44+00:00",
    "last_post": {
      "url": "https://recurse.social/@lindsey/110396194594060604",
      "content": "<p>Very happy to share that \"HasChor: Functional Choreographic Programming for All\", a functional pearl paper led by my students Gan Shen and Shun Kashiwa, has been accepted to ICFP 2023! \ud83c\udf89 <a href=\"https://arxiv.org/abs/2303.00924\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.00924</span><span class=\"invisible\"></span></a></p><p>A short thread about HasChor: \ud83d\udc47</p>"
    },
    "people": [
      {
        "url": "https://hci.social/@chrisamaphone",
        "display_name": "chris martens (they/them)"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10154v1",
    "title": "Iterated learning and communication jointly explain efficient color naming systems",
    "latest": "2023-05-19T09:54:26+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394741008903010",
      "content": "<p>\ud83d\udcdd Iterated Learning and Communication Jointly Explain Efficient Color Naming Systems \ud83d\udcda</p><p>\"A combination of iterated learning and communication yields systems efficient with respect to the Information Bottleneck principle, and similar to human color naming systems, while iterated learning or communication on their own do not.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10154v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10154v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10149v1",
    "title": "https://arxiv.org/abs/2305.10149v1",
    "latest": "2023-05-19T09:18:25+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394599390464470",
      "content": "<p>\ud83d\udcdd Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog \ud83d\udcda</p><p>\"Proposes to decouple knowledge retrieval from response generation and introduce a multi-grained knowledge retriever (MAKER) that includes an entity selector to search for relevant entities and an attribute selector to filter out irrelevant attributes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/18907305772/MAKER\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/18907305772/MAKER</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10149v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10149v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2211.01301",
    "title": "Computational Desire Line Analysis of Cyclists on the Dybb\u00f8lsbro Intersection in Copenhagen",
    "latest": "2023-05-19T08:31:12+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@underdarkGIS/110390854326449081",
      "content": "<p>Did you know that <span class=\"h-card\"><a href=\"https://fosstodon.org/@movingpandas\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>movingpandas</span></a></span> also supports local image coordinates? </p><p>Currently working on a demo using <a href=\"https://fosstodon.org/tags/bicycle\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>bicycle</span></a> tracks extracted from <a href=\"https://fosstodon.org/tags/video\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>video</span></a> footage \ud83d\udeb4\ud83c\udfa5   published by <span class=\"h-card\"><a href=\"https://datasci.social/@mszll\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>mszll</span></a></span> </p><p><a href=\"https://arxiv.org/abs/2211.01301\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2211.01301</span><span class=\"invisible\"></span></a></p><p><a href=\"https://fosstodon.org/tags/openscience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>openscience</span></a> <a href=\"https://fosstodon.org/tags/MobilityAnalytics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MobilityAnalytics</span></a> <a href=\"https://fosstodon.org/tags/MobilityDataScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MobilityDataScience</span></a> <a href=\"https://fosstodon.org/tags/mobility\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>mobility</span></a> <a href=\"https://fosstodon.org/tags/MovingPandas\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MovingPandas</span></a> <a href=\"https://fosstodon.org/tags/TrafficFlow\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>TrafficFlow</span></a> <a href=\"https://fosstodon.org/tags/planning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>planning</span></a> <a href=\"https://fosstodon.org/tags/cycling\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>cycling</span></a> <a href=\"https://fosstodon.org/tags/TrafficBehavior\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>TrafficBehavior</span></a></p>"
    },
    "people": [
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      },
      {
        "url": "https://fosstodon.org/@underdarkGIS",
        "display_name": "Anita Graser \ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10142v1",
    "title": "https://arxiv.org/abs/2305.10142v1",
    "latest": "2023-05-19T08:30:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394410744349025",
      "content": "<p>\ud83d\udcdd Improving Language Model Negotiation with Self-Play and in-Context Learning From AI Feedback \ud83d\udcda</p><p>\"Uses different LLMs as the buyer, the seller, and the critic, respectively, in a negotiation game that aims to reach an agreement on a deal price for a product.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/FranxYao/GPT-Bargaining\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/FranxYao/GPT-Bargai</span><span class=\"invisible\">ning</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10142v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10142v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10122v1",
    "title": "https://arxiv.org/abs/2305.10122v1",
    "latest": "2023-05-19T08:06:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394316501401879",
      "content": "<p>\ud83d\udcdd Empirical Analysis of Oral and Nasal Vowels of Konkani \ud83d\udcda</p><p>\"Compares the acoustic-phonetic properties of Konkani oral and nasal vowels, which can be helpful for linguistic research on vowels and speech synthesis systems specific to the Konkani language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/shashwatup9k/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/shashwatup9k/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10122v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10122v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10601",
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "latest": "2023-05-19T07:10:50+00:00",
    "last_post": {
      "url": "https://hachyderm.io/@markcarter/110394077530832644",
      "content": "<p>\ud83e\udd14 Tree of Thoughts: Deliberate Problem Solving with Large Language Models. we introduce a new framework for language model inference, which enables exploration over coherent units of text (thoughts) that serve as steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking <a href=\"https://arxiv.org/abs/2305.10601\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10601</span><span class=\"invisible\"></span></a> <a href=\"https://hachyderm.io/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10096v1",
    "title": "Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots",
    "latest": "2023-05-19T06:54:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110394033326123274",
      "content": "<p>\ud83d\udcdd Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots \ud83d\udcda\ud83d\udc7e</p><p>\"Consists of two modules: 1) a response emotion/intent prediction module; and 2) a response generation module which conditions on these predicted emotion/intents.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10096v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10096v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2302.05669",
    "title": "Treat societally impactful scientific insights as open-source software artifacts",
    "latest": "2023-05-19T06:44:32+00:00",
    "last_post": {
      "url": "https://akademienl.social/@cynthiacsliem/110391932047344538",
      "content": "<p>For any <a href=\"https://akademienl.social/tags/icse2023\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>icse2023</span></a> attendees/followers and enthusiasts about open science and academic reform: looking forward to presenting 'Treat societally impactful scientific insights as open-source software artifacts', a Software Engineering in Society position paper co-authored with my PhD Andrew Demetriou, this afternoon 15:52 h @ room 110. Promising to make it provocative \ud83d\ude07 </p><p>Pre-print: <a href=\"https://arxiv.org/abs/2302.05669\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2302.05669</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://dair-community.social/@DrVeronikaCH",
        "display_name": "Veronika Cheplygina"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10046v1",
    "title": "https://arxiv.org/abs/2305.10046v1",
    "latest": "2023-05-19T05:54:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393797469724681",
      "content": "<p>\ud83d\udcdd Probing the Role of Positional Information in Vision-Language Models \ud83d\udcda\ud83d\udd2d</p><p>\"Introduces two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PUses Cross-Modality Matching.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/airsplay/lxmert/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/airsplay/lxmert/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10046v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10046v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10037v1",
    "title": "https://arxiv.org/abs/2305.10037v1",
    "latest": "2023-05-19T05:06:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393608680789679",
      "content": "<p>\ud83d\udcdd Can Language Models Solve Graph Problems in Natural Language? \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes NLGraph, a comprehensive benchmark of graph-based problem solving designed in natural language to evaluate the graph reasoning abilities of large language models, and provide instruction-based approaches to improve their performance.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Arthur-Heng/NLGraph\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Arthur-Heng/NLGraph</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10037v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10037v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10036v1",
    "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
    "latest": "2023-05-19T04:30:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393467089995099",
      "content": "<p>\ud83d\udcdd Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark \ud83d\udcda</p><p>\"The Embedding Watermark (EmbMarker) implants a watermark into the embeddings of the EaaS model's training set to verify copyright infringement.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CY\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CY</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10036v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10036v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.10013v1",
    "title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario",
    "latest": "2023-05-19T03:42:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110393278468069950",
      "content": "<p>\ud83d\udcdd When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario \ud83d\udcda\ud83d\udc7e</p><p>\"GDFO integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonized manner through knowledge distillation, which can achieve significant performance gains over previous state-of-the-art methods.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.10013v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.10013v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  }
]