[
  {
    "link": "http://osf.io/vhkd8/",
    "title": "http://osf.io/vhkd8/",
    "latest": "2023-05-13T22:02:31+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110363630047948529",
      "content": "<p>Is there a risk of APC-driven guest authorship? <a href=\"http://osf.io/vhkd8/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/vhkd8/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/s6yxw/",
    "title": "http://osf.io/s6yxw/",
    "latest": "2023-05-13T22:02:31+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110363630067746582",
      "content": "<p>Uncertainty in Crisis Bargaining with Multiple Policy Options <a href=\"http://osf.io/s6yxw/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/s6yxw/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/eg6cb/",
    "title": "http://osf.io/eg6cb/",
    "latest": "2023-05-13T22:02:30+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110363630016376066",
      "content": "<p>Disturbing the Balance? How team media position themselves in the digital communication ecology of sports <a href=\"http://osf.io/eg6cb/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/eg6cb/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/d2zv6/",
    "title": "http://osf.io/d2zv6/",
    "latest": "2023-05-13T21:56:39+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110363606998654235",
      "content": "<p>BUILDING INTEGRATED MODELS IN ENVIRONMENTAL AND NATURAL RESOURCE ECONOMICS: THE CASE OF GORDON'S 1954 FISHERY MODEL <a href=\"http://osf.io/d2zv6/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/d2zv6/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06677v1",
    "title": "INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models",
    "latest": "2023-05-13T21:09:22+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110363421049641587",
      "content": "<p>\ud83d\udcdd INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Consists of two stages (see Figure for an illustration): (a) data subset selection, and (b) model training on the subsets.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06677v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06677v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/bsvr8/",
    "title": "http://osf.io/bsvr8/",
    "latest": "2023-05-13T20:41:22+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110363311010012054",
      "content": "<p>New Methods for Old Questions: Predicting Historical Urban Renewal Areas in the United States <a href=\"http://osf.io/bsvr8/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/bsvr8/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/vdspj/",
    "title": "http://osf.io/vdspj/",
    "latest": "2023-05-13T20:41:22+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110363310985163850",
      "content": "<p>Le Jacobin mecanique par Russel Kirk <a href=\"http://osf.io/vdspj/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/vdspj/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06595v1",
    "title": "https://arxiv.org/abs/2305.06595v1",
    "latest": "2023-05-13T20:34:11+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110361533428197820",
      "content": "<p>\ud83d\udcdd BanglaBook: A Large-Scale Bangla Dataset for Sentiment Analysis From Book Reviews \ud83d\udcda</p><p>\"Presents a large-scale dataset of Bangla book reviews consisting of 158,065 samples, classified into three broad categories: positive, negative, and neutral.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mohsinulkabir14/BanglaBook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/mohsinulkabir14/Ban</span><span class=\"invisible\">glaBook</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06595v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06595v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@davidmortensen",
        "display_name": "David Mortensen"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-00633-w",
    "title": "Fed up and burnt out: \u2018quiet quitting\u2019 hits academia",
    "latest": "2023-05-13T19:41:54+00:00",
    "last_post": {
      "url": "https://sciences.social/@WeedenKim/110355268010980468",
      "content": "<p>20% (=505/2502) of readers in Nature's nonrandom poll on \"quiet quitting\" said they had reduced effort in peer review. If generalizes, expect ever-longer review times, esp. if AI-generated papers swamp system.</p><p>25% (=635/2502) had reduced their effort in conferences. This seems kinda bad for organizations that rely on conferences for operating revenue.</p><p>(Rank-specific Ns are not reported, so the rank-specific counts of activities are fairly meaningless.)</p><p>Article here: <br><a href=\"https://www.nature.com/articles/d41586-023-00633-w\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-00633-w</span></a></p>"
    },
    "people": [
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      },
      {
        "url": "https://mastodon.social/@ojala",
        "display_name": "Luis Apiolaza"
      },
      {
        "url": "https://tech.lgbt/@_dmh",
        "display_name": "Dave Howcroft \ud83e\udd94"
      },
      {
        "url": "https://social.luca.run/@luca",
        "display_name": "Luca \ud83d\udd28"
      },
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://mstdn.social/@felwert",
        "display_name": "Frederik Elwert"
      },
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06655v1",
    "title": "QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing",
    "latest": "2023-05-13T19:29:19+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110363027671050974",
      "content": "<p>\ud83d\udcdd QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing \ud83d\udcda</p><p>\"QURG is a two-stream matrix encoder that jointly models the rewriting relations between question and context, and the schema linking relations between natural language and structured schema.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06655v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06655v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41587-022-01618-2",
    "title": "Large language models generate functional protein sequences across diverse families - Nature Biotechnology",
    "latest": "2023-05-13T19:20:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110362991179124574",
      "content": "<p>Large language models generate functional protein sequences across families - <a href=\"https://www.nature.com/articles/s41587-022-01618-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41587-022</span><span class=\"invisible\">-01618-2</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06647v1",
    "title": "PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization",
    "latest": "2023-05-13T18:29:21+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110362791854688499",
      "content": "<p>\ud83d\udcdd PROM: A Phrase-Level Copying Mechanism with Pre-Training for Abstractive Summarization \ud83d\udcda</p><p>\"PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06647v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06647v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.00118",
    "title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",
    "latest": "2023-05-13T17:36:36+00:00",
    "last_post": {
      "url": "https://masto.ai/@caseydurfee/110317412756473839",
      "content": "<p>\"OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. \" <a href=\"https://arxiv.org/abs/2305.00118\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.00118</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@dbamman",
        "display_name": "David Bamman"
      },
      {
        "url": "https://fediscience.org/@UlrichJunker",
        "display_name": "Ulrich Junker"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch (moderator)"
      },
      {
        "url": "https://mastodon.social/@jose_eduardo",
        "display_name": "Jos\u00e9 Eduardo Gonz\u00e1lez"
      },
      {
        "url": "https://sigmoid.social/@roban",
        "display_name": "Roban Hultman Kramer"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06620v1",
    "title": "https://arxiv.org/abs/2305.06620v1",
    "latest": "2023-05-13T16:49:22+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110362398719167717",
      "content": "<p>\ud83d\udcdd Improving Continual Relation Extraction by Distinguishing Analogous Semantics \ud83d\udcda</p><p>\"Introduces integrated training to enhance the performance on analogous relations and design memory-insensitive relation prototypes and memory augmentation for alleviating the forgetting problem while avoiding overfitting on replayed samples.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/nju-websoft/CEAR\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/nju-websoft/CEAR</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06620v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06620v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04091",
    "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
    "latest": "2023-05-13T09:45:03+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110336284626211643",
      "content": "<p>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</p><p>Improved performance on various reasoning tasks in zero/few shot </p><p><a href=\"https://arxiv.org/abs/2305.04091\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04091</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41598-021-97778-3",
    "title": "nature.com/articles/s41598-021...",
    "latest": "2023-05-13T09:35:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110360690848293622",
      "content": "<p>A Tunguska sized airburst destroyed Tall el-Hammam in the Jordan Valley (2021) - <a href=\"https://www.nature.com/articles/s41598-021-97778-3\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41598-021</span><span class=\"invisible\">-97778-3</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06557v1",
    "title": "https://arxiv.org/abs/2305.06557v1",
    "latest": "2023-05-13T09:09:23+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110360590015891803",
      "content": "<p>\ud83d\udcdd Long-Tailed Question Answering in an Open World \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Proposes a dynamic and multi-stage knowledge sharing and transfer approach, named DAMO-Anserini, to tackle Open Long-Tailed QA (OLTQA) problems in real-world scenarios.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/AlibabaResearch/DAMO-ConvAI\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/AlibabaResearch/DAM</span><span class=\"invisible\">O-ConvAI</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06557v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06557v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04241",
    "title": "Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens",
    "latest": "2023-05-13T07:44:59+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110336116421156322",
      "content": "<p>Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens</p><p>Proposes to significantly reduce the long attention cost by compressing the input into a fixed-size set of vectors at each layer.</p><p><a href=\"https://arxiv.org/abs/2305.04241\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04241</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06555v1",
    "title": "https://arxiv.org/abs/2305.06555v1",
    "latest": "2023-05-13T07:29:20+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110360196594182817",
      "content": "<p>\ud83d\udcdd Domain Incremental Lifelong Learning in an Open World \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Diana is a dynamic architecture-based lifelong learning model that tries to learn a sequence of tasks with a prompt-enhanced language model, and four types of hierarchically organized prompts are used: task level, instance level, unseen task, and prompt key vector.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/AlibabaResearch/DAMO-ConvAI\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/AlibabaResearch/DAM</span><span class=\"invisible\">O-ConvAI</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06555v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06555v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06424",
    "title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question",
    "latest": "2023-05-13T06:20:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110359924111790362",
      "content": "<p>Bot or Human? Detecting ChatGPT Imposters with a Single Question - <a href=\"https://arxiv.org/abs/2305.06424\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06424</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06545v1",
    "title": "GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark",
    "latest": "2023-05-13T06:09:19+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110359881919241970",
      "content": "<p>\ud83d\udcdd GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark \ud83d\udcda\ud83d\udc7e</p><p>\"Collects data from open-released geographic resources and introduce six natural language understanding tasks, including geographic textual similarity on recall, geographic textual similarity on rerank, geographic elements tagging, geographic composition analysis, geographic where what cut, and geographic entity alignment.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06545v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06545v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04757",
    "title": "Augmented Large Language Models with Parametric Knowledge Guiding",
    "latest": "2023-05-13T05:44:55+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110336104349372354",
      "content": "<p>Augmented Large Language Models with Parametric Knowledge Guiding</p><p>Enhances the performance of \"black-box\" LLMs on a range of long-tail and domain-specific downstream tasks requiring factual, tabular, medical, and multimodal knowledge.</p><p><a href=\"https://arxiv.org/abs/2305.04757\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04757</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/ukmbq/",
    "title": "http://osf.io/ukmbq/",
    "latest": "2023-05-13T05:33:35+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110359741440704406",
      "content": "<p>No Justice Without Sustainability: Taking the Climate and Environment Literally in Diversity, Equity, and Inclusion Work <a href=\"http://osf.io/ukmbq/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/ukmbq/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/e2zxr/",
    "title": "http://osf.io/e2zxr/",
    "latest": "2023-05-13T05:33:35+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110359741411445753",
      "content": "<p>Embracing Generational Labels: An Analysis of Self-Identification and Sociopolitical Alignment <a href=\"http://osf.io/e2zxr/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/e2zxr/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06535v1",
    "title": "https://arxiv.org/abs/2305.06535v1",
    "latest": "2023-05-13T05:29:20+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110359724703198175",
      "content": "<p>\ud83d\udcdd KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment \ud83d\udcda</p><p>\"The main idea is to maintain the distribution differences between the unlearned model and the model retrained from scratch without the forgotten instances (i,e, the knowledge gap).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/mjpost/sacrebleu\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/mjpost/sacrebleu</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06535v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06535v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/gqd9s/",
    "title": "http://osf.io/gqd9s/",
    "latest": "2023-05-13T05:25:27+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110359709464263284",
      "content": "<p>What the Purple Women of Oakmont Taught Me About Aging and Resilience: Qualitative Perspectives from Lesbian Women <a href=\"http://osf.io/gqd9s/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/gqd9s/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/w9rvu/",
    "title": "http://osf.io/w9rvu/",
    "latest": "2023-05-13T05:15:10+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110359669020196299",
      "content": "<p>A biopsychosocial analysis of Arthur's developmental status in the film \"Joker\" <a href=\"http://osf.io/w9rvu/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/w9rvu/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06522v1",
    "title": "https://arxiv.org/abs/2305.06522v1",
    "latest": "2023-05-13T03:29:21+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110359252915826691",
      "content": "<p>\ud83d\udcdd Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications \ud83d\udcda\ud83d\udc7e</p><p>\"Introduces RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve adversarial robustness of NLP systems.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Han8931/rsmi_nlp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Han8931/rsmi_nlp</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06522v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06522v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06434v1",
    "title": "https://arxiv.org/abs/2305.06434v1",
    "latest": "2023-05-13T01:49:19+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110358859557201119",
      "content": "<p>\ud83d\udcdd Word Grounded Graph Convolutional Network \ud83d\udcda\ud83e\udde0</p><p>\"The proposed inductive WGCN model could infer out-of-graph documents based on a document-independent word graph, to learn word representation with commonly-used word co-occurrences in corpora, but also incorporating extra global semantic dependency derived from inter-document relationships.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/Louis-udm/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/Louis-udm/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06434v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06434v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/10.1145/3594737",
    "title": "Data Statements: From Technical Concept to Community Practice | ACM Journal on Responsible Computing",
    "latest": "2023-05-13T01:34:43+00:00",
    "last_post": {
      "url": "https://dair-community.social/@emilymbender/110353864271022853",
      "content": "<p>Dataset documentation fans, please check out \"Data Statements: From Technical Concept to Community Practice\" (McMillan-Major, Bender &amp; Friedman 2023) -- reporting on how we took data statements v1 to v2 through learning with and from practitioners.</p><p><a href=\"https://dl.acm.org/doi/10.1145/3594737\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">dl.acm.org/doi/10.1145/3594737</span><span class=\"invisible\"></span></a></p><p><a href=\"https://dair-community.social/tags/nlp\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>nlp</span></a> <a href=\"https://dair-community.social/tags/DataDocumentation\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DataDocumentation</span></a> <a href=\"https://dair-community.social/tags/ethnlp\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ethnlp</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@rich",
        "display_name": "Rich Tong \u2764\ufe0f\ud83c\udf93"
      },
      {
        "url": "https://dair-community.social/@emilymbender",
        "display_name": "Emily M. Bender (she/her)"
      },
      {
        "url": "https://mastodon.social/@mmitchell_ai",
        "display_name": "mmitchell_ai"
      },
      {
        "url": "https://mamot.fr/@pluralistic",
        "display_name": "Cory Doctorow's linkblog"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07004v1",
    "title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
    "latest": "2023-05-13T00:17:40+00:00",
    "last_post": {
      "url": "https://techhub.social/@nic221/110358428975248685",
      "content": "<p>Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting <a href=\"https://arxiv.org/abs/2305.07004v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07004v1</span><span class=\"invisible\"></span></a> <a href=\"https://techhub.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://techhub.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06424v1",
    "title": "https://arxiv.org/abs/2305.06424v1",
    "latest": "2023-05-13T00:09:19+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110358466393214539",
      "content": "<p>\ud83d\udcdd Bot or Human? Detecting ChatGPT Imposters with a Single Question \ud83d\udcda</p><p>\"We target a single question scenario that can effectively differentiate human users from bots in a conversation, which has two categories of questions: those that are easy for humans but difficult for bots (e.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/hongwang600/FLAIR\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/hongwang600/FLAIR</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06424v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06424v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06404v1",
    "title": "LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM",
    "latest": "2023-05-12T23:09:18+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110358230336981481",
      "content": "<p>\ud83d\udcdd LACoS-BLOOM: Low-Rank Adaptation with Contrastive Objective on 8 Bits Siamese-Bloom \ud83d\udcda\ud83d\udc7e</p><p>\"LACoS-BLOOM is a sentence embedding model built on 8-bit Siamese-BLOOM and a scalable adapter (LoRA) with contrastive objective.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06404v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06404v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01532-w",
    "title": "Researchers who agree to manipulate citations are more likely to get their papers published",
    "latest": "2023-05-12T22:40:00+00:00",
    "last_post": {
      "url": "https://glammr.us/@ppival/110358115183504653",
      "content": "<p>Researchers who agree to manipulate citations are more likely to get their papers published <a href=\"https://www.nature.com/articles/d41586-023-01532-w\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01532-w</span></a></p><p>Yikes! <a href=\"https://glammr.us/tags/ResearcEthics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ResearcEthics</span></a> While the author says the data from 2012 are still relevant, I *hope* maybe editors are more ethical 10 years later?</p>"
    },
    "people": [
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      },
      {
        "url": "https://dair-community.social/@DrVeronikaCH",
        "display_name": "Veronika Cheplygina"
      },
      {
        "url": "https://fediscience.org/@petersuber",
        "display_name": "petersuber"
      },
      {
        "url": "https://glammr.us/@ppival",
        "display_name": "Paul R. Pival (he/him)"
      }
    ]
  },
  {
    "link": "https://doi.org/10.3998/jep.3398",
    "title": "https://doi.org/10.3998/jep.3398",
    "latest": "2023-05-12T21:13:47+00:00",
    "last_post": {
      "url": "https://mastodon.social/@ResearchOrgs/110356785230508815",
      "content": "<p>Terrific article on the marvelous COKI <a href=\"https://mastodon.social/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> dashboard in the latest issue of the Journal of Electronic Publishing. The dashboard 1) uses understandable <a href=\"https://mastodon.social/tags/OA\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OA</span></a> terms instead of colors, and 2) covers 189 countries and 7000+ orgs. Glad ROR was available to help them build this terrific tool! Article: <a href=\"https://doi.org/10.3998/jep.3398\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.3998/jep.3398</span><span class=\"invisible\"></span></a>  Dashboard: <a href=\"https://open.coki.ac/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">open.coki.ac/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://hcommons.social/@jcpeyssard",
        "display_name": "Jean-Christophe Peyssard"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06355v1",
    "title": "https://arxiv.org/abs/2305.06355v1",
    "latest": "2023-05-12T20:47:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110357673694185379",
      "content": "<p>\ud83d\udcdd VideoChat: Chat-Centric Video Understanding \ud83d\udd2d\ud83d\udcda</p><p>\"An end-to-end chat-centric video understanding system integrating video foundation models and large language models via a learnable neural interface, which excels in spatiotemporal reasoning, event localization, and causal relationship inference.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/OpenGVLab/Ask-Anything\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/OpenGVLab/Ask-Anyth</span><span class=\"invisible\">ing</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06355v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06355v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06349v1",
    "title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
    "latest": "2023-05-12T20:02:42+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110357496623620758",
      "content": "<p>\ud83d\udcdd RECKONING: Reasoning Through Dynamic Knowledge Encoding \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"A bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge, allowing them to then answer questions using the updated parameters (i.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06349v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06349v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.06500",
    "title": "http://arxiv.org/abs/2305.06500",
    "latest": "2023-05-12T19:33:03+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110357380007391154",
      "content": "<p>RT @LiJunnan0409@twitter.com</p><p>A new member in the BLIP family: \ud83d\udd25InstructBLIP\ud83d\udd25, a vision-language instruction tuning framework. InstructBLIP achieves SoTA zero-shot performance with various advantages over other multimodal models such as GPT-4!<br>Github: <a href=\"https://github.com/salesforce/LAVIS/tree/main/projects/instructblip\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/salesforce/LAVIS/tr</span><span class=\"invisible\">ee/main/projects/instructblip</span></a><br>Paper: <a href=\"http://arxiv.org/abs/2305.06500\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.06500</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/LiJunnan0409/status/1656821806593101827\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/LiJunnan0409/statu</span><span class=\"invisible\">s/1656821806593101827</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.06989.pdf",
    "title": "https://arxiv.org/pdf/2305.06989.pdf",
    "latest": "2023-05-12T19:24:56+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@pfau/110355415047131359",
      "content": "<p>Very pleased to share our latest paper, and the first by our student Tonny Lou (<span class=\"h-card\"><a href=\"https://mastodon.social/@tonnylou\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>tonnylou</span></a></span>), showing how the FermiNet can be used to study the Fermi gas, a model system for superconductors and superfluids!</p><p><a href=\"https://arxiv.org/pdf/2305.06989.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.06989.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@UlrichJunker",
        "display_name": "Ulrich Junker"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.12712",
    "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
    "latest": "2023-05-12T19:09:40+00:00",
    "last_post": {
      "url": "https://fediscience.org/@m8ta/110357250262283079",
      "content": "<p>Still amazed at the graphics abilities of Gpt4 given that it ... doesn't have vision / can't see what it's done.  This is all open-loop. </p><p><a href=\"https://arxiv.org/abs/2303.12712\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.12712</span><span class=\"invisible\"></span></a></p><p>Hard to beat the aggregate, cumulative, distributed - then - distilled intellectual output of the human race...!</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      },
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://sigmoid.social/@ceperez",
        "display_name": "Carlos E. Perez @IntuitMachine"
      },
      {
        "url": "https://social.skewed.de/@tiago",
        "display_name": "Tiago Peixoto"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06161",
    "title": "StarCoder: may the source be with you!",
    "latest": "2023-05-12T18:59:36+00:00",
    "last_post": {
      "url": "https://techhub.social/@lnsmith/110357237737895169",
      "content": "<p>A lot of folks try but this one gets a gold start for having a truly clever name for their paper:<br>StarCoder: may the source be with you!<br><a href=\"https://arxiv.org/abs/2305.06161\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06161</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07017",
    "title": "https://arxiv.org/abs/2305.07017",
    "latest": "2023-05-12T18:45:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110357191511080483",
      "content": "<p>RT @_akhaliq@twitter.com</p><p>An Inverse Scaling Law for CLIP Training</p><p>abs: <a href=\"https://arxiv.org/abs/2305.07017\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07017</span><span class=\"invisible\"></span></a> <br>paper page: <a href=\"https://huggingface.co/papers/2305.07017\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">huggingface.co/papers/2305.070</span><span class=\"invisible\">17</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/_akhaliq/status/1656908423278084096\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/_akhaliq/status/16</span><span class=\"invisible\">56908423278084096</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "http://osf.io/rwtzs/",
    "title": "http://osf.io/rwtzs/",
    "latest": "2023-05-12T18:41:37+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110357177762152223",
      "content": "<p>Can Generative AI Improve Social Science? <a href=\"http://osf.io/rwtzs/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/rwtzs/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06946",
    "title": "Big-PERCIVAL: Exploring the Native Use of 64-Bit Posit Arithmetic in Scientific Computing",
    "latest": "2023-05-12T18:30:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110357132301350155",
      "content": "<p>Exploring the Native Use of 64-Bit Posit Arithmetic in Scientific Computing - <a href=\"https://arxiv.org/abs/2305.06946\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06946</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.09542v1",
    "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
    "latest": "2023-05-12T18:21:12+00:00",
    "last_post": {
      "url": "https://techhub.social/@nic221/110237680239423849",
      "content": "<p>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent <a href=\"https://arxiv.org/abs/2304.09542v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.09542v1</span><span class=\"invisible\"></span></a> <a href=\"https://techhub.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://techhub.social/tags/search\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>search</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01569-x",
    "title": "Hammerhead sharks are first fish found to \u2018hold their breath\u2019",
    "latest": "2023-05-12T18:00:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110357014283087821",
      "content": "<p>Hammerhead sharks are first fish found to \u2018hold their breath\u2019 - <a href=\"https://www.nature.com/articles/d41586-023-01569-x\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01569-x</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06330v1",
    "title": "Korean Named Entity Recognition Based on Language-Specific Features",
    "latest": "2023-05-12T17:47:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110356965899445996",
      "content": "<p>\ud83d\udcdd Korean Named Entity Recognition Based on Language-Specific Features \ud83d\udcda</p><p>\"A morpheme-based annotation scheme that adopts the CoNLL-U format decomposes Korean words into morphemes and reduces the ambiguity of named entities in the original segmentation which may contain functional morphemes such as postpositions and particles.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06330v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06330v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06311v1",
    "title": "Automatic Evaluation of Attribution by Large Language Models",
    "latest": "2023-05-12T17:17:43+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110356847887572147",
      "content": "<p>\ud83d\udcdd Automatic Evaluation of Attribution by Large Language Models \ud83d\udcda</p><p>\"Investigates the automatic evaluation of attribution by LLMs, and find promising signals as well as remaining challenges for it using two approaches: prompting LLMs and fine-tuning smaller LMs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06311v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06311v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "http://osf.io/8vpwr/",
    "title": "http://osf.io/8vpwr/",
    "latest": "2023-05-12T17:17:34+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110356847284293772",
      "content": "<p>Regulatory Capture's Third Face of Power <a href=\"http://osf.io/8vpwr/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/8vpwr/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/pdf/10.1145/258549.258715",
    "title": "dl.acm.org/doi/pdf/10.1145/258...",
    "latest": "2023-05-12T16:51:54+00:00",
    "last_post": {
      "url": "https://hci.social/@andresmh/110356746341313390",
      "content": "<p><span class=\"h-card\"><a href=\"https://hci.social/@vitak\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>vitak</span></a></span> what about Ishii's Tangible Bits? <a href=\"https://dl.acm.org/doi/pdf/10.1145/258549.258715\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/258</span><span class=\"invisible\">549.258715</span></a> It has fun figures :)</p>"
    },
    "people": [
      {
        "url": "https://hci.social/@andresmh",
        "display_name": "Andr\u00e9s Monroy-Hern\u00e1ndez"
      }
    ]
  },
  {
    "link": "https://journals.sagepub.com/doi/10.1177/25152459221074654",
    "title": "journals.sagepub.com/doi/10.11...",
    "latest": "2023-05-12T16:31:00+00:00",
    "last_post": {
      "url": "https://tech.lgbt/@emilynordmann/110356664170282238",
      "content": "<p>I'm giving a talk &amp; workshop on our data viz paper at <a href=\"https://tech.lgbt/tags/aps23dc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>aps23dc</span></a>. If you use <a href=\"https://tech.lgbt/tags/rstats\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>rstats</span></a> for data viz, what's your number 1 reason why you would encourage researchers to adopt it?</p><p>And if you don't use R/another programming language for data viz (or quant data analysis in general), what's your number 1 barrier/reason for not doing so?</p><p><a href=\"https://journals.sagepub.com/doi/10.1177/25152459221074654\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.sagepub.com/doi/10.11</span><span class=\"invisible\">77/25152459221074654</span></a></p>"
    },
    "people": [
      {
        "url": "https://tech.lgbt/@emilynordmann",
        "display_name": ""
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-023-01521-z",
    "title": "Humans and algorithms work together \u2014 so study them together",
    "latest": "2023-05-12T16:26:13+00:00",
    "last_post": {
      "url": "https://greatjustice.net/@elplatt/110356555568903195",
      "content": "<p>New paper in Nature on human-algorithm science from <span class=\"h-card\"><a href=\"https://social.coop/@natematias\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>natematias</span></a></span>:</p><p><a href=\"https://www.nature.com/articles/d41586-023-01521-z\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-023</span><span class=\"invisible\">-01521-z</span></a></p><p>\"Humans and algorithms work together \u2014 so study them together.<br>Adaptive algorithms have been linked to terrorist attacks and beneficial social movements. Governing them requires new science on collective human\u2013algorithm behaviour.\"</p>"
    },
    "people": [
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      },
      {
        "url": "https://social.coop/@natematias",
        "display_name": "J. Nathan Matias \ud83e\udda3"
      },
      {
        "url": "https://mstdn.social/@kissane",
        "display_name": "Erin Kissane"
      },
      {
        "url": "https://mastodon.social/@markcmarino",
        "display_name": "Mark C Marino"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06294v1",
    "title": "https://arxiv.org/abs/2305.06294v1",
    "latest": "2023-05-12T16:17:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110356612013184908",
      "content": "<p>\ud83d\udcdd CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation \ud83d\udcda</p><p>\"Context-aware GAT incorporates global features of relevant knowledge graphs based on a context-enhanced knowledge aggregation process, which hierarchically applies graph knowledge aggregation on a connected subgraph in addition to contextual information to support commonsense dialogue generation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/StevenZHB/CADGE\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/StevenZHB/CADGE</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06294v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06294v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06161v1",
    "title": "StarCoder: may the source be with you!",
    "latest": "2023-05-12T09:47:42+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110355078333712779",
      "content": "<p>\ud83d\udcdd StarCoder: May the Source Be with You! \ud83d\udcda\ud83d\udc7e</p><p>\"StarCoder is a 155 billion parameter language model fine-tuned on 35 billion Python tokens from GitHub and other sources, using an infilling objective.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/PL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06161v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06161v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/fullHtml/10.1145/3543873.3585579",
    "title": "dl.acm.org/doi/fullHtml/10.114...",
    "latest": "2023-05-12T09:43:25+00:00",
    "last_post": {
      "url": "https://mas.to/@vrandecic/110334985014764119",
      "content": "<p>\"Wikidata: The Making Of\"</p><p>The history of <a href=\"https://mas.to/tags/wikidata\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>wikidata</span></a> by Markus Kr\u00f6tzsch, <span class=\"h-card\"><a href=\"https://mastodon.online/@nightrose\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>nightrose</span></a></span> , and me. From the inception of the idea to the project proposal and development, and the first ten years of <span class=\"h-card\"><a href=\"https://wikis.world/@wikidata\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>wikidata</span></a></span> </p><p>Video on YouTube with a recording of the presentation (not the actual presentation from The Web Conference, though):<br><a href=\"https://www.youtube.com/watch?v=P3-nklyrDx4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">youtube.com/watch?v=P3-nklyrDx</span><span class=\"invisible\">4</span></a></p><p>Paper as HTML:<br><a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3543873.3585579\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/fullHtml/10.114</span><span class=\"invisible\">5/3543873.3585579</span></a></p><p>Paper as PDF (Open access):<br><a href=\"https://dl.acm.org/doi/pdf/10.1145/3543873.3585579\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/354</span><span class=\"invisible\">3873.3585579</span></a></p>"
    },
    "people": [
      {
        "url": "https://mas.to/@vrandecic",
        "display_name": "Denny Vrande\u010di\u0107"
      },
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/pdf/10.1145/3543873.3585579",
    "title": "dl.acm.org/doi/pdf/10.1145/354...",
    "latest": "2023-05-12T09:43:25+00:00",
    "last_post": {
      "url": "https://mas.to/@vrandecic/110334985014764119",
      "content": "<p>\"Wikidata: The Making Of\"</p><p>The history of <a href=\"https://mas.to/tags/wikidata\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>wikidata</span></a> by Markus Kr\u00f6tzsch, <span class=\"h-card\"><a href=\"https://mastodon.online/@nightrose\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>nightrose</span></a></span> , and me. From the inception of the idea to the project proposal and development, and the first ten years of <span class=\"h-card\"><a href=\"https://wikis.world/@wikidata\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>wikidata</span></a></span> </p><p>Video on YouTube with a recording of the presentation (not the actual presentation from The Web Conference, though):<br><a href=\"https://www.youtube.com/watch?v=P3-nklyrDx4\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">youtube.com/watch?v=P3-nklyrDx</span><span class=\"invisible\">4</span></a></p><p>Paper as HTML:<br><a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3543873.3585579\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/fullHtml/10.114</span><span class=\"invisible\">5/3543873.3585579</span></a></p><p>Paper as PDF (Open access):<br><a href=\"https://dl.acm.org/doi/pdf/10.1145/3543873.3585579\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">dl.acm.org/doi/pdf/10.1145/354</span><span class=\"invisible\">3873.3585579</span></a></p>"
    },
    "people": [
      {
        "url": "https://mas.to/@vrandecic",
        "display_name": "Denny Vrande\u010di\u0107"
      },
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      }
    ]
  },
  {
    "link": "https://doi.org/10.5281/zenodo.3826444",
    "title": "https://doi.org/10.5281/zenodo.3826444",
    "latest": "2023-05-12T09:22:34+00:00",
    "last_post": {
      "url": "https://fediscience.org/@seanfobbe/110345978577533811",
      "content": "<p>\u26a0\ufe0f UPDATE with better OCR \u26a0\ufe0f</p><p>All judgments and opinions of the International Court of Justice (ICJ) from 1947 up to April 2023 are now available as a data set!</p><p>\u2705 Enhanced OCR<br>\u2705 <a href=\"https://fediscience.org/tags/PublicDomain\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PublicDomain</span></a><br>\u2705 <a href=\"https://fediscience.org/tags/RStats\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>RStats</span></a> Code<br>\u2705 PDF, TXT and CSV formats</p><p>Codebook: <a href=\"https://zenodo.org/record/7876286/files/CD-ICJ_2023-05-07_Codebook.pdf?download=1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">zenodo.org/record/7876286/file</span><span class=\"invisible\">s/CD-ICJ_2023-05-07_Codebook.pdf?download=1</span></a></p><p>JELS Paper: <a href=\"https://zenodo.org/record/6628885/files/Fobbe2022_JELS_Twin-Corpora-of-Decisions-for-the-International-Court-of-Justice.pdf?download=1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">zenodo.org/record/6628885/file</span><span class=\"invisible\">s/Fobbe2022_JELS_Twin-Corpora-of-Decisions-for-the-International-Court-of-Justice.pdf?download=1</span></a></p><p>All downloads: <a href=\"https://doi.org/10.5281/zenodo.3826444\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.5281/zenodo.3826444</span><span class=\"invisible\"></span></a></p><p><a href=\"https://fediscience.org/tags/OpenData\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenData</span></a> <a href=\"https://fediscience.org/tags/OpenSource\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenSource</span></a> <a href=\"https://fediscience.org/tags/Law\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Law</span></a> <a href=\"https://fediscience.org/tags/LawFedi\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LawFedi</span></a> <a href=\"https://fediscience.org/tags/ICJ\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ICJ</span></a> <a href=\"https://fediscience.org/tags/Histodons\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Histodons</span></a> <a href=\"https://fediscience.org/tags/DigitalHumanities\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>DigitalHumanities</span></a> <a href=\"https://fediscience.org/tags/Jessup\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Jessup</span></a> <a href=\"https://fediscience.org/tags/InternationalLaw\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>InternationalLaw</span></a> <a href=\"https://fediscience.org/tags/papers\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>papers</span></a> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/histodons\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>histodons</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/law\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>law</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/politicalscience\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>politicalscience</span></a></span> <span class=\"h-card\"><a href=\"https://a.gup.pe/u/rstats\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>rstats</span></a></span></p>"
    },
    "people": [
      {
        "url": "https://esq.social/@icymi_law",
        "display_name": "ICYMI (Law)"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch"
      },
      {
        "url": "https://colliderbias.net/@conjugateprior",
        "display_name": "Will Lowe"
      },
      {
        "url": "https://scholar.social/@hendrikerz",
        "display_name": "Hendrik Erz"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06159v1",
    "title": "https://arxiv.org/abs/2305.06159v1",
    "latest": "2023-05-12T08:47:43+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110354842503335274",
      "content": "<p>\ud83d\udcdd A Review of Vision-Language Models and Their Performance on the Hateful Memes Challenge \ud83d\udcda\ud83d\udc7e</p><p>\"Implemented using unimodal models for text and images separately using BERT and ResNet, respectively, and the outputs from these are concatenated together to create a late fusion model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/bzhao18/CS-7643-Project\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/bzhao18/CS-7643-Pro</span><span class=\"invisible\">ject</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06159v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06159v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04365",
    "title": "LatinCy: Synthetic Trained Pipelines for Latin NLP",
    "latest": "2023-05-12T08:44:29+00:00",
    "last_post": {
      "url": "https://fedihum.org/@stefan_hessbrueggen/110346343402465297",
      "content": "<p>New Latin model for spacy. Preprint: <a href=\"https://arxiv.org/abs/2305.04365\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04365</span><span class=\"invisible\"></span></a> <a href=\"https://fedihum.org/tags/nlp\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>nlp</span></a>  <a href=\"https://fedihum.org/tags/dh\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dh</span></a> <a href=\"https://fedihum.org/tags/digiclassics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>digiclassics</span></a></p>"
    },
    "people": [
      {
        "url": "https://fedihum.org/@stefan_hessbrueggen",
        "display_name": "@frueheneuzeit"
      },
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06156v1",
    "title": "The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation",
    "latest": "2023-05-12T07:47:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110354606641911158",
      "content": "<p>\ud83d\udcdd The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation \ud83d\udcda\ud83d\udc7e</p><p>\"Provides 40 million code-text pairs across 10 popular programming languages, thorough cleaning for 10+ prevalent issues, and various levels of code-text pairings.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/PL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>PL</span></a> <a href=\"https://creative.ai/tags/SE\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SE</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06156v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06156v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://academic.oup.com/mnras/advance-article/doi/10.1093/mnras/stad1000/7115325",
    "title": "Multiwavelength observations of the extraordinary accretion event AT2021lwx",
    "latest": "2023-05-12T07:25:46+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@gregeganSF/110354520256666701",
      "content": "<p>\u201cWe present observations from X-ray to mid-infrared wavelengths of the most energetic non-quasar transient ever observed \u2026 A plausible scenario is the accretion of a giant molecular cloud by a dormant black hole of 10^8\u201310^9 solar masses.\u201d</p><p><a href=\"https://academic.oup.com/mnras/advance-article/doi/10.1093/mnras/stad1000/7115325\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">academic.oup.com/mnras/advance</span><span class=\"invisible\">-article/doi/10.1093/mnras/stad1000/7115325</span></a></p>"
    },
    "people": [
      {
        "url": "https://mathstodon.xyz/@gregeganSF",
        "display_name": "Greg Egan"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06155v1",
    "title": "Leveraging Synthetic Targets for Machine Translation",
    "latest": "2023-05-12T07:02:43+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110354429619865076",
      "content": "<p>\ud83d\udcdd Leveraging Synthetic Targets for Machine Translation \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Using a large pre-trained model, the source sentence is translated into synthetic target data, which is then used to fine-tune the model instead of the original target sentence.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06155v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06155v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06154v1",
    "title": "https://arxiv.org/abs/2305.06154v1",
    "latest": "2023-05-12T06:17:43+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110354252679466379",
      "content": "<p>\ud83d\udcdd Alleviating Over-Smoothing for Unsupervised Sentence Representation \ud83d\udcda</p><p>\"SSCL samples negatives from PLMs intermediate layers, improving the quality of the sentence representation by solving the over-smoothing problem with contrastive learning and self-supervised contrastive learning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/nuochenpku/SSCL\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/nuochenpku/SSCL</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06154v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06154v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.3998/jep.3332",
    "title": "https://doi.org/10.3998/jep.3332",
    "latest": "2023-05-12T05:56:47+00:00",
    "last_post": {
      "url": "https://openbiblio.social/@ferli90/110353940018970638",
      "content": "<p>Frankl, J., (2023) \u201cTowards an Author-Centered Open Access Monograph Program: Understanding Open Access Cultures in Scholarly Publishing\u201d, The Journal of Electronic Publishing 26(1). <a href=\"https://doi.org/10.3998/jep.3332\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.3998/jep.3332</span><span class=\"invisible\"></span></a> <a href=\"https://openbiblio.social/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> <a href=\"https://openbiblio.social/tags/HSS\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>HSS</span></a>, <a href=\"https://openbiblio.social/tags/OpenAccessCultures\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccessCultures</span></a> <a href=\"https://openbiblio.social/tags/OpenAccessAuthorAttitudes\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccessAuthorAttitudes</span></a></p>"
    },
    "people": [
      {
        "url": "https://fedihum.org/@jcls",
        "display_name": "JCLS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06152v1",
    "title": "Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge",
    "latest": "2023-05-12T05:17:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110354016802786194",
      "content": "<p>\ud83d\udcdd Structure-Clip: Enhance Multi-Modal Language Representations with Structure Knowledge \ud83d\udcda\ud83d\udc7e</p><p>\"Structure-CLIP uses scene graphs to pay more attention to the detailed semantic learning in the text and fully explore structured knowledge between fine-grained semantics, it utilizes the knowledge-enhanced framework to make full use of representations of structured knowledge.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/MM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MM</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06152v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06152v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1080/21670811.2023.2202873",
    "title": "doi.org/10.1080/21670811.2023....",
    "latest": "2023-05-12T05:10:39+00:00",
    "last_post": {
      "url": "https://hci.social/@rodzam/110351427760625576",
      "content": "<p>Out now: Open-Source Repositories as Trust-Building Journalism Infrastructure (<a href=\"https://doi.org/10.1080/21670811.2023.2202873\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1080/21670811.2023.</span><span class=\"invisible\">2202873</span></a>)</p><p>In this study of 124 predominantly U.S.-based news outlets over a decade, I found that although the use of GitHub isn't widespread, several outlets do actively use it. Unfortunately, the repositories they publish tend to receive little engagement, and there has actually been a decline in use and collaboration in recent years. (The study only looks at repos in organizational accounts, though.)</p>"
    },
    "people": [
      {
        "url": "https://hci.social/@andresmh",
        "display_name": "Andr\u00e9s Monroy-Hern\u00e1ndez"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06147v1",
    "title": "https://arxiv.org/abs/2305.06147v1",
    "latest": "2023-05-12T04:17:41+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110353780643359026",
      "content": "<p>\ud83d\udcdd CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia \ud83d\udcda</p><p>\"We harness the language generation capabilities of ChatGPT to regenerate its queries to make it suitable for query-focused abstractive summarization task in debatepedia dataset [1].\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/PrekshaNema25/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/PrekshaNema25/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.06147v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06147v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05994v1",
    "title": "https://arxiv.org/abs/2305.05994v1",
    "latest": "2023-05-12T03:32:40+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110353603675084515",
      "content": "<p>\ud83d\udcdd ANALOGYKB: Unlocking Analogical Reasoning of Language Models with a Million-Scale Knowledge Base \ud83d\udcda\ud83d\udc7e</p><p>\"ANALOGYKB is an analogy knowledge base derived from existing knowledge graphs (KGs) using large language models (LMs) followed by minor human efforts for data quality control.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/siyuyuan/analogykb\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/siyuyuan/analogykb</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05994v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05994v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05976v1",
    "title": "https://arxiv.org/abs/2305.05976v1",
    "latest": "2023-05-12T02:32:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110353367986565625",
      "content": "<p>\ud83d\udcdd Say What You Mean! Large Language Models Speak Too Positively About Negative Commonsense Knowledge \ud83d\udcda</p><p>\"Designs a constrained keywords-to-sentence generation task and a boolean question task for commonsense probing in large language models (LLMs), such as GPT-3 and GPT-2.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/jiangjiechen/uncommongen\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/jiangjiechen/uncomm</span><span class=\"invisible\">ongen</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05976v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05976v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.04388.pdf",
    "title": "https://arxiv.org/pdf/2305.04388.pdf",
    "latest": "2023-05-12T02:04:13+00:00",
    "last_post": {
      "url": "https://mastodon.social/@wirepair/110353174052349108",
      "content": "<p>&gt; In this paper, we demonstrate that CoT explanations can be plausible yet systematically unfaithful:<br>Models\u2019 behavior can be predictably influenced by<br>biasing features in their inputs which they fail to<br>mention in their CoT explanations. In this regard,<br>LLMs do not always say what they think.</p><p>def worth the read for those trying to test, write prompts, or benchmark LLMs <a href=\"https://arxiv.org/pdf/2305.04388.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.04388.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05968v1",
    "title": "Investigating Forgetting in Pre-Trained Representations Through Continual Learning",
    "latest": "2023-05-12T01:32:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110353132048854983",
      "content": "<p>\ud83d\udcdd Investigating Forgetting in Pre-Trained Representations Through Continual Learning \ud83d\udcda</p><p>\"Designs three metrics to measure the evolution of general knowledge in continual learning: overall generality destruction (GD), syntactic knowledge forgetting (SynF), and semantic knowledge forgetting (SemF).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05968v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05968v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06500",
    "title": "https://arxiv.org/abs/2305.06500",
    "latest": "2023-05-12T01:24:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110353098221995840",
      "content": "<p>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</p><p>- Systematic and comprehensive study on vision-language instruction tuning on BLIP-2<br>- SotA (90.7% acc.) on ScienceQA IMG</p><p>repo: <a href=\"https://github.com/salesforce/LAVIS/tree/main/projects/instructblip\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/salesforce/LAVIS/tr</span><span class=\"invisible\">ee/main/projects/instructblip</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.06500\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06500</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03111",
    "title": "https://arxiv.org/abs/2305.03111",
    "latest": "2023-05-12T01:20:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110353083124565878",
      "content": "<p>RT @huybery@twitter.com</p><p>\ud83d\udca1Can LLM Already Serve as A Database Interface? <br>Please check out our latest benchmark BIRD:<br>A Big Bench for Large-scale Database Grounded Text-to-SQLs.<br>We just released the train and dev set. Pls enjoy it ! \ud83d\ude80</p><p>Leaderboard: <a href=\"https://bird-bench.github.io/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">bird-bench.github.io/</span><span class=\"invisible\"></span></a><br>Paper: <a href=\"https://arxiv.org/abs/2305.03111\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03111</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/huybery/status/1656644096696655872\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/huybery/status/165</span><span class=\"invisible\">6644096696655872</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2305.04388",
    "title": "http://arxiv.org/abs/2305.04388",
    "latest": "2023-05-12T01:18:59+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110353077973165153",
      "content": "<p>RT @_jasonwei@twitter.com</p><p>Really cool paper studying the faithfulness of chain-of-thought (CoT): <a href=\"http://arxiv.org/abs/2305.04388\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2305.04388</span><span class=\"invisible\"></span></a></p><p>The paper uses a biased prompt to try to mislead the model. For example, all few-shot exemplars could be answer (A), or they could add a suffix such as \"I think the answer is &lt;X&gt; but\u2026 <a href=\"https://twitter.com/i/web/status/1656080743343480832\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/i/web/status/16560</span><span class=\"invisible\">80743343480832</span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/_jasonwei/status/1656080743343480832\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/_jasonwei/status/1</span><span class=\"invisible\">656080743343480832</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06983",
    "title": "Active Retrieval Augmented Generation",
    "latest": "2023-05-12T01:01:04+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110353007563498020",
      "content": "<p>Active Retrieval Augmented Generation</p><p><a href=\"https://arxiv.org/abs/2305.06983\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06983</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06355",
    "title": "https://arxiv.org/abs/2305.06355",
    "latest": "2023-05-12T01:00:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110353003899019062",
      "content": "<p>RT @likunchang1998@twitter.com</p><p>We have released 11K video instruction data for spatiotemporal reasoning \ud83c\udf89.<br>With these data, our project \"VideoChat\ud83e\udd9c: Chat-Centric Video Understanding\" has built an interesting chatbot\ud83e\udd16 for both image and video.<br> Project: <a href=\"https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/OpenGVLab/Ask-Anyth</span><span class=\"invisible\">ing/tree/main/video_chat</span></a><br>Paper: <a href=\"https://arxiv.org/abs/2305.06355\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06355</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/likunchang1998/status/1656551897506119680\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/likunchang1998/sta</span><span class=\"invisible\">tus/1656551897506119680</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.07015",
    "title": "https://arxiv.org/abs/2305.07015",
    "latest": "2023-05-12T00:59:08+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110352999911769030",
      "content": "<p>Exploiting Diffusion Prior for Real-World Image Super-Resolution</p><p>Presents a novel approach to leverage Stable Diffusion for super-resolution, which establishes a new SotA.</p><p>proj: <a href=\"https://iceclear.github.io/projects/stablesr/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">iceclear.github.io/projects/st</span><span class=\"invisible\">ablesr/</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.07015\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.07015</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05948v1",
    "title": "Multi-Path Transformer is Better: A Case Study on Neural Machine Translation",
    "latest": "2023-05-12T00:17:43+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110352837092875838",
      "content": "<p>\ud83d\udcdd Multi-Path Transformer Is Better: A Case Study on Neural Machine Translation \ud83d\udcda\ud83d\udc7e</p><p>\"By adding three additional operations to the multi-path Transformer model: a normalization at the end of each path, a cheap operation to produce more features, and a learnable weighted mechanism to fuse all features flexibly.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05948v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05948v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05945v1",
    "title": "https://arxiv.org/abs/2305.05945v1",
    "latest": "2023-05-11T23:32:43+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110352660149617206",
      "content": "<p>\ud83d\udcdd Adapter-Tst: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer \ud83d\udcda</p><p>\"AdapterTST is a framework that can be used to adapt a pre-trained language model for text style transfer by utilizing multiple attribute-specific adapters without modifying the original parameters of the language model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/luofuli/DualRL\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/luofuli/DualRL</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05945v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05945v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05940v1",
    "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
    "latest": "2023-05-11T23:29:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110352424040730688",
      "content": "<p>\ud83d\udcdd Multilingual LLMs Are Better Cross-Lingual in-Context Learners with Alignment \ud83d\udcda</p><p>\"Proposes a novel prompt construction strategy -- Cross-lingual In-context Source-Target Alignment (X-InSTA) that is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05940v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05940v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@ljvmiranda",
        "display_name": "Lj V. Miranda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2009.07936",
    "title": "How to marry a star: probabilistic constraints for meaning in context",
    "latest": "2023-05-11T21:50:27+00:00",
    "last_post": {
      "url": "https://mastodon.social/@BDehbozorgi83/110352257732377038",
      "content": "<p>An outstanding paper (pre-print) on <a href=\"https://mastodon.social/tags/Computational\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Computational</span></a> <a href=\"https://mastodon.social/tags/Semantics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Semantics</span></a>, by Profs. Katrin Erk &amp; <br><span class=\"h-card\"><a href=\"https://fosstodon.org/@minimalparts\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>minimalparts</span></a></span>! H/t to both of the venerable scholars!!!!</p><p><a href=\"https://arxiv.org/abs/2009.07936\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2009.07936</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.social/tags/linguistics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>linguistics</span></a> <a href=\"https://mastodon.social/tags/ai\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ai</span></a> <a href=\"https://mastodon.social/tags/computational_semantics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computational_semantics</span></a> <a href=\"https://mastodon.social/tags/formal_pragmatics\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>formal_pragmatics</span></a> <a href=\"https://mastodon.social/tags/automated_reasoning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>automated_reasoning</span></a> <a href=\"https://mastodon.social/tags/preprints\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>preprints</span></a> </p><p>@arxiv</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05936v1",
    "title": "Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering",
    "latest": "2023-05-11T21:47:44+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110352247297517094",
      "content": "<p>\ud83d\udcdd Multi-Hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering \ud83d\udcda</p><p>\"Proposes a novel multi-hop commonsense knowledge injection framework that explore multi-hop reasoning paradigm in KGs to inject multi-hop commonsense knowledge into a model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05936v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05936v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05351",
    "title": "https://doi.org/10.21105/joss.05351",
    "latest": "2023-05-11T20:51:58+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110348893420202308",
      "content": "<p>Just published in JOSS: 'jsPsych: Enabling an Open-Source Collaborative Ecosystem of Behavioral Experiments' <a href=\"https://doi.org/10.21105/joss.05351\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05351</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      },
      {
        "url": "https://piaille.fr/@paulanomalie",
        "display_name": "Paul"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05921v1",
    "title": "Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification",
    "latest": "2023-05-11T20:02:43+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110351834407181547",
      "content": "<p>\ud83d\udcdd Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification \ud83d\udcda</p><p>\"Given a commonsense claim, Decker first retrieves a set of relevant facts from structured knowledge bases and relevant sentences from unstructured knowledge sources, and then leverages graph neural network and pre-trained model to encode heterogeneous knowledge, respectively.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05921v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05921v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.00118.pdf",
    "title": "https://arxiv.org/pdf/2305.00118.pdf",
    "latest": "2023-05-11T18:50:50+00:00",
    "last_post": {
      "url": "https://techhub.social/@dwaynephillips/110304863911160899",
      "content": "<p>These researchers reverse engineered ChatGPT to lean what books the software \"memorized.\" Alice's Adventures in Wonderland tops the list.<br><a href=\"https://arxiv.org/pdf/2305.00118.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.00118.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008469",
    "title": "Ten quick tips for making things findable",
    "latest": "2023-05-11T18:08:33+00:00",
    "last_post": {
      "url": "https://mastodon.social/@gvwilson/110351385448454475",
      "content": "<p><span class=\"h-card\"><a href=\"https://scholar.social/@hendrikerz\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>hendrikerz</span></a></span> <a href=\"https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008469\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/ploscompbiol</span><span class=\"invisible\">/article?id=10.1371/journal.pcbi.1008469</span></a> might have some references that would be useful (disclaimer: I was a co-author, but most of the content came from the two library scientists, both of whom would probably be very happy to talk with you)</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05754v1",
    "title": "When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution",
    "latest": "2023-05-11T18:02:41+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110351362399012050",
      "content": "<p>\ud83d\udcdd When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Proposes a classification model to predict whether the builder should ask for clarification based on the current world state and dialogue history, as well as a ranking model to rank the relevant clarification questions from a pool of candidates.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05754v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05754v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05920",
    "title": "Fast Distributed Inference Serving for Large Language Models",
    "latest": "2023-05-11T17:20:55+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110351198154855279",
      "content": "<p>Fast Distributed Inference Serving for Large Language Models</p><p>FastServe improves the average and tail job completion time by up to 5.1x and 6.4x, respectively, compared to the SotA solution Orca.</p><p><a href=\"https://arxiv.org/abs/2305.05920\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05920</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@nezubn",
        "display_name": "Ankit Sharma"
      },
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.06300",
    "title": "Evaluating Embedding APIs for Information Retrieval",
    "latest": "2023-05-11T17:20:39+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110351197095302154",
      "content": "<p>Evaluating Embedding APIs for Information Retrieval</p><p><a href=\"https://arxiv.org/abs/2305.06300\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.06300</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2303.10130",
    "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
    "latest": "2023-05-11T16:49:10+00:00",
    "last_post": {
      "url": "https://kolektiva.social/@danmcquillan/110350852304375752",
      "content": "<p>please point me to critical commentary (threads \ud83e\uddf5, blogs or articles) on this paper about GPT &amp; jobs:<br>*'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models' <a href=\"https://arxiv.org/abs/2303.10130\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2303.10130</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://data-folks.masto.host/@healthstatsdude",
        "display_name": "Tired Old Health Stats Dude"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05496v1",
    "title": "https://arxiv.org/abs/2305.05496v1",
    "latest": "2023-05-11T09:22:41+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110349317640584984",
      "content": "<p>\ud83d\udcdd Exploiting Pseudo Image Captions for Multimodal Summarization \ud83d\udcda</p><p>\"Proposes a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize mutual information between an image/text anchor and its negative texts/images instead of improperly minimizing it.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/sitaProject/SITA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/sitaProject/SITA</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05496v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05496v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05200",
    "title": "https://doi.org/10.21105/joss.05200",
    "latest": "2023-05-11T08:31:11+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110349115181678179",
      "content": "<p>Just published in JOSS: 'fRAT: an interactive, Python-based tool for region-of-interest summaries of functional imaging data' <a href=\"https://doi.org/10.21105/joss.05200\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05200</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.08073",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "latest": "2023-05-11T08:20:32+00:00",
    "last_post": {
      "url": "https://terere.social/@pabloacastillo/110339506124320414",
      "content": "<p>Constitutional AI: Harmlessness from AI Feedback</p><p>We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. <br>As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them.<br>These methods make it possible to control AI behavior more precisely and with far fewer human labels. </p><p><a href=\"https://arxiv.org/abs/2212.08073\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2212.08073</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://creative.ai/@arxiv",
        "display_name": "arXiv Highlights AI/ML \ud83d\udcdd\ud83e\udd16"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05471v1",
    "title": "https://arxiv.org/abs/2305.05471v1",
    "latest": "2023-05-11T07:37:40+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110348904752876047",
      "content": "<p>\ud83d\udcdd Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good \ud83d\udcda</p><p>\"Builds three datasets with a large number of NLP4SG papers, and each with a different task: NLP4SGPAPERS-ID, NLP4SGPAPERS-TASK and NLP4SGPAPERS-SDG.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/feradauto/nlp4sg\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/feradauto/nlp4sg</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05471v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05471v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05461v1",
    "title": "What is the best recipe for character-level encoder-only modelling?",
    "latest": "2023-05-11T06:22:37+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110348609613170525",
      "content": "<p>\ud83d\udcdd What Is the Best Recipe for Character-Level Encoder-Only Modelling? \ud83d\udcda</p><p>\"CharacterBERT uses the BERT architecture and trains it at the character level, using a masked language model objective and a next-character prediction objective on unsegmented text.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05461v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05461v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.01393",
    "title": "Every Author as First Author",
    "latest": "2023-05-11T05:24:47+00:00",
    "last_post": {
      "url": "https://chaos.social/@blinry/110339449766097832",
      "content": "<p>Finally, a solution to the unfairness of authorship ordering in scientific papers! \ud83d\ude02</p><p>\"Every Author as First Author\"</p><p><a href=\"https://arxiv.org/abs/2304.01393\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.01393</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://social.skewed.de/@tiago",
        "display_name": "Tiago Peixoto"
      },
      {
        "url": "https://fediscience.org/@bendfulcher",
        "display_name": "bendfulcher"
      },
      {
        "url": "https://fediscience.org/@dustinstoltz",
        "display_name": "D\u1d1cs\u1d1b\u026a\u0274 S\u1d1b\u1d0f\u029f\u1d1b\u1d22 :tardis:"
      },
      {
        "url": "https://dair-community.social/@DrVeronikaCH",
        "display_name": "Veronika Cheplygina"
      },
      {
        "url": "https://octodon.social/@hmason",
        "display_name": "Hilary"
      },
      {
        "url": "https://mathstodon.xyz/@csk",
        "display_name": "Craig S. Kaplan"
      },
      {
        "url": "https://vis.social/@mcnutt",
        "display_name": "Andrew McNutt"
      },
      {
        "url": "https://toot.aquilenet.fr/@rougier",
        "display_name": "Nicolas P. Rougier"
      },
      {
        "url": "https://mastodon.social/@freakonometrics",
        "display_name": "Arthur Charpentier"
      },
      {
        "url": "https://sigmoid.social/@osma",
        "display_name": "Osma Suominen"
      },
      {
        "url": "https://sigmoid.social/@BenjaminHan",
        "display_name": "Benjamin Han"
      },
      {
        "url": "https://friend.camp/@aparrish",
        "display_name": "allison"
      },
      {
        "url": "https://octodon.social/@craignicol",
        "display_name": "Craig Nicol"
      },
      {
        "url": "https://emacs.ch/@stefanv",
        "display_name": "St\u00e9fan van der Walt"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05432v1",
    "title": "WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset",
    "latest": "2023-05-11T05:07:38+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110348314786871781",
      "content": "<p>\ud83d\udcdd WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset \ud83d\udcda\ud83d\udd2d</p><p>\"WikiWeb2M comprises over 2 million webpages from English Wikipedia, each paired with its images, HTML source, structured data, and metadata (e.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05432v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05432v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05390v1",
    "title": "COKE: A Cognitive Knowledge Graph for Machine Theory of Mind",
    "latest": "2023-05-11T04:07:40+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110348078985808241",
      "content": "<p>\ud83d\udcdd COKE: A Cognitive Knowledge Graph for Machine Theory of Mind \ud83d\udcda</p><p>\"COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05390v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05390v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03353",
    "title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic",
    "latest": "2023-05-11T03:46:20+00:00",
    "last_post": {
      "url": "https://mastodon.social/@compsci_discussions/110347995119063776",
      "content": "<p>[R] MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic</p><p><a href=\"https://arxiv.org/abs/2305.03353\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03353</span><span class=\"invisible\"></span></a></p><p>Discussions: <a href=\"https://discu.eu/q/https://arxiv.org/abs/2305.03353\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">discu.eu/q/https://arxiv.org/a</span><span class=\"invisible\">bs/2305.03353</span></a></p><p><a href=\"https://mastodon.social/tags/compsci\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>compsci</span></a> <a href=\"https://mastodon.social/tags/machinelearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>machinelearning</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05378v1",
    "title": "PLM-GNN: A Webpage Classification Method based on Joint Pre-trained Language Model and Graph Neural Network",
    "latest": "2023-05-11T02:52:39+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110347783965581365",
      "content": "<p>\ud83d\udcdd PLM-GNN: A Webpage Classification Method Based on Joint Pre-Trained Language Model and Graph Neural Network \ud83d\udcda</p><p>\"Based on the joint encoding of text and HTML DOM trees in the web pages, it performs well on the KI-04 and SWDE datasets and on practical dataset AHS for the project of scholar's homepage crawling.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05378v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05378v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05334v1",
    "title": "ArgU: A Controllable Factual Argument Generator",
    "latest": "2023-05-11T02:22:42+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110347666208855519",
      "content": "<p>\ud83d\udcdd ArgU: A Controllable Factual Argument Generator \ud83d\udcda</p><p>\"ArgU is a neural argument generator capable of producing factual arguments from input facts and real-world concepts that can be explicitly controlled for stance and argument structure using Walton's argument scheme-based control codes.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05334v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05334v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://dl.acm.org/doi/10.1145/3587157",
    "title": "What\u2019s (not) Working in Programmer User Studies? | ACM Transactions on Software Engineering and Methodology",
    "latest": "2023-05-11T01:38:17+00:00",
    "last_post": {
      "url": "https://hci.social/@infinimatt/110347248958582807",
      "content": "<p><span class=\"h-card\"><a href=\"https://hci.social/@heades\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>heades</span></a></span> Check out our just accepted ToSEM article, \u201cWhat\u2019s (not) Working in Programmer User Studies.\u201d </p><p>Page 9 shares a bunch of strategies under Barrier 10 that researchers are applying in their own recruiting efforts \u2014 many may apply to your own study as well.  </p><p>Hope this article is useful to you and your students! <a href=\"https://dl.acm.org/doi/10.1145/3587157\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">dl.acm.org/doi/10.1145/3587157</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://hci.social/@eglassman",
        "display_name": "Elena Glassman"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05311v1",
    "title": "https://arxiv.org/abs/2305.05311v1",
    "latest": "2023-05-11T01:37:38+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110347489021408554",
      "content": "<p>\ud83d\udcdd Structured Sentiment Analysis as Transition-Based Dependency Parsing \ud83d\udcda\ud83d\udc7e</p><p>\"A transition-based model that processes the input text in a left-to-right pass, incrementally generating the graph structure containing all identified opinions with a backbone of a Pointer Network architecture.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/jerbarnes/sentiment\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/jerbarnes/sentiment</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05311v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05311v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.17613/syv8-cp10",
    "title": "https://doi.org/10.17613/syv8-cp10",
    "latest": "2023-05-11T00:27:54+00:00",
    "last_post": {
      "url": "https://neuromatch.social/@jonny/110346670913611962",
      "content": "<p>Glad to formally release my latest work - Surveillance Graphs: Vulgarity and Cloud Orthodoxy in Linked Data Infrastructures. </p><p>web: <a href=\"https://jon-e.net/surveillance-graphs\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">jon-e.net/surveillance-graphs</span><span class=\"invisible\"></span></a><br>hcommons: <a href=\"https://doi.org/10.17613/syv8-cp10\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.17613/syv8-cp10</span><span class=\"invisible\"></span></a></p><p>A bit of an overview and then I'll get into some of the more specific arguments in a thread:</p><p>This piece is in three parts: </p><p>First I trace the mutation of the liberatory ambitions of the <a href=\"https://neuromatch.social/tags/SemanticWeb\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SemanticWeb</span></a> into <a href=\"https://neuromatch.social/tags/KnowledgeGraphs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>KnowledgeGraphs</span></a>, an underappreciated component in the architecture of <a href=\"https://neuromatch.social/tags/SurveillanceCapitalism\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SurveillanceCapitalism</span></a>. This mutation plays out against the backdrop of the broader platform capture of the web, rendering us as consumer-users of information services rather than empowered people communicating over informational protocols. </p><p>I then show how this platform logic influences two contemporary public information infrastructure projects: the NIH's Biomedical Data Translator and the NSF's Open Knowledge Network. I argue that projects like these, while well intentioned, demonstrate the fundamental limitations of platformatized public infrastructure and create new capacities for harm by their enmeshment in and inevitable capture by information conglomerates. The dream of a seamless \"knowledge graph of everything\" is unlikely to deliver on the utopian promises made by techno-solutionists, but they do create new opportunities for algorithmic oppression -- automated conversion therapy, predictive policing, abuse of bureacracy in \"smart cities,\" etc. Given the framing of corporate knowledge graphs, these projects are poised to create facilitating technologies (that the info conglomerates write about needing themselves) for a new kind of interoperable corporate data infrastructure, where a gradient of public to private information is traded between \"open\" and quasi-proprietary knowledge graphs to power derivative platforms and services. </p><p>When approaching \"AI\" from the perspective of the semantic web and knowledge graphs, it becomes apparent that the new generation of <a href=\"https://neuromatch.social/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a> are intended to serve as interfaces to knowledge graphs. These \"augmented language models\" are joint systems that combine a language model as a means of interacting with some underlying knowledge graph, integrated in multiple places in the computing ecosystem: eg. mobile apps, assistants, search, and enterprise platforms. I concretize and extend prior criticism about the capacity for LLMs to concentrate power by capturing access to information in increasingly isolated platforms and expand surveillance by creating the demand for extended personalized data graphs across multiple systems from home surveillance to your workplace, medical, and governmental data.</p><p>I pose Vulgar Linked Data as an alternative to the infrastructural pattern I call the Cloud Orthodoxy: rather than platforms operated by an informational priesthood, reorienting our public infrastructure efforts to support vernacular expression across heterogeneous <a href=\"https://neuromatch.social/tags/p2p\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>p2p</span></a> mediums. This piece extends a prior work of mine: <a href=\"https://jon-e.net/infrastructure\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">Decentralized Infrastructure for (Neuro)science)</a> which has more complete draft of what that might look like. </p><p>(I don't think you can pre-write threads on masto, so i'll post some thoughts as I write them under this) /1</p><p><a href=\"https://neuromatch.social/tags/SurveillanceGraphs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SurveillanceGraphs</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.thi.ng/@toxi",
        "display_name": "Karsten Schmidt"
      },
      {
        "url": "https://hachyderm.io/@schlink",
        "display_name": "Sam Schlinkert"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05290v1",
    "title": "Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue",
    "latest": "2023-05-11T00:07:40+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110347135229120518",
      "content": "<p>\ud83d\udcdd Dialogue Planning via Brownian Bridge Stochastic Process for Goal-Directed Proactive Dialogue \ud83d\udcda\ud83e\udde0</p><p>\"Proposes a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths, where the coherence is defined using a Brownian bridge process that allows us to incorporate user feedback flexibly in dialogue planning.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05290v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05290v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05280v1",
    "title": "https://arxiv.org/abs/2305.05280v1",
    "latest": "2023-05-10T23:37:40+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110347017265390085",
      "content": "<p>\ud83d\udcdd VCSUM: A Versatile Chinese Meeting Summarization Dataset \ud83d\udcda\ud83d\udc7e</p><p>\"VCSum consists of 239 real-life meetings with a total duration of over 230 hours, annotated with topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/hahahawu/VCSum\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/hahahawu/VCSum</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05280v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05280v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.04861",
    "title": "https://doi.org/10.21105/joss.04861",
    "latest": "2023-05-10T21:56:35+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110346619835474823",
      "content": "<p>Just published in JOSS: 'SPyCi-PDB: A modular command-line interface for back-calculating experimental datatypes of protein structures.' <a href=\"https://doi.org/10.21105/joss.04861\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.04861</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05227",
    "title": "https://doi.org/10.21105/joss.05227",
    "latest": "2023-05-10T21:48:50+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110346589371941708",
      "content": "<p>Just published in JOSS: 'mutyper: assigning and summarizing mutation types for analyzing germline mutation spectra' <a href=\"https://doi.org/10.21105/joss.05227\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05227</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@joss",
        "display_name": "JOSS"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.14787.pdf",
    "title": "https://arxiv.org/pdf/2304.14787.pdf",
    "latest": "2023-05-10T21:22:45+00:00",
    "last_post": {
      "url": "https://infosec.exchange/@juanan/110346204106706084",
      "content": "<p>A Network Perspective on the Influence of Code Review Bots on the Structure of Developer Collaborations<br><a href=\"https://arxiv.org/pdf/2304.14787.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.14787.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2304.14628.pdf",
    "title": "https://arxiv.org/pdf/2304.14628.pdf",
    "latest": "2023-05-10T21:22:40+00:00",
    "last_post": {
      "url": "https://infosec.exchange/@juanan/110346200821591223",
      "content": "<p>Barriers and Self-Efficacy: A Large-Scale Study on the Impact of<br>OSS Courses on Student Perceptions<br><a href=\"https://arxiv.org/pdf/2304.14628.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2304.14628.pdf</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@gvwilson",
        "display_name": "Greg Wilson"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05252v1",
    "title": "https://arxiv.org/abs/2305.05252v1",
    "latest": "2023-05-10T21:07:41+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110346427558887700",
      "content": "<p>\ud83d\udcdd Distilling Script Knowledge From Large Language Models for Constrained Language Planning \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/siyuyuan/coscript\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/siyuyuan/coscript</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05252v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05252v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05226v1",
    "title": "Multi-Teacher Knowledge Distillation For Text Image Machine Translation",
    "latest": "2023-05-10T20:22:40+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110346250502806011",
      "content": "<p>\ud83d\udcdd Multi-Teacher Knowledge Distillation for Text Image Machine Translation \ud83d\udcda</p><p>\"Multi-Teacher Knowledge Distillation (MTKD) is proposed to effectively distill knowledge from the pipeline model into the end-to-end TIMT model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05226v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05226v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05214v1",
    "title": "https://arxiv.org/abs/2305.05214v1",
    "latest": "2023-05-10T20:07:38+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110346191397451467",
      "content": "<p>\ud83d\udcdd Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-Resource Languages \ud83d\udcda</p><p>\"We inject character and character-span noise into the training data of the high-resource language prior to learning the vocabulary and then train the model on the noisy high-resource language and low-resource language pair.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/anoopkunchukuttan/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/anoopkunchukuttan/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05214v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05214v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.00612",
    "title": "https://arxiv.org/abs/2304.00612",
    "latest": "2023-05-10T20:05:31+00:00",
    "last_post": {
      "url": "https://mastodon.social/@rrmutt/110340291659030519",
      "content": "<p>Term of the moment: \"sandbagging\" (via <a href=\"https://arxiv.org/abs/2304.00612\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.00612</span><span class=\"invisible\"></span></a>)</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://mastodon.social/@compsci_discussions",
        "display_name": "Compsci Weekly"
      },
      {
        "url": "https://mastodon.social/@rrmutt",
        "display_name": "rrmutt"
      }
    ]
  },
  {
    "link": "https://doi.org/10.3998/jep.3372",
    "title": "https://doi.org/10.3998/jep.3372",
    "latest": "2023-05-10T19:44:05+00:00",
    "last_post": {
      "url": "https://openbiblio.social/@ferli90/110346053149724805",
      "content": "<p>Snyder, L. O. &amp; Fathallah, J., (2023) \u201cSustainable Futures for OA Books: The Open Book Collective\u201d, The Journal of Electronic Publishing 26(1). <a href=\"https://doi.org/10.3998/jep.3372\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.3998/jep.3372</span><span class=\"invisible\"></span></a> <a href=\"https://openbiblio.social/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> <a href=\"https://openbiblio.social/tags/Books\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Books</span></a></p>"
    },
    "people": [
      {
        "url": "https://fedihum.org/@christof",
        "display_name": "Christof Sch\u00f6ch"
      }
    ]
  },
  {
    "link": "http://arxiv.org/abs/2304.15004",
    "title": "http://arxiv.org/abs/2304.15004",
    "latest": "2023-05-10T19:42:58+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@BenjaminHan/110345199904887368",
      "content": "<p><a href=\"https://sigmoid.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a>\u2019s Ostensible Emergent Abilities Are a Mirage</p><p>Blog: <a href=\"https://hai.stanford.edu/news/ais-ostensible-emergent-abilities-are-mirage\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">hai.stanford.edu/news/ais-oste</span><span class=\"invisible\">nsible-emergent-abilities-are-mirage</span></a></p><p>Paper: Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. \u201cAre Emergent Abilities of Large Language Models a Mirage?\u201d ArXiv [Cs.AI]. arXiv. <a href=\"http://arxiv.org/abs/2304.15004\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">arxiv.org/abs/2304.15004</span><span class=\"invisible\"></span></a>.</p><p><a href=\"https://sigmoid.social/tags/Paper\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Paper</span></a> <a href=\"https://sigmoid.social/tags/LLM\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLM</span></a> <a href=\"https://sigmoid.social/tags/GenerativeAI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>GenerativeAI</span></a> <a href=\"https://sigmoid.social/tags/NLP\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLP</span></a> <a href=\"https://sigmoid.social/tags/NLProc\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>NLProc</span></a></p>"
    },
    "people": [
      {
        "url": "https://fediscience.org/@UlrichJunker",
        "display_name": "Ulrich Junker"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.01056",
    "title": "From Organizations to Individuals: Psychoactive Substance Use By Professional Programmers",
    "latest": "2023-05-10T19:15:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110345984600017873",
      "content": "<p>Psychoactive Substance Use by Professional Programmers - <a href=\"https://arxiv.org/abs/2305.01056\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.01056</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "http://osf.io/uh5y6/",
    "title": "http://osf.io/uh5y6/",
    "latest": "2023-05-10T19:06:22+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110345950474885510",
      "content": "<p>Sensing weather and climate: phenomenological and ethnographic approaches <a href=\"http://osf.io/uh5y6/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/uh5y6/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05191v1",
    "title": "COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective",
    "latest": "2023-05-10T18:37:42+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110345837779914191",
      "content": "<p>\ud83d\udcdd COLA: Contextualized Commonsense Causal Reasoning From the Causal Inference Perspective \ud83d\udcda\ud83d\udc7e</p><p>\"Obtains rich incidental supervision from temporality and balances covariates from multiple timestamps to remove confounding effects, which can accurately detect commonsense causality between events in an event sequence.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05191v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05191v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05189v1",
    "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models",
    "latest": "2023-05-10T18:22:39+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110345778611187665",
      "content": "<p>\ud83d\udcdd SUR-adapter: Enhancing Text-to-Image Pre-Trained Diffusion Models with Large Language Models \ud83d\udcda\ud83d\udd2d</p><p>\"SUR-adapter is a parameter-efficient approach that can enable pre-trained diffusion models to understand and reason concise natural language without image quality degradation, by bridging the semantic gap between simple narrative prompts and complex keyword-based prompts.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05189v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05189v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05183v1",
    "title": "CSED: A Chinese Semantic Error Diagnosis Corpus",
    "latest": "2023-05-10T17:22:38+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110345542590794094",
      "content": "<p>\ud83d\udcdd CSED: A Chinese Semantic Error Diagnosis Corpus \ud83d\udcda\ud83d\udc7e</p><p>\"Of Chinese semantic error diagnosis (CSED) is important because it focuses on the correction of semantic errors, which are common in Chinese writing and may cause syntactic irregularities or even problems of comprehension.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05183v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05183v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/nmeth.2659",
    "title": "Error bars - Nature Methods",
    "latest": "2023-05-10T17:03:48+00:00",
    "last_post": {
      "url": "https://fediscience.org/@ct_bergstrom/110331658837624116",
      "content": "<p>It sounds trivial and obvious, but are you reading error bars correctly? </p><p>Do you know whether you're looking at standard errors (measures of inferential uncertainty), standard deviations (measures of spread of individual observations), or 95% confidence intervals around the mean? </p><p>And are your intuitions correct about how to interpret each of these?</p><p>Here's a nice primer, refresher, or teaching article. </p><p><a href=\"https://www.nature.com/articles/nmeth.2659\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"\">nature.com/articles/nmeth.2659</span><span class=\"invisible\"></span></a></p><p>h/t <span class=\"h-card\"><a href=\"https://hci.social/@jakehofman\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>jakehofman</span></a></span></p>"
    },
    "people": [
      {
        "url": "https://datasci.social/@bearloga",
        "display_name": "Mikhail Popov"
      },
      {
        "url": "https://datasci.social/@mszll",
        "display_name": "Michael Szell"
      },
      {
        "url": "https://fediscience.org/@ct_bergstrom",
        "display_name": "Carl T. Bergstrom"
      },
      {
        "url": "https://ecoevo.social/@naupaka",
        "display_name": "Naupaka Zimmerman"
      },
      {
        "url": "https://nerdculture.de/@dornhaus",
        "display_name": "Anna Dornhaus"
      },
      {
        "url": "https://mathstodon.xyz/@theta_max",
        "display_name": "Tom"
      },
      {
        "url": "https://sigmoid.social/@ogrisel",
        "display_name": "Olivier Grisel"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05181v1",
    "title": "https://arxiv.org/abs/2305.05181v1",
    "latest": "2023-05-10T16:22:39+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110345306742358570",
      "content": "<p>\ud83d\udcdd MoT: Pre-Thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes Memory of Thoughts (MoT), which let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/LeeSureman/MoT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/LeeSureman/MoT</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.05181v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05181v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04990v1",
    "title": "Explanation-based Finetuning Makes Models More Robust to Spurious Cues",
    "latest": "2023-05-10T09:07:40+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110343596332263652",
      "content": "<p>\ud83d\udcdd Explanation-Based Finetuning Makes Models More Robust to Spurious Cues \ud83d\udcda\ud83e\udde0</p><p>\"Fine-tunes the model to generate the free-text explanation using training data consisting of examples and their corresponding human-written explanations, and then further finetune the model to predict the answer given the input and the generated explanation using the same training data.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04990v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04990v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04989v1",
    "title": "Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust",
    "latest": "2023-05-10T08:07:38+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110343360272919252",
      "content": "<p>\ud83d\udcdd Knowledge Graph Guided Semantic Evaluation of Language Models for User Trust \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04989v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04989v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04978v1",
    "title": "NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge",
    "latest": "2023-05-10T07:22:41+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110343183516265841",
      "content": "<p>\ud83d\udcdd NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge \ud83d\udcda</p><p>\"Proposes a novel NeuroComparative framework to acquire the comparative knowledge for common objects, such as \"steel is heavier than styrofoam\" or \"spiders are more creepy than snakes\", by distilling the knowledge from a larger pretrained model (e.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04978v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04978v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.3998/jep.3303",
    "title": "https://doi.org/10.3998/jep.3303",
    "latest": "2023-05-10T05:21:36+00:00",
    "last_post": {
      "url": "https://openbiblio.social/@ferli90/110342649869004728",
      "content": "<p>Ferwerda, E. &amp; Snijder, R. &amp; Stern, N., (2023) \u201cOpen Access to Books \u2013 the Perspective of a Non-profit Infrastructure Provider\u201d, The Journal of Electronic Publishing 26(1). <a href=\"https://doi.org/10.3998/jep.3303\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.3998/jep.3303</span><span class=\"invisible\"></span></a> <a href=\"https://openbiblio.social/tags/OpenAccess\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenAccess</span></a> <a href=\"https://openbiblio.social/tags/Books\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Books</span></a> <a href=\"https://openbiblio.social/tags/OAPENLibrary\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OAPENLibrary</span></a> <a href=\"https://openbiblio.social/tags/Directoryof\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Directoryof</span></a> OpenAccessBooks</p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@RonaldSnijder",
        "display_name": "Ronald Snijder"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04166v1",
    "title": "https://arxiv.org/abs/2305.04166v1",
    "latest": "2023-05-10T04:02:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110342396349702898",
      "content": "<p>\ud83d\udcdd UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese \ud83d\udd2d\ud83d\udcda</p><p>\"Works by using a multi-level encoder output fusion mechanism, which effectively enhanced the image representation ability by a multi-level encoder output fusion mechanism to improve the quality of generated captions compared to previous image captioning models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/caodoanh2001/UIT-OpenViIC-labeller\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/caodoanh2001/UIT-Op</span><span class=\"invisible\">enViIC-labeller</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04166v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04166v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04082v1",
    "title": "A Minimal Approach for Natural Language Action Space in Text-based Games",
    "latest": "2023-05-10T03:32:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110342278174333522",
      "content": "<p>\ud83d\udcdd A Minimal Approach for Natural Language Action Space in Text-Based Games \ud83e\udde0\ud83d\udcda</p><p>\"Presents a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04082v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04082v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03157",
    "title": "Good Will Hunting's Problem: Counting Homeomorphically Irreducible Trees",
    "latest": "2023-05-10T02:46:44+00:00",
    "last_post": {
      "url": "https://social.skewed.de/@tiago/110337125502312003",
      "content": "<p>How do you like them apples?</p><p><a href=\"https://arxiv.org/abs/2305.03157\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03157</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://social.skewed.de/@tiago",
        "display_name": "Tiago Peixoto"
      },
      {
        "url": "https://hci.social/@bkeegan",
        "display_name": "Brian C. Keegan"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05364",
    "title": "Large Language Model Programs",
    "latest": "2023-05-10T02:24:00+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110342009048427910",
      "content": "<p>Large Language Model Programs</p><p>Presents LLM programs, the emerging methodology of embedding LLMs in a classic program to carry out more complex tasks</p><p><a href=\"https://arxiv.org/abs/2305.05364\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05364</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05383",
    "title": "Code Execution with Pre-trained Language Models",
    "latest": "2023-05-10T02:23:58+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110342008899127695",
      "content": "<p>Code Execution with Pre-trained Language Models</p><p>Presents CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension.</p><p><a href=\"https://arxiv.org/abs/2305.05383\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05383</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03668",
    "title": "A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding",
    "latest": "2023-05-10T02:23:56+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110342008754247182",
      "content": "<p>A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding</p><p>Presents the Wikipedia Webpage 2M suite; the first to retain the full set of images, text, and structure data available in a page.</p><p><a href=\"https://arxiv.org/abs/2305.03668\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03668</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.05662",
    "title": "https://arxiv.org/abs/2305.05662",
    "latest": "2023-05-10T02:23:53+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110342008591442905",
      "content": "<p>InternChat: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language</p><p>repo: <a href=\"https://github.com/OpenGVLab/InternChat\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/OpenGVLab/InternCha</span><span class=\"invisible\">t</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.05662\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.05662</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03923v1",
    "title": "Active Continual Learning: Labelling Queries in a Sequence of Tasks",
    "latest": "2023-05-10T02:17:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110341983481072693",
      "content": "<p>\ud83d\udcdd Active Continual Learning: Labelling Queries in a Sequence of Tasks \ud83e\udde0\ud83d\udcda</p><p>\"Proposes a forgetting-learning profile to evaluate the trade-off between forgetting and learning in continual active learning (ACL), and demonstrate the effectiveness of conditioning the query strategy on the annotations collected for the previous tasks.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03923v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03923v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1080/00031305.2017.1375989",
    "title": "doi.org/10.1080/00031305.2017....",
    "latest": "2023-05-10T01:58:10+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@kbroman/110339581912808989",
      "content": "<p>\ud83e\uddf510 fun facts about my paper with <span class=\"h-card\"><a href=\"https://vis.social/@kara_woo\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>kara_woo</span></a></span> on Data Organization in Spreadsheets <a href=\"https://doi.org/10.1080/00031305.2017.1375989\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1080/00031305.2017.</span><span class=\"invisible\">1375989</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.social/@bramzandbelt",
        "display_name": "Bram Zandbelt"
      },
      {
        "url": "https://ecoevo.social/@naupaka",
        "display_name": "Naupaka Zimmerman"
      },
      {
        "url": "https://fosstodon.org/@hadleywickham",
        "display_name": "Hadley Wickham"
      },
      {
        "url": "https://toot.cat/@infotroph",
        "display_name": "Chris Black"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04183v1",
    "title": "https://arxiv.org/abs/2305.04183v1",
    "latest": "2023-05-10T01:32:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110341806599738707",
      "content": "<p>\ud83d\udcdd OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese \ud83d\udcda</p><p>\"OpenViVQA consists of a dataset of 11,000+ images associated with 37,000+ question-answer pairs.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/hieunghia-pat/OpenViVQA-dataset\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/hieunghia-pat/OpenV</span><span class=\"invisible\">iVQA-dataset</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04183v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04183v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04181v1",
    "title": "https://arxiv.org/abs/2305.04181v1",
    "latest": "2023-05-10T00:32:27+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110341570364872042",
      "content": "<p>\ud83d\udcdd Shall We Trust All Relational Tuples by Open Information Extraction? A Study on Speculation Detection \ud83d\udcda\ud83d\udc7e</p><p>\"OIE-Spec consists of two components: (1) Sentence-level speculation detection model trained on LSOIE, and (2) a rule-based post-processing algorithm.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/daviddongkc/OIE_Spec\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/daviddongkc/OIE_Spe</span><span class=\"invisible\">c</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04181v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04181v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04160v1",
    "title": "https://arxiv.org/abs/2305.04160v1",
    "latest": "2023-05-09T23:47:26+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110341393404048601",
      "content": "<p>\ud83d\udcdd X-Llm: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages \ud83d\udcda\ud83d\udc7e\ud83d\udd2d</p><p>\"Proposes X-LLM, which aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ''X'' denotes multi-modalities such as image, speech, and videos, and ''L'' denotes languages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/THUDM/ChatGLM-6B\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/THUDM/ChatGLM-6B</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04160v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04160v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04100v1",
    "title": "Rhetorical Role Labeling of Legal Documents using Transformers and Graph Neural Networks",
    "latest": "2023-05-09T21:17:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110340803705483606",
      "content": "<p>\ud83d\udcdd Rhetorical Role Labeling of Legal Documents Using Transformers and Graph Neural Networks \ud83d\udcda</p><p>\"Graph Convolutional Networks, Label Propagation Algorithm, Transformer-based Approaches, and variants of BERT to perform text classification on complex legal documents for the task of rhetorical role labelling.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04100v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04100v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04091v1",
    "title": "https://arxiv.org/abs/2305.04091v1",
    "latest": "2023-05-09T21:02:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110340744884649811",
      "content": "<p>\ud83d\udcdd Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models \ud83d\udcda</p><p>\"Devises a plan to divide the entire task into smaller subtasks, and then carry out the subtasks according to the plan to obtain final answers for the original task.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/AGI-Edgerunners/Pla</span><span class=\"invisible\">n-and-Solve-Prompting</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04091v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04091v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41562-023-01589-7",
    "title": "Seeing racial avoidance on New York City streets - Nature Human Behaviour",
    "latest": "2023-05-09T20:40:01+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110340656455796116",
      "content": "<p>Seeing racial avoidance on New York City streets - <a href=\"https://www.nature.com/articles/s41562-023-01589-7\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41562-023</span><span class=\"invisible\">-01589-7</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.00955",
    "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation",
    "latest": "2023-05-09T20:19:59+00:00",
    "last_post": {
      "url": "https://creative.ai/@alexjc/110340577670535193",
      "content": "<p>RT @HelloSurgeAI@twitter.com</p><p>Human feedback is important to train safe and helpful LLMs.</p><p>Did you know there is now a large taxonomy of methods that leverage human feedback?</p><p>This recent paper provides a comprehensive overview of recent methods.</p><p><a href=\"https://arxiv.org/abs/2305.00955\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.00955</span><span class=\"invisible\"></span></a></p><p>\ud83d\udc26\ud83d\udd17: <a href=\"https://twitter.com/HelloSurgeAI/status/1655920376612806657\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">twitter.com/HelloSurgeAI/statu</span><span class=\"invisible\">s/1655920376612806657</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@alexjc",
        "display_name": "Alex J. Champandard \ud83c\udf31"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04076v1",
    "title": "https://arxiv.org/abs/2305.04076v1",
    "latest": "2023-05-09T20:17:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110340567873392015",
      "content": "<p>\ud83d\udcdd SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition \ud83d\udcda\ud83d\udc7e</p><p>\"Our SANTA framework is composed of memory-smoothed focal loss (M-SFL), entity-aware KNN (E-KNN), boundary mixup, and noise-tolerant loss (NT).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/PKUnlpicler/SANTA\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/PKUnlpicler/SANTA</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04076v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04076v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/1809.05679",
    "title": "Graph Convolutional Networks for Text Classification",
    "latest": "2023-05-09T19:29:09+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ansgarscherp/110340377554329480",
      "content": "<p>We have been tracking Graph Neural Networks (GNN)  for text classification (TC) for the past 2+ years (w <span class=\"h-card\"><a href=\"https://sigmoid.social/@lpag\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>lpag</span></a></span>). Every other week, a new paper comes out, but *still* does not improve the results. Will it stop? There is now even a full survey: <a href=\"https://arxiv.org/abs/1809.05679\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/1809.05679</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04067v1",
    "title": "Reactive Perturbation Defocusing for Textual Adversarial Defense",
    "latest": "2023-05-09T19:17:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110340331981515701",
      "content": "<p>\ud83d\udcdd Reactive Perturbation Defocusing for Textual Adversarial Defense \ud83d\udcda</p><p>\"Uses an adversarial detector to identify adversarial examples and reduce false defenses on natural examples, and then injects safe perturbations into adversarial examples to distract the objective models from the malicious ones.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04067v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04067v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04049v1",
    "title": "https://arxiv.org/abs/2305.04049v1",
    "latest": "2023-05-09T19:02:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110340272998800590",
      "content": "<p>\ud83d\udcdd Actively Discovering New Slots for Task-Oriented Conversation \ud83d\udcda</p><p>\"A general new slot discovery task is formulated in an information extraction fashion and incorporated into a active learning framework to realize human-in-the-loop learning by incorporating uncertainty-based sampling and diversity-based sampling.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/newslotdetection/newslotdetection\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/newslotdetection/ne</span><span class=\"invisible\">wslotdetection</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04049v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04049v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04044v1",
    "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
    "latest": "2023-05-09T18:17:28+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110340095928063731",
      "content": "<p>\ud83d\udcdd Diffusion-Nat: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation \ud83d\udcda</p><p>\"Proposes Diffusion-NAT which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04044v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04044v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04039v1",
    "title": "https://arxiv.org/abs/2305.04039v1",
    "latest": "2023-05-09T18:02:30+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110340037068191098",
      "content": "<p>\ud83d\udcdd Refining the Responses of LLMs by Themselves \ud83d\udcda\ud83d\udc7e</p><p>\"An iterative self-evaluating optimization mechanism, leveraging the large language model itself, is employed to iteratively optimize the answers generated by large language models without relying on auxiliary models.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/henryyantq/OptimaLLM\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/henryyantq/OptimaLL</span><span class=\"invisible\">M</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04039v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04039v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04003v1",
    "title": "https://arxiv.org/abs/2305.04003v1",
    "latest": "2023-05-09T17:47:31+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110339978112853741",
      "content": "<p>\ud83d\udcdd ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"Works in three steps: (1) it converts a natural language processing (NLP) problem statement into a logic problem using a formal grammar, (2) it converts this logic problem into a verification query using an encoding scheme, (3) it solves this query using a neural network verifier.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/aisecprivate/ANTONIO\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/aisecprivate/ANTONI</span><span class=\"invisible\">O</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.04003v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04003v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03987v1",
    "title": "Replicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization",
    "latest": "2023-05-09T17:17:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110339860044119852",
      "content": "<p>\ud83d\udcdd Replicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization \ud83d\udcda\ud83d\udc7e</p><p>\"An offline imitation learning that can learn from real dialogues dataset, the state transition information is used which can alleviate the covariate shift problem caused by sequential decision-making process.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03987v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03987v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03981v1",
    "title": "Pre-training Language Model as a Multi-perspective Course Learner",
    "latest": "2023-05-09T16:47:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110339742054030765",
      "content": "<p>\ud83d\udcdd Pre-Training Language Model as a Multi-Perspective Course Learner \ud83d\udcda</p><p>\"Multiperspective self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way; Two self-correction courses are proposed to bridge the chasm between the two encoders by creating a \"correction notebook\" for secondary-supervision.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03981v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03981v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41386-023-01588-2",
    "title": "Dose-response relationships of LSD-induced subjective experiences in humans - Neuropsychopharmacology",
    "latest": "2023-05-09T16:40:02+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110339712806028003",
      "content": "<p>Dose-response relationships of LSD-induced subjective experiences in humans - <a href=\"https://www.nature.com/articles/s41386-023-01588-2\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41386-023</span><span class=\"invisible\">-01588-2</span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://doi.org/10.7287/peerj.preprints.3210v1",
    "title": "doi.org/10.7287/peerj.preprint...",
    "latest": "2023-05-09T16:11:14+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@kbroman/110339599543047419",
      "content": "<p>9. The series includes a cool paper by Hilary Parker that's available only with the preprints.<br><a href=\"https://doi.org/10.7287/peerj.preprints.3210v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.7287/peerj.preprint</span><span class=\"invisible\">s.3210v1</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@kbroman",
        "display_name": "Karl Broman"
      }
    ]
  },
  {
    "link": "https://www.tandfonline.com/toc/utas20/72/1?nav=tocList",
    "title": "The American Statistician",
    "latest": "2023-05-09T16:11:01+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@kbroman/110339598685430542",
      "content": "<p>8. The paper is part of a great series of articles on data science, instigated by <span class=\"h-card\"><a href=\"https://fosstodon.org/@jennybryan\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>jennybryan</span></a></span> and <span class=\"h-card\"><a href=\"https://fosstodon.org/@hadleywickham\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>hadleywickham</span></a></span>. <a href=\"https://www.tandfonline.com/toc/utas20/72/1?nav=tocList\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">tandfonline.com/toc/utas20/72/</span><span class=\"invisible\">1?nav=tocList</span></a><br><a href=\"https://peerj.com/collections/50-practicaldatascistats\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">peerj.com/collections/50-pract</span><span class=\"invisible\">icaldatascistats</span></a></p>"
    },
    "people": [
      {
        "url": "https://fosstodon.org/@kbroman",
        "display_name": "Karl Broman"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04603.pdf",
    "title": "Privacy-Preserving Representations are not Enough -- Recovering Scene Content from Camera Poses",
    "latest": "2023-05-09T09:44:25+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@ducha_aiki/110338078515823649",
      "content": "<p>Privacy-Preserving Representations are not Enough -- Recovering Scene Content from Camera Poses</p><p>Kunal Chelani, Torsten Sattler, Fredrik Kahl, Zuzana Kukelova</p><p>tl;dr: clever attack: you query with IKEA furniture, get camera pose and recover the scene. <br><a href=\"https://arxiv.org/abs/2305.04603.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04603.pdf</span><span class=\"invisible\"></span></a><br><a href=\"https://sigmoid.social/tags/computervision\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>computervision</span></a> <a href=\"https://sigmoid.social/tags/deeplearning\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>deeplearning</span></a> <br><a href=\"https://sigmoid.social/tags/dmytrotweetsaboutDL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>dmytrotweetsaboutDL</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@ducha_aiki",
        "display_name": "Dmytro Mishkin \ud83c\uddfa\ud83c\udde6"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.02437",
    "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory",
    "latest": "2023-05-09T09:28:27+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110313372685356350",
      "content": "<p>Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory</p><p><a href=\"https://arxiv.org/abs/2305.02437\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.02437</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2212.08073?utm_source=substack&amp;utm_medium=email",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "latest": "2023-05-09T09:18:24+00:00",
    "last_post": {
      "url": "https://techpolicy.social/@guifitz/110337975847092477",
      "content": "<p>Well, this is an interesting proposition for <a href=\"https://techpolicy.social/tags/ArtificialIntelligence\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ArtificialIntelligence</span></a>:</p><p>\"As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'.\"</p><p><a href=\"https://arxiv.org/abs/2212.08073?utm_source=substack&amp;utm_medium=email\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">arxiv.org/abs/2212.08073?utm_s</span><span class=\"invisible\">ource=substack&amp;utm_medium=email</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03819v1",
    "title": "Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces",
    "latest": "2023-05-09T09:17:29+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110337972618900846",
      "content": "<p>\ud83d\udcdd Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces \ud83d\udcda</p><p>\"S are trained on text data in a self-supervised fashion to predict the next word given its previous words as input, and then fine-tuned to predict the next character given its previous characters as input.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03819v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03819v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03796v1",
    "title": "Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation",
    "latest": "2023-05-09T08:32:25+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110337795406316262",
      "content": "<p>\ud83d\udcdd Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation \ud83d\udcda</p><p>\"A new Transformer variant named RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY, and it rediscovers the local windowed attention effect deemed necessary for length extrapolation.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03796v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03796v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/d41586-022-00793-1",
    "title": "The rise of citational justice: how scholars are making references fairer",
    "latest": "2023-05-09T07:28:12+00:00",
    "last_post": {
      "url": "https://scholar.social/@dingemansemark/110337537130685675",
      "content": "<p>Clarivate's Web of Science \u2014for-profit purveyor of scholarly status\u2014 offers a \"geographic citation map\" showing the distribution of citations of researcher's work. In the interest of citational justice, I would be much more interested in being able to generate the reverse: the geographic diversity of the research cited *by* a researcher</p><p>See this on <a href=\"https://scholar.social/tags/citationaljustice\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>citationaljustice</span></a>: <a href=\"https://www.nature.com/articles/d41586-022-00793-1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/d41586-022</span><span class=\"invisible\">-00793-1</span></a></p>"
    },
    "people": [
      {
        "url": "https://aleph.land/@mtriclot",
        "display_name": "Mathieu Triclot"
      },
      {
        "url": "https://dair-community.social/@DrVeronikaCH",
        "display_name": "Veronika Cheplygina"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.02412",
    "title": "Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents",
    "latest": "2023-05-09T07:23:17+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110313368544832809",
      "content": "<p>Plan, Eliminate, and Track \u2014 Language Models are Good Teachers for Embodied Agents</p><p><a href=\"https://arxiv.org/abs/2305.02412\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.02412</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0081714",
    "title": "The M\u00fcller-Lyer Illusion in Ant Foraging",
    "latest": "2023-05-09T07:15:55+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@gregeganSF/110337494574679035",
      "content": "<p>Some researchers claim to have shown that swarms of ants can collectively be subject to the same illusion:</p><p><a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0081714\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">journals.plos.org/plosone/arti</span><span class=\"invisible\">cle?id=10.1371/journal.pone.0081714</span></a></p><p>But I got frustrated trying to make sense of their paper, given that Figs 2 &amp; 3, supposedly outwards/inwards arrows respectively, are identical!</p>"
    },
    "people": [
      {
        "url": "https://mathstodon.xyz/@gregeganSF",
        "display_name": "Greg Egan"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03793v1",
    "title": "Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies and Simple Labels",
    "latest": "2023-05-09T07:02:17+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110337440959515738",
      "content": "<p>\ud83d\udcdd Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies and Simple Labels \ud83d\udcda\ud83e\udde0</p><p>\"OpenFSP relies on sentence encoders for intent detection and slot filling tasks, and on a novel domain matching algorithm to predict the domains from a handful of domain-agnostic slot types.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03793v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03793v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.14454",
    "title": "PMC-LLaMA: Further Finetuning LLaMA on Medical Papers",
    "latest": "2023-05-09T05:10:03+00:00",
    "last_post": {
      "url": "https://die-partei.social/@hackernews/110336999651048914",
      "content": "<p>PMC-LLaMA: Further Finetuning LLaMA on Medical Papers - <a href=\"https://arxiv.org/abs/2304.14454\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.14454</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://die-partei.social/@hackernews",
        "display_name": "Hackernews"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03726v1",
    "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
    "latest": "2023-05-09T03:41:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110336649928577936",
      "content": "<p>\ud83d\udcdd Otter: A Multi-Modal Model with in-Context Instruction Tuning \ud83d\udd2d\ud83d\udcda</p><p>\"A multi-modal large language model (LLM) with in-context learning ability that is fine-tuned from OpenFlamingo, which is a multi-modal extension of DeepMind's Flamingo model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03726v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03726v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03602v1",
    "title": "A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation",
    "latest": "2023-05-09T03:17:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110336555492724520",
      "content": "<p>\ud83d\udcdd A Dual Semantic-Aware Recurrent Global-Adaptive Network for Vision-and-Language Navigation \ud83d\udd2d\ud83d\udcda</p><p>\"DSRG consists of three parts: Instruction-guidance linguistic module (IGL), Appearance-semantics visual module (ASV), Global-adaptive aggregation module (GAA) and Recurrent memory fusion module (RMF).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03602v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03602v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03204v1",
    "title": "VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation",
    "latest": "2023-05-09T02:53:04+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110336461040993213",
      "content": "<p>\ud83d\udcdd VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation \ud83d\udd2d\ud83d\udcda</p><p>\"Our VideoOFA model first pre-trains a vision-language Transformer on image-text data, and then adapts it to video data in an intermediate video-text pre-training stage.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a> <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03204v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03204v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/pdf/2305.04532.pdf",
    "title": "https://arxiv.org/pdf/2305.04532.pdf",
    "latest": "2023-05-09T02:32:51+00:00",
    "last_post": {
      "url": "https://mastodon.radio/@mgiven/110336381529639323",
      "content": "<p>\"Latest Trends in Artificial Intelligence Technology: A Scoping Review\"</p><p><a href=\"https://arxiv.org/pdf/2305.04532.pdf\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/pdf/2305.04532.pdf</span><span class=\"invisible\"></span></a></p><p><a href=\"https://mastodon.radio/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.radio/@mgiven",
        "display_name": "Mott"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03695v1",
    "title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
    "latest": "2023-05-09T02:29:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110336366812745064",
      "content": "<p>\ud83d\udcdd Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements \ud83d\udcda\ud83d\udc7e</p><p>\"Uses a RoBERTa-Large as the backbone, and a training set of 7 million commonsense statements created from 19 QA datasets and two large-scale knowledge bases.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03695v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03695v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04087",
    "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
    "latest": "2023-05-09T02:10:14+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110336292594128861",
      "content": "<p>Self-Edit: Fault-Aware Code Editor for Code Generation</p><p>Improves the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B.</p><p><a href=\"https://arxiv.org/abs/2305.04087\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04087</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03668v1",
    "title": "A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding",
    "latest": "2023-05-09T01:53:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110336225144290093",
      "content": "<p>\ud83d\udcdd A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding \ud83d\udcda\ud83d\udd2d</p><p>\"By separating the most relevant image and text content as global tokens to attend to the rest of the webpage for context, Prefix Global performs better than full attention with lower computational complexity.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CV\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CV</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03668v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03668v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03655v1",
    "title": "https://arxiv.org/abs/2305.03655v1",
    "latest": "2023-05-09T01:41:03+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110336177833135689",
      "content": "<p>\ud83d\udcdd White-Box Multi-Objective Adversarial Attack on Dialogue Generation \ud83d\udcda\ud83e\udde0</p><p>\"DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adaptive searching mechanism to iteratively craft adversarial samples with only a few modifications.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/CR\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CR</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/yul091/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/yul091/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03655v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03655v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04369",
    "title": "Getting More out of Large Language Models for Proofs",
    "latest": "2023-05-09T01:19:25+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110336092790565906",
      "content": "<p>Getting More out of Large Language Models for Proofs</p><p><a href=\"https://arxiv.org/abs/2305.04369\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04369</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03573v1",
    "title": "In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models",
    "latest": "2023-05-09T01:05:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110336036489399245",
      "content": "<p>\ud83d\udcdd In-Context Learning as Maintaining Coherency: A Study of on-the-Fly Machine Translation Using Large Language Models \ud83d\udcda\ud83d\udc7e</p><p>\"Achieved by finetuning the pretrained language model on the prompt examples and then evaluating the finetuned model for translation of the test sentence, using the prompt examples as the context for generating the translation of the test sentence.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03573v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03573v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03520v1",
    "title": "https://arxiv.org/abs/2305.03520v1",
    "latest": "2023-05-09T00:53:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110335989324551631",
      "content": "<p>\ud83d\udcdd Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation \ud83d\udcda\ud83d\udc7e</p><p>\"Introduces a new context-aware approach to unsupervised word sense disambiguation that provides a flexible mechanism for incorporating contextual information into the similarity measurement process by leveraging the contextual similarity between word senses.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/jorge-martinez-gil/uwsd\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/jorge-martinez-gil/</span><span class=\"invisible\">uwsd</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03520v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03520v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.04790",
    "title": "https://arxiv.org/abs/2305.04790",
    "latest": "2023-05-09T00:50:13+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110335977955364104",
      "content": "<p>MultiModal-GPT: A Vision and Language Model for Dialogue with Humans</p><p>repo: <a href=\"https://github.com/open-mmlab/Multimodal-GPT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/open-mmlab/Multimod</span><span class=\"invisible\">al-GPT</span></a><br>abs: <a href=\"https://arxiv.org/abs/2305.04790\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.04790</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.03612",
    "title": "What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory",
    "latest": "2023-05-09T00:37:10+00:00",
    "last_post": {
      "url": "https://hci.social/@mluczak/110335926209422753",
      "content": "<p>And finally we brought it all back into the immediate present with \"What does <a href=\"https://hci.social/tags/ChatGPT\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ChatGPT</span></a> return about human <a href=\"https://hci.social/tags/values\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>values</span></a>? Exploring value bias in ChatGPT using a descriptive value theory\" <a href=\"https://arxiv.org/abs/2304.03612\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.03612</span><span class=\"invisible\"></span></a> 10/10 <a href=\"https://hci.social/tags/LLMs\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LLMs</span></a> <a href=\"https://hci.social/tags/psychology\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>psychology</span></a> <a href=\"https://hci.social/tags/science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>science</span></a> <a href=\"https://hci.social/tags/research\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>research</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03518v1",
    "title": "Black-box Prompt Tuning with Subspace Learning",
    "latest": "2023-05-09T00:05:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110335800554801451",
      "content": "<p>\ud83d\udcdd Black-Box Prompt Tuning with Subspace Learning \ud83d\udcda\ud83d\udc7e</p><p>\"First identifies nearly optimal prompts for source tasks in a shared subspace by meta-learning and then finds a prompt for the target task by optimizing prompts in the subspace.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03518v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03518v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://www.nature.com/articles/s41550-023-01962-6",
    "title": "Spatially resolved imaging of the inner Fomalhaut disk using JWST/MIRI - Nature Astronomy",
    "latest": "2023-05-08T23:36:30+00:00",
    "last_post": {
      "url": "https://mathstodon.xyz/@filipw/110334781318606083",
      "content": "<p>new beautiful James Webb Space Telescope image - the star Fomalhaut and its planetary system with spectacular nested concentric rings of dust</p><p>\ud83d\udcdd The accompanying paper in Nature Astronomy: Spatially resolved imaging of the inner Fomalhaut disk using JWST/MIRI<br>\ud83d\udd17 <a href=\"https://www.nature.com/articles/s41550-023-01962-6\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://www.</span><span class=\"ellipsis\">nature.com/articles/s41550-023</span><span class=\"invisible\">-01962-6</span></a><br>\ud83c\udff7\ufe0f <a href=\"https://mathstodon.xyz/tags/astrodon\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>astrodon</span></a> <a href=\"https://mathstodon.xyz/tags/astronomy\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>astronomy</span></a> <a href=\"https://mathstodon.xyz/tags/science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>science</span></a></p><p>Credits<br>IMAGE: NASA, ESA, CSA \u2028IMAGE PROCESSING: Andr\u00e1s G\u00e1sp\u00e1r (University of Arizona), Alyssa Pagan (STScI) \u2028SCIENCE: Andr\u00e1s G\u00e1sp\u00e1r (University of Arizona)</p>"
    },
    "people": [
      {
        "url": "https://mastodon.art/@mtyka",
        "display_name": "Mike Tyka"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03517v1",
    "title": "https://arxiv.org/abs/2305.03517v1",
    "latest": "2023-05-08T23:29:06+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110335658998693803",
      "content": "<p>\ud83d\udcdd Few-Shot Domain-Adaptive Visually-Fused Event Detection From Text \ud83d\udcda\ud83d\udc7e</p><p>\"Relies on an imaginator, which is trained on text-only data points and can generate images from text descriptions in the absence of visual context for training the event detection model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/huggingface/transfo</span><span class=\"invisible\">rmers</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03517v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03517v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.1126/science.adg0493",
    "title": "doi.org/10.1126/science.adg049...",
    "latest": "2023-05-08T23:10:44+00:00",
    "last_post": {
      "url": "https://fediscience.org/@Forrest/110321615593912438",
      "content": "<p>RT @erleellis<br>Malthus &amp; Ehrlich were wrong.</p><p>Boserup &amp; Ostrom were right.</p><p>Why are they ignored?</p><p>My review of Jonsson &amp; Wennerlind's \"Scarcity\" @ScienceMagazine<br><a href=\"https://doi.org/10.1126/science.adg0493\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">doi.org/10.1126/science.adg049</span><span class=\"invisible\">3</span></a></p>"
    },
    "people": [
      {
        "url": "https://mastodon.nz/@ojala",
        "display_name": "Luis Apiolaza"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03511v1",
    "title": "Shared Latent Space by Both Languages in Non-Autoregressive Neural Machine Translation",
    "latest": "2023-05-08T21:53:03+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110335281330801667",
      "content": "<p>\ud83d\udcdd Shared Latent Space by Both Languages in Non-Autoregressive Neural Machine Translation \ud83d\udcda\ud83e\udde0</p><p>\"Designs an advanced hierarchical latent modeling that is based on a dual reconstruction perspective and shares a latent space across both languages so that it hypothetically alleviates or solves the above disadvantages.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03511v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03511v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03510v1",
    "title": "Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment",
    "latest": "2023-05-08T21:17:03+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110335139728589151",
      "content": "<p>\ud83d\udcdd Parameter-Efficient Cross-Lingual Transfer of Vision and Language Models via Translation-Based Alignment \ud83d\udcda\ud83d\udc7e</p><p>\"Proposes a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03510v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03510v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.01711",
    "title": "Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner",
    "latest": "2023-05-08T21:12:58+00:00",
    "last_post": {
      "url": "https://sigmoid.social/@aran/110312515022885739",
      "content": "<p>Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner</p><p>Proposes Prompt-based Continued Pre-training, which consistently improves the performance of prompt-based fine-tuning (up to 20.1% absolute).</p><p>Paper: <a href=\"https://arxiv.org/abs/2305.01711\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.01711</span><span class=\"invisible\"></span></a><br>Code: <a href=\"https://github.com/ZhengxiangShi/PowerfulPromptFT\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/ZhengxiangShi/Power</span><span class=\"invisible\">fulPromptFT</span></a></p>"
    },
    "people": [
      {
        "url": "https://sigmoid.social/@aran",
        "display_name": "Aran Komatsuzaki"
      },
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03508v1",
    "title": "https://arxiv.org/abs/2305.03508v1",
    "latest": "2023-05-08T20:41:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110334998295380724",
      "content": "<p>\ud83d\udcdd CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing \ud83d\udcda\ud83e\udde0</p><p>\"Introduces a labeled dataset of 178M sentences for citation-worthiness detection in the legal domain from the Caselaw Access Project (CAP).\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/fnl/segtok\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/fnl/segtok</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03508v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03508v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2304.13712",
    "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
    "latest": "2023-05-08T20:36:27+00:00",
    "last_post": {
      "url": "https://mastodon.online/@tomstafford/110332961058200534",
      "content": "<p>The evolutionary tree of Large Language Models</p><p>Figure 1 from Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond <a href=\"https://arxiv.org/abs/2304.13712\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2304.13712</span><span class=\"invisible\"></span></a>, via <a href=\"https://datamachina.substack.com/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">datamachina.substack.com/</span><span class=\"invisible\"></span></a> @ds_ldn</p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      },
      {
        "url": "https://ecoevo.social/@naupaka",
        "display_name": "Naupaka Zimmerman"
      }
    ]
  },
  {
    "link": "http://osf.io/hxuws/",
    "title": "http://osf.io/hxuws/",
    "latest": "2023-05-08T20:11:05+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110334880325749222",
      "content": "<p>It all runs in the family? A life course perspective on intergenerational family positions and wealth accumulation <a href=\"http://osf.io/hxuws/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/hxuws/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "http://osf.io/ka6nr/",
    "title": "http://osf.io/ka6nr/",
    "latest": "2023-05-08T20:11:05+00:00",
    "last_post": {
      "url": "https://botsin.space/@SocArXivBot/110334880356703724",
      "content": "<p>Transitions into and out of Car Ownership among Low-Income Households in the United States <a href=\"http://osf.io/ka6nr/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">http://</span><span class=\"\">osf.io/ka6nr/</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://botsin.space/@SocArXivBot",
        "display_name": "SocArXiv"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03453v1",
    "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering",
    "latest": "2023-05-08T19:53:07+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110334809699146096",
      "content": "<p>\ud83d\udcdd T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering \ud83d\udcda</p><p>\"Generates high-quality chain-of-thought (CoT) rationales as teaching signals to train smaller models to perform CoT reasoning in complex modalities.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03453v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03453v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03461v1",
    "title": "https://arxiv.org/abs/2305.03461v1",
    "latest": "2023-05-08T19:41:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110334762396044128",
      "content": "<p>\ud83d\udcdd Interactive Acquisition of Fine-Grained Visual Concepts by Exploiting Semantics of Generic Characterizations in Discourse \ud83d\udcda</p><p>\"Composed of two interacting neural modules: a (1) referential learner which grounds the novel words, and (2) pragmatic inference module which infers the meaning of these grounded concepts.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/itl-ed/ns-arch\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/itl-ed/ns-arch</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03461v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03461v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03445v1",
    "title": "https://arxiv.org/abs/2305.03445v1",
    "latest": "2023-05-08T19:17:04+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110334667955438771",
      "content": "<p>\ud83d\udcdd LMs Stand Their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models \ud83d\udcda</p><p>\"Language models conceptualise embodied concepts to a degree which facilitates the interpretation of figurative language in a better fashion when the action of the figurative sentence is more embodied.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/armandrotaru/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/armandrotaru/</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03445v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03445v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03429v1",
    "title": "Simulating H.P. Lovecraft horror literature with the ChatGPT large language model",
    "latest": "2023-05-08T18:41:04+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110334526408047753",
      "content": "<p>\ud83d\udcdd Simulating H.P. Lovecraft Horror Literature with the ChatGPT Large Language Model \ud83d\udcda</p><p>\"Presents a novel approach to simulating H P Lovecraft's horror literature using the ChatGPT large language model, specifically the GPT-4 architecture.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03429v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03429v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03423v1",
    "title": "Using ChatGPT for Entity Matching",
    "latest": "2023-05-08T18:05:04+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110334384844671592",
      "content": "<p>\ud83d\udcdd Using ChatGPT for Entity Matching \ud83d\udcda</p><p>\"Investigates using ChatGPT for entity matching as a more robust, training data-efficient alternative to traditional Transformer models by designing three experiments on different aspects of this approach.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03423v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03423v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03356v1",
    "title": "From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base",
    "latest": "2023-05-08T17:05:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110334148967331912",
      "content": "<p>\ud83d\udcdd From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering Over Knowledge Base \ud83d\udcda\ud83d\udc7e\ud83e\udde0</p><p>\"A novel parse-execute-refine paradigm is proposed to improve reasoning abilities of semantic parsers on complex questions over knowledge bases (KBQA) by demonstrating intermediate reasoning processes to the KBQA model.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a> <a href=\"https://creative.ai/tags/LG\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>LG</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03356v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03356v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.36850/mr5",
    "title": "https://doi.org/10.36850/mr5",
    "latest": "2023-05-08T16:49:22+00:00",
    "last_post": {
      "url": "https://fediscience.org/@MarkRubin/110329879444046719",
      "content": "<p>Open Science and Academic Workload</p><p>New article by Thomas Hostler in the Journal of Trial and Error:</p><p>\u201cThere is a high chance that without intervention, increased expectations to engage in open research practices may lead to unacceptable increases in demands on academics.\u201d</p><p>Open access: <a href=\"https://doi.org/10.36850/mr5\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.36850/mr5</span><span class=\"invisible\"></span></a> </p><p><a href=\"https://fediscience.org/tags/Science\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>Science</span></a><br><a href=\"https://fediscience.org/tags/OpenScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>OpenScience</span></a><br><a href=\"https://fediscience.org/tags/MetaScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MetaScience</span></a><br><a href=\"https://fediscience.org/tags/MetaResearch\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>MetaResearch</span></a><br><a href=\"https://fediscience.org/tags/SociologyofScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>SociologyofScience</span></a><br><a href=\"https://fediscience.org/tags/ScienceofScience\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>ScienceofScience</span></a><br><a href=\"https://fediscience.org/tags/STS\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>STS</span></a><br><span class=\"h-card\"><a href=\"https://a.gup.pe/u/stsing\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>stsing</span></a></span><br><span class=\"h-card\"><a href=\"https://a.gup.pe/u/academicchatter\" class=\"u-url mention\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">@<span>academicchatter</span></a></span><br><a href=\"https://fediscience.org/tags/AcademicWorkload\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AcademicWorkload</span></a></p>"
    },
    "people": [
      {
        "url": "https://mapstodon.space/@floledermann",
        "display_name": "Florian Ledermann"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03353v1",
    "title": "https://arxiv.org/abs/2305.03353v1",
    "latest": "2023-05-08T16:41:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110334054587542018",
      "content": "<p>\ud83d\udcdd MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic \ud83d\udcda\ud83d\udc7e</p><p>\"Uses dynamic epistemic logic (DEL), a formal theory of knowledge, to generate more intricate problems than in prior studies and express these problems using natural language.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a> <a href=\"https://creative.ai/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>AI</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/antoinelrnld/modlog\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">github.com/antoinelrnld/modlog</span><span class=\"invisible\"></span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03353v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03353v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://doi.org/10.21105/joss.05135",
    "title": "https://doi.org/10.21105/joss.05135",
    "latest": "2023-05-08T16:34:08+00:00",
    "last_post": {
      "url": "https://fosstodon.org/@joss/110333274647485807",
      "content": "<p>Just published in JOSS: 'LaMa: a thematic labelling web application' <a href=\"https://doi.org/10.21105/joss.05135\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">doi.org/10.21105/joss.05135</span><span class=\"invisible\"></span></a></p>"
    },
    "people": [
      {
        "url": "https://piaille.fr/@paulanomalie",
        "display_name": "Paul"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03132v1",
    "title": "The Role of Global and Local Context in Named Entity Recognition",
    "latest": "2023-05-08T09:29:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110332355898722537",
      "content": "<p>\ud83d\udcdd The Role of Global and Local Context in Named Entity Recognition \ud83d\udcda</p><p>\"Finds that correctly retrieving global document context has a greater impact on performance than only leveraging local context, prompting for further research on how to better retrieve that context.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03132v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03132v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  },
  {
    "link": "https://arxiv.org/abs/2305.03130v1",
    "title": "https://arxiv.org/abs/2305.03130v1",
    "latest": "2023-05-08T09:05:05+00:00",
    "last_post": {
      "url": "https://creative.ai/@arxiv_cl/110332261551212177",
      "content": "<p>\ud83d\udcdd Chain-of-Skills: A Configurable Model for Open-Domain Question Answering \ud83d\udcda</p><p>\"Modular retriever is a multitask retriever trained in a modular way, where individual retriever modules correspond to key retrieval skills that can be reused across datasets.\" [gal30b+] \ud83e\udd16 <a href=\"https://creative.ai/tags/CL\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>CL</span></a></p><p>\u2699\ufe0f <a href=\"https://github.com/facebookresearch/DPR\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"ellipsis\">github.com/facebookresearch/DP</span><span class=\"invisible\">R</span></a><br>\ud83d\udd17 <a href=\"https://arxiv.org/abs/2305.03130v1\" rel=\"nofollow noopener noreferrer\" target=\"_blank\"><span class=\"invisible\">https://</span><span class=\"\">arxiv.org/abs/2305.03130v1</span><span class=\"invisible\"></span></a> <a href=\"https://creative.ai/tags/arxiv\" class=\"mention hashtag\" rel=\"nofollow noopener noreferrer\" target=\"_blank\">#<span>arxiv</span></a></p>"
    },
    "people": [
      {
        "url": "https://creative.ai/@arxiv_cl",
        "display_name": "arXiv Comp. Linguistics\ud83d\udcda"
      }
    ]
  }
]